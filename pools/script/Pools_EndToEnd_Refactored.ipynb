{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ffadb4c",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸŠâ€â™€ï¸ Berlin Pools â€” Endâ€‘toâ€‘End Pipeline (Refactored)\n",
    "\n",
    "**Purpose:** run this notebook topâ€‘toâ€‘bottom to build the unified dataset of Berlin pools from legacy and OSM sources.\n",
    "\n",
    "---\n",
    "\n",
    "## What this notebook does (high level)\n",
    "1. **Environment & Config** â€” installs minimal deps (if missing) and sets file paths & knobs.\n",
    "2. **Run legacy extractor** â€” executes `pool_data_processing.ipynb` (if needed) to produce:  \n",
    "   - `berlin_pools_final_dataset.csv`  \n",
    "   - `pools_data_cleaned.csv`\n",
    "3. **(Optional) OSM wide export** â€” builds `osm_pools_wide.csv` via OSMnx (can be heavy, disabled by default).\n",
    "4. **OSM preparation** â€” creates normalized names and the list of **new public named pools** not matched in legacy:  \n",
    "   - `legacy_enrichment_list.csv` (candidate pairs for enrichment)  \n",
    "   - `osm_public_named.csv` (OSM-only named pools)\n",
    "5. **Reverse geocode enrich (cache-aware)** â€” fills missing **street** and **postal_code** in `osm_public_named.csv` using Nominatim with `reverse_geocode_cache.csv`.\n",
    "6. **Master build** â€” combines legacy + OSM-only rows into `pools_master_minimal.csv`, ensuring:\n",
    "   - stable **pool_id** for legacy rows (preserved if present)\n",
    "   - robust **pool_id** for OSM rows (from OSM id or coordinate surrogate)\n",
    "   - **district_id** mapped from district names (and reverseâ€‘geocoded if needed)\n",
    "   - missing `open_all_year` filled with `False`\n",
    "7. **Validation & outputs** â€” prints row counts and missing fields.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a671214f",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Environment & Config\n",
    "\n",
    "**Why:** ensure the notebook is reproducible, selfâ€‘contained, and gives you simple toggles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbaf49f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Minimal deps auto-install (safe no-ops if already installed) ---\n",
    "import sys, subprocess, importlib\n",
    "\n",
    "def ensure(pkg, pip_name=None):\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name or pkg])\n",
    "\n",
    "for mod, pipn in [(\"pandas\", None),\n",
    "                  (\"numpy\", None),\n",
    "                  (\"geopy\", None),\n",
    "                  (\"nbformat\", None),\n",
    "                  (\"nbconvert\", None)]:\n",
    "    ensure(mod, pipn)\n",
    "\n",
    "# OSMnx only needed if MAKE_OSM_WIDE = True\n",
    "try:\n",
    "    import osmnx  # noqa\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1a6cbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured paths:\n",
      "  LEGACY_NOTEBOOK: C:\\Users\\micha\\Projects VS\\refacforing hopefuly last\\pool_data_processing.ipynb\n",
      "  LEGACY_XLSX    : C:\\Users\\micha\\Projects VS\\refacforing hopefuly last\\baederleben_berlin.xlsx\n",
      "  LEGACY_MAIN_CSV: C:\\Users\\micha\\Projects VS\\refacforing hopefuly last\\berlin_pools_final_dataset.csv\n",
      "  LEGACY_ALT_CSV : C:\\Users\\micha\\Projects VS\\refacforing hopefuly last\\pools_data_cleaned.csv\n",
      "  OSM_WIDE_CSV   : C:\\Users\\micha\\Projects VS\\refacforing hopefuly last\\osm_pools_wide.csv (build: True place: Berlin, Germany )\n",
      "  OSM_PUBLIC_NAMED_CSV: C:\\Users\\micha\\Projects VS\\refacforing hopefuly last\\osm_public_named.csv\n",
      "  DIST_M: 100.0 | STRICT_PUBLIC: False\n",
      "  CACHE_PATH: C:\\Users\\micha\\Projects VS\\refacforing hopefuly last\\reverse_geocode_cache.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Configuration knobs & paths (adjust as needed) ---\n",
    "def resolve_path(p):\n",
    "    return Path(str(p)).expanduser().resolve()\n",
    "\n",
    "# Input artifacts expected next to this notebook\n",
    "LEGACY_NOTEBOOK = resolve_path(\"pool_data_processing.ipynb\")\n",
    "LEGACY_XLSX     = resolve_path(\"baederleben_berlin.xlsx\")\n",
    "\n",
    "# Outputs produced by legacy extractor\n",
    "LEGACY_MAIN_CSV = resolve_path(\"berlin_pools_final_dataset.csv\")\n",
    "LEGACY_ALT_CSV  = resolve_path(\"pools_data_cleaned.csv\")\n",
    "\n",
    "# OSM artifacts\n",
    "MAKE_OSM_WIDE   = True  # set True to build OSM via OSMnx (heavy + may need internet)\n",
    "OSM_PLACE       = \"Berlin, Germany\"\n",
    "OSM_WIDE_CSV    = resolve_path(\"osm_pools_wide.csv\")   # optional\n",
    "OSM_PUBLIC_NAMED_CSV = resolve_path(\"osm_public_named.csv\")\n",
    "\n",
    "# Matching & filtering knobs\n",
    "DIST_M          = 100.0\n",
    "STRICT_PUBLIC   = False  # keep False unless you want to only keep strictly public pools\n",
    "\n",
    "# Enrichment cache for reverse geocoding\n",
    "CACHE_PATH      = resolve_path(\"reverse_geocode_cache.csv\")\n",
    "\n",
    "print(\"Configured paths:\")\n",
    "print(\"  LEGACY_NOTEBOOK:\", LEGACY_NOTEBOOK)\n",
    "print(\"  LEGACY_XLSX    :\", LEGACY_XLSX)\n",
    "print(\"  LEGACY_MAIN_CSV:\", LEGACY_MAIN_CSV)\n",
    "print(\"  LEGACY_ALT_CSV :\", LEGACY_ALT_CSV)\n",
    "print(\"  OSM_WIDE_CSV   :\", OSM_WIDE_CSV, \"(build:\", MAKE_OSM_WIDE, \"place:\", OSM_PLACE, \")\")\n",
    "print(\"  OSM_PUBLIC_NAMED_CSV:\", OSM_PUBLIC_NAMED_CSV)\n",
    "print(\"  DIST_M:\", DIST_M, \"| STRICT_PUBLIC:\", STRICT_PUBLIC)\n",
    "print(\"  CACHE_PATH:\", CACHE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22d9264",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Run legacy extractor (if needed)\n",
    "\n",
    "**Why:** reproduce the legacy outputs that serve as the baseline ground truth.  \n",
    "This executes `pool_data_processing.ipynb` with its own working directory to generate the CSVs.\n",
    "\n",
    "**Outputs:**\n",
    "- `berlin_pools_final_dataset.csv`\n",
    "- `pools_data_cleaned.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9d4fa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Executing legacy extractor notebookâ€¦\n",
      "[ok] Legacy outputs ready: MAIN=berlin_pools_final_dataset.csv | ALT=pools_data_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nbconvert.preprocessors import ExecutePreprocessor\n",
    "import nbformat, io, os\n",
    "\n",
    "need_legacy = (not LEGACY_MAIN_CSV.exists()) or (not LEGACY_ALT_CSV.exists())\n",
    "\n",
    "if need_legacy:\n",
    "    if not LEGACY_NOTEBOOK.exists():\n",
    "        raise FileNotFoundError(f\"Legacy notebook missing: {LEGACY_NOTEBOOK}\")\n",
    "    if not LEGACY_XLSX.exists():\n",
    "        raise FileNotFoundError(f\"Legacy input Excel missing: {LEGACY_XLSX}\")\n",
    "    print(\"[info] Executing legacy extractor notebookâ€¦\")\n",
    "    with open(LEGACY_NOTEBOOK, \"r\", encoding=\"utf-8\") as f:\n",
    "        nb = nbformat.read(f, as_version=4)\n",
    "    ep = ExecutePreprocessor(timeout=1800, kernel_name=\"python3\")\n",
    "    ep.preprocess(nb, resources={\"metadata\":{\"path\": str(LEGACY_NOTEBOOK.parent)}})\n",
    "\n",
    "# Probe outputs / fallbacks\n",
    "found = []\n",
    "if LEGACY_MAIN_CSV.exists(): found.append(LEGACY_MAIN_CSV)\n",
    "if LEGACY_ALT_CSV.exists():  found.append(LEGACY_ALT_CSV)\n",
    "if not found:\n",
    "    # search for likely candidates\n",
    "    candidates = list(LEGACY_NOTEBOOK.parent.rglob(\"*pools*cleaned*.csv\"))\n",
    "    if candidates:\n",
    "        print(\"[warn] Expected outputs not found â€” using best candidate:\", candidates[0])\n",
    "        if not LEGACY_ALT_CSV.exists():\n",
    "            LEGACY_ALT_CSV = candidates[0]\n",
    "\n",
    "print(\"[ok] Legacy outputs ready:\",\n",
    "      \"MAIN=\" + str(LEGACY_MAIN_CSV.name if LEGACY_MAIN_CSV.exists() else \"missing\"),\n",
    "      \"| ALT=\" + str(LEGACY_ALT_CSV.name if LEGACY_ALT_CSV.exists() else \"missing\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf8f244",
   "metadata": {},
   "source": [
    "## (Optional) Build OSM wide export\n",
    "\n",
    "**Why:** reproducible OSM pull to CSV (`osm_pools_wide.csv`). This step is **disabled by default** because it can be heavy and requires internet.\n",
    "\n",
    "**Skip it** if you already have an OSM CSV to work from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b478beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Building OSM wide export via OSMnxâ€¦\n",
      "[ok] Wrote: C:\\Users\\micha\\Projects VS\\refacforing hopefuly last\\osm_pools_wide.csv | rows: 1764\n"
     ]
    }
   ],
   "source": [
    "# 3) (Optional) Build OSM wide export (osm_pools_wide.csv) â€” version-safe for OSMnx\n",
    "\n",
    "MAKE_OSM_WIDE = globals().get(\"MAKE_OSM_WIDE\", False)\n",
    "OSM_PLACE     = globals().get(\"OSM_PLACE\", \"Berlin, Germany\")\n",
    "OSM_WIDE_CSV  = globals().get(\"OSM_WIDE_CSV\", Path(\"osm_pools_wide.csv\"))\n",
    "\n",
    "if MAKE_OSM_WIDE:\n",
    "    try:\n",
    "        import os\n",
    "        import pandas as pd\n",
    "        from pathlib import Path\n",
    "        import osmnx as ox\n",
    "\n",
    "        # be polite & reproducible\n",
    "        try:\n",
    "            ox.settings.use_cache = True\n",
    "            ox.settings.log_console = False\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        print(\"[info] Building OSM wide export via OSMnxâ€¦\")\n",
    "        tags = {\"leisure\": [\"swimming_pool\", \"sports_centre\"]}\n",
    "\n",
    "        # Handle OSMnx API differences\n",
    "        if hasattr(ox, \"geometries_from_place\"):\n",
    "            gdf = ox.geometries_from_place(OSM_PLACE, tags)\n",
    "        elif hasattr(ox, \"features_from_place\"):\n",
    "            gdf = ox.features_from_place(OSM_PLACE, tags)\n",
    "        elif hasattr(ox, \"features\") and hasattr(ox.features, \"features_from_place\"):\n",
    "            gdf = ox.features.features_from_place(OSM_PLACE, tags)\n",
    "        else:\n",
    "            raise AttributeError(\n",
    "                \"Your OSMnx version doesnâ€™t expose geometries_from_place/features_from_place.\"\n",
    "            )\n",
    "\n",
    "        gdf = gdf.reset_index(drop=False)\n",
    "\n",
    "        # Ensure WGS84 and compute representative lat/lon\n",
    "        try:\n",
    "            if gdf.crs is None or (getattr(gdf.crs, \"to_epsg\", lambda: None)() != 4326):\n",
    "                gdf = gdf.to_crs(epsg=4326)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        if \"geometry\" in gdf.columns:\n",
    "            def _lat(geom):\n",
    "                try:\n",
    "                    return geom.y if geom.geom_type == \"Point\" else geom.centroid.y\n",
    "                except Exception:\n",
    "                    return None\n",
    "            def _lon(geom):\n",
    "                try:\n",
    "                    return geom.x if geom.geom_type == \"Point\" else geom.centroid.x\n",
    "                except Exception:\n",
    "                    return None\n",
    "            gdf[\"lat\"] = gdf[\"geometry\"].map(_lat)\n",
    "            gdf[\"lon\"] = gdf[\"geometry\"].map(_lon)\n",
    "\n",
    "        # Choose a tidy subset and rename addr columns\n",
    "        keep_cols = [\n",
    "            \"osmid\",\"name\",\"leisure\",\"sport\",\"website\",\"opening_hours\",\n",
    "            \"addr:street\",\"addr:postcode\",\"lat\",\"lon\"\n",
    "        ]\n",
    "        keep_cols = [c for c in keep_cols if c in gdf.columns]\n",
    "        out = gdf[keep_cols].rename(columns={\"addr:street\":\"street\",\"addr:postcode\":\"postal_code\"})\n",
    "\n",
    "        out.to_csv(OSM_WIDE_CSV, index=False)\n",
    "        print(\"[ok] Wrote:\", OSM_WIDE_CSV, \"| rows:\", len(out))\n",
    "    except Exception as e:\n",
    "        print(\"[warn] OSM wide build failed:\", repr(e))\n",
    "        print(\"      Tip: you can skip this step and place an existing OSM CSV in the folder instead.\")\n",
    "else:\n",
    "    print(\"[skip] MAKE_OSM_WIDE=False â€” expecting an existing OSM CSV if needed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5236f635",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Build additional files to cross check what is missing in legacy tool vs OSM based on osm_pools_wide.csv and berlin_pools_final_dataset.csv\n",
    "- `legacy_enrichment_list.csv` â€” candidates to pull extra attributes from OSM\n",
    "- `osm_public_named.csv` â€” OSM pools not matched to legacy (potential new pools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b168c504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Using legacy: berlin_pools_final_dataset.csv\n",
      "[info] Using OSM:    osm_pools_wide.csv\n",
      "{'legacy_rows': 144, 'osm_wide_rows': 1764, 'osm_swim_only': 984, 'osm_named_valid': 71}\n",
      "[ok] Wrote: legacy_enrichment_list.csv, osm_public_named.csv\n",
      "{'legacy_enrichment_list': 49, 'osm_public_named': 24}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, math, unicodedata, re\n",
    "\n",
    "# --- inputs already defined earlier in the NB ---\n",
    "LEGACY_PATH = LEGACY_MAIN_CSV if LEGACY_MAIN_CSV.exists() else LEGACY_ALT_CSV\n",
    "OSM_PATH    = OSM_WIDE_CSV\n",
    "DIST_M      = 250   # or your preferred matching radius\n",
    "STRICT_PUBLIC = False\n",
    "\n",
    "print(f\"[info] Using legacy: {LEGACY_PATH.name}\")\n",
    "print(f\"[info] Using OSM:    {OSM_PATH.name}\")\n",
    "\n",
    "legacy_raw = pd.read_csv(LEGACY_PATH)\n",
    "osm_raw    = pd.read_csv(OSM_PATH)\n",
    "\n",
    "# ------- helpers -------\n",
    "def normalize_ascii(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    return \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "\n",
    "def normalize_name(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    s = normalize_ascii(s).lower().strip()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def is_public_access(v, strict=False) -> bool:\n",
    "    if v is None or (isinstance(v, float) and math.isnan(v)):\n",
    "        return not strict\n",
    "    v = str(v).strip().lower()\n",
    "    if v in {\"private\",\"customers\",\"residents\"}: return False\n",
    "    return (v in {\"yes\",\"public\"} if strict else v in {\"\",\"yes\",\"public\",\"permissive\"} or v is None)\n",
    "\n",
    "def haversine_m(lat1, lon1, lat2, lon2) -> float:\n",
    "    if any(pd.isna([lat1, lon1, lat2, lon2])): return np.nan\n",
    "    R=6371000.0\n",
    "    phi1=math.radians(lat1); phi2=math.radians(lat2)\n",
    "    dphi=math.radians(lat2-lat1); dl=math.radians(lon2-lon1)\n",
    "    a=math.sin(dphi/2)**2+math.cos(phi1)*math.cos(phi2)*math.sin(dl/2)**2\n",
    "    return 2*R*math.asin(math.sqrt(a))\n",
    "\n",
    "def colmap(df): return {c.lower(): c for c in df.columns}\n",
    "def cget(cols, *choices):\n",
    "    for ch in choices:\n",
    "        if ch in cols: return cols[ch]\n",
    "    return None\n",
    "\n",
    "def coalesce_series(df, colnames):\n",
    "    if not colnames: return pd.Series([\"\"]*len(df), index=df.index, dtype=object)\n",
    "    out = df[colnames[0]].astype(object).fillna(\"\")\n",
    "    for c in colnames[1:]:\n",
    "        nxt = df[c].astype(object).fillna(\"\")\n",
    "        use_next = out.astype(str).str.strip().eq(\"\")\n",
    "        out = out.where(~use_next, nxt)\n",
    "    return out.fillna(\"\").astype(str)\n",
    "\n",
    "def has_swimming(val):\n",
    "    if val is None or (isinstance(val, float) and np.isnan(val)): return False\n",
    "    s = str(val).lower()\n",
    "    return (\"swimming\" in s) or (\"schwimm\" in s)\n",
    "\n",
    "# 1) Filter OSM wide to swimming features\n",
    "cols = colmap(osm_raw)\n",
    "leisure_c = cget(cols, \"leisure\")\n",
    "sport_c   = cget(cols, \"sport\")\n",
    "if not leisure_c:\n",
    "    raise KeyError(\"OSM-wide CSV must include a 'leisure' column.\")\n",
    "\n",
    "leisure = osm_raw[leisure_c].astype(str).str.lower()\n",
    "sport   = osm_raw[sport_c].astype(str) if sport_c else pd.Series([\"\"]*len(osm_raw), index=osm_raw.index)\n",
    "\n",
    "is_pool = (\n",
    "    leisure.eq(\"swimming_pool\") |\n",
    "    leisure.eq(\"swimming_area\") |\n",
    "    (leisure.eq(\"sports_centre\") & sport.apply(has_swimming))\n",
    ")\n",
    "osm_swim = osm_raw[is_pool].copy()\n",
    "\n",
    "# 2) Map to a working schema + keep addr fields\n",
    "def map_osm_to_db(df: pd.DataFrame, strict_public: bool=False) -> pd.DataFrame:\n",
    "    if df.empty: return df.copy()\n",
    "    c = colmap(df)\n",
    "\n",
    "    name_candidates = [x for x in [\"name\",\"official_name\",\"short_name\",\"alt_name\",\"name:de\",\"name:en\",\"brand\",\"operator\"] if x in c]\n",
    "    name_best = coalesce_series(df, [c[x] for x in name_candidates])\n",
    "\n",
    "    access_col = cget(c, \"access\")\n",
    "    lat_col    = cget(c, \"lat\",\"latitude\")\n",
    "    lon_col    = cget(c, \"lon\",\"longitude\")\n",
    "    typ_col    = cget(c, \"pool_type\",\"leisure\",\"sport\",\"type\")\n",
    "    phone_col  = cget(c, \"phone\",\"contact:phone\")\n",
    "    web_col    = cget(c, \"website\",\"contact:website\",\"url\")\n",
    "    oh_col     = cget(c, \"opening_hours\")\n",
    "    wh_col     = cget(c, \"wheelchair\")\n",
    "    id_col     = cget(c, \"osm_id\",\"@id\",\"id\",\"element_id\",\"osmid\")\n",
    "    street_col = cget(c, \"addr:street\",\"addr_street\",\"street\")\n",
    "    post_col   = cget(c, \"addr:postcode\",\"addr_postcode\",\"postal_code\")\n",
    "\n",
    "    df_f = df.copy()\n",
    "    if access_col and access_col in df_f.columns:\n",
    "        df_f = df_f[df_f[access_col].apply(lambda v: is_public_access(v, strict_public))].copy()\n",
    "\n",
    "    # stable-ish source_id with surrogate fallback\n",
    "    src = df_f[id_col].astype(\"string\") if id_col else pd.Series([\"\"]*len(df_f), index=df_f.index, dtype=\"string\")\n",
    "    needs_sur = src.isna() | src.str.strip().eq(\"\") | (src == \"<NA>\")\n",
    "    src = src.where(~needs_sur, \"sur_\" + pd.Series(df_f.index.astype(str), index=df_f.index))\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"source\":      \"osm\",\n",
    "        \"source_id\":   src,\n",
    "        \"name\":        name_best.reindex(df_f.index).fillna(\"\"),\n",
    "        \"name_norm\":   name_best.reindex(df_f.index).fillna(\"\").astype(str).map(normalize_name),\n",
    "        \"lat\":         pd.to_numeric(df_f.get(lat_col, np.nan), errors=\"coerce\"),\n",
    "        \"lon\":         pd.to_numeric(df_f.get(lon_col, np.nan), errors=\"coerce\"),\n",
    "        \"street\":      df_f.get(street_col, \"\"),\n",
    "        \"postal_code\": df_f.get(post_col, \"\"),\n",
    "        \"pool_type\":   df_f.get(typ_col, \"\"),\n",
    "        \"phone\":       df_f.get(phone_col, \"\"),\n",
    "        \"website\":     df_f.get(web_col, \"\"),\n",
    "        \"opening_hours\": df_f.get(oh_col, \"\"),\n",
    "        \"wheelchair\":  df_f.get(wh_col, \"\"),\n",
    "    })\n",
    "    return out\n",
    "\n",
    "def map_legacy_to_db(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty: return df.copy()\n",
    "    c = colmap(df)\n",
    "    return pd.DataFrame({\n",
    "        \"source\":\"legacy\",\n",
    "        \"source_id\": df.get(cget(c,\"legacy_id\",\"pool_id\",\"id\"), pd.Series(index=df.index, dtype=object)),\n",
    "        \"name\": df.get(cget(c,\"name\",\"pool_name\"), \"\"),\n",
    "        \"name_norm\": df.get(cget(c,\"name\",\"pool_name\"), \"\").astype(str).map(normalize_name),\n",
    "        \"lat\": pd.to_numeric(df.get(cget(c,\"lat\",\"latitude\"), np.nan), errors=\"coerce\"),\n",
    "        \"lon\": pd.to_numeric(df.get(cget(c,\"lon\",\"longitude\"), np.nan), errors=\"coerce\"),\n",
    "        \"address\": df.get(cget(c,\"address\",\"addr\",\"street\"), \"\"),\n",
    "        \"district\": df.get(cget(c,\"district\",\"bezirk\"), \"\"),\n",
    "        \"pool_type\": df.get(cget(c,\"pool_type\",\"type\"), \"\"),\n",
    "        \"phone\": df.get(cget(c,\"phone\"), \"\"),\n",
    "        \"website\": df.get(cget(c,\"website\",\"url\"), \"\"),\n",
    "        \"opening_hours\": df.get(cget(c,\"opening_hours\",\"hours\"), \"\"),\n",
    "        \"wheelchair\": df.get(cget(c,\"wheelchair\"), \"\"),\n",
    "    })\n",
    "\n",
    "legacy_db = map_legacy_to_db(legacy_raw)\n",
    "osm_db    = map_osm_to_db(osm_swim, strict_public=STRICT_PUBLIC)\n",
    "\n",
    "# keep only named + valid coords + light de-dup\n",
    "named_mask = osm_db[\"name\"].astype(\"string\").fillna(\"\").str.strip().ne(\"\")\n",
    "coord_mask = osm_db[\"lat\"].notna() & osm_db[\"lon\"].notna()\n",
    "osm_db = osm_db[named_mask & coord_mask].copy()\n",
    "osm_db[\"lat_r\"] = osm_db[\"lat\"].round(5)\n",
    "osm_db[\"lon_r\"] = osm_db[\"lon\"].round(5)\n",
    "osm_db = osm_db.drop_duplicates(subset=[\"name_norm\",\"lat_r\",\"lon_r\"], keep=\"first\").drop(columns=[\"lat_r\",\"lon_r\"])\n",
    "\n",
    "print({\n",
    "    \"legacy_rows\": len(legacy_db),\n",
    "    \"osm_wide_rows\": len(osm_raw),\n",
    "    \"osm_swim_only\": len(osm_swim),\n",
    "    \"osm_named_valid\": len(osm_db)\n",
    "})\n",
    "\n",
    "# 3) Pair & split unmatched â†’ osm_public_named\n",
    "L = legacy_db.assign(bucket=legacy_db[\"name_norm\"].str[:10])\n",
    "O = osm_db.assign(bucket=osm_db[\"name_norm\"].str[:10])\n",
    "\n",
    "cand = L.merge(O, on=\"bucket\", suffixes=(\"_l\",\"_o\"))\n",
    "cand[\"name_match\"] = (cand[\"name_norm_l\"].ne(\"\")) & cand[\"name_norm_l\"].eq(cand[\"name_norm_o\"])\n",
    "cand[\"dist_m\"] = cand.apply(lambda r: haversine_m(r[\"lat_l\"], r[\"lon_l\"], r[\"lat_o\"], r[\"lon_o\"]), axis=1)\n",
    "\n",
    "legacy_enrichment_list = cand[(cand[\"name_match\"]) | (cand[\"dist_m\"] <= DIST_M)].copy()\n",
    "keep_cols = [\n",
    "    \"source_id_l\",\"name_l\",\"lat_l\",\"lon_l\",\"address_l\",\"district_l\",\n",
    "    \"source_id_o\",\"name_o\",\"lat_o\",\"lon_o\",\"street_o\",\"postal_code_o\",\n",
    "    \"dist_m\",\"name_match\",\"website_o\",\"opening_hours_o\",\"wheelchair_o\",\"phone_o\"\n",
    "]\n",
    "legacy_enrichment_list = legacy_enrichment_list[[c for c in keep_cols if c in legacy_enrichment_list.columns]]\n",
    "\n",
    "matched_osm_ids = set(\n",
    "    legacy_enrichment_list.get(\"source_id_o\", pd.Series(dtype=object))\n",
    "    .dropna().astype(str).tolist()\n",
    ")\n",
    "osm_public_named = osm_db[~osm_db[\"source_id\"].astype(str).isin(matched_osm_ids)].copy()\n",
    "\n",
    "# standardize columns + save\n",
    "cols_order = [\"source_id\",\"name\",\"name_norm\",\"lat\",\"lon\",\"street\",\"postal_code\",\"pool_type\",\"phone\",\"website\",\"opening_hours\",\"wheelchair\"]\n",
    "legacy_enrichment_list.to_csv(\"legacy_enrichment_list.csv\", index=False)\n",
    "osm_public_named[ [c for c in cols_order if c in osm_public_named.columns] ].to_csv(\"osm_public_named.csv\", index=False)\n",
    "\n",
    "print(\"[ok] Wrote: legacy_enrichment_list.csv, osm_public_named.csv\")\n",
    "print({\n",
    "    \"legacy_enrichment_list\": len(legacy_enrichment_list),\n",
    "    \"osm_public_named\": len(osm_public_named)\n",
    "})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ef699f",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Load inputs & prep candidate pairs\n",
    "\n",
    "**Why:** we align legacy and OSM pools by approximate name + geo distance to produce:\n",
    "- `legacy_enrichment_list.csv` â€” candidates to pull extra attributes from OSM\n",
    "- `osm_public_named.csv` â€” OSM pools not matched to legacy (potential new pools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e40fb19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- add normalized names (robust + coalesce for OSM)\n",
    "import unicodedata, re\n",
    "import numpy as np\n",
    "\n",
    "def normalize_name(s: str) -> str:\n",
    "    s = \"\" if s is None or (isinstance(s, float) and np.isnan(s)) else str(s)\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s.lower()).strip()\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def ensure_text_col(df, *candidates, create=\"name\"):\n",
    "    \"\"\"Pick the first existing col; if none exist, create an empty text col.\"\"\"\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    if create not in df.columns:\n",
    "        df[create] = \"\"\n",
    "    return create\n",
    "\n",
    "def coalesce_cols(df, *cols):\n",
    "    \"\"\"First non-empty string across given columns (if present).\"\"\"\n",
    "    present = [c for c in cols if c in df.columns]\n",
    "    if not present:\n",
    "        return pd.Series([\"\"] * len(df), index=df.index, dtype=object)\n",
    "    out = df[present[0]].astype(str).fillna(\"\")\n",
    "    for c in present[1:]:\n",
    "        nxt = df[c].astype(str).fillna(\"\")\n",
    "        use_nxt = out.str.strip().eq(\"\")\n",
    "        out = out.where(~use_nxt, nxt)\n",
    "    return out.fillna(\"\")\n",
    "\n",
    "# --- legacy: use existing name/pool_name\n",
    "legacy_name_col = ensure_text_col(legacy_db, \"name\", \"pool_name\", create=\"name\")\n",
    "legacy_db[\"name\"] = legacy_db[legacy_name_col].astype(str).fillna(\"\").str.strip()\n",
    "legacy_db[\"name_norm\"] = legacy_db[\"name\"].map(normalize_name)\n",
    "\n",
    "# --- OSM: coalesce multiple name fields before normalizing\n",
    "osm_db[\"name\"] = coalesce_cols(\n",
    "    osm_db,\n",
    "    \"name\", \"official_name\", \"short_name\", \"alt_name\", \"name:de\", \"name:en\", \"brand\", \"operator\"\n",
    ").str.strip()\n",
    "\n",
    "# require string and normalize\n",
    "osm_db[\"name\"] = osm_db[\"name\"].fillna(\"\").astype(str)\n",
    "osm_db[\"name_norm\"] = osm_db[\"name\"].map(normalize_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8ff693",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Reverse geocode (street & postal_code) for OSM-only named and berlin pools final\n",
    "\n",
    "**Why:** OSM points can miss addresses. We fill `street` & `postal_code` using Nominatim, **with a local cache** to stay fast and polite.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca521e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] OSM: rows needing enrichment: 0\n",
      "[info] LEGACY: rows needing enrichment: 3\n",
      "[ok] Updated legacy file â†’ berlin_pools_final_dataset.csv\n",
      "[report] OSM missing â†’ street:0  postal_code:0  district:0\n",
      "[report] LEGACY missing â†’ street:0  postal_code:0  district:0\n"
     ]
    }
   ],
   "source": [
    "# --- Enrich street / postal_code / district for BOTH OSM + LEGACY with one cache ---\n",
    "from pathlib import Path\n",
    "import pandas as pd, re, unicodedata\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "# --------------------------- CONFIG ---------------------------\n",
    "OSM_PUBLIC_NAMED_CSV = Path(\"osm_public_named.csv\")\n",
    "CACHE_PATH = Path(\"reverse_geocode_cache.csv\")\n",
    "WRITE_LEGACY_INPLACE = True  # False -> writes berlin_pools_final_dataset_enriched.csv\n",
    "\n",
    "# Try to reuse notebook vars, else fall back to filename\n",
    "try:\n",
    "    LEGACY_CSV = (LEGACY_MAIN_CSV if LEGACY_MAIN_CSV.exists() else LEGACY_ALT_CSV)\n",
    "except Exception:\n",
    "    LEGACY_CSV = Path(\"berlin_pools_final_dataset.csv\")\n",
    "\n",
    "# -------------------------- HELPERS ---------------------------\n",
    "def strip_accents(s: str) -> str:\n",
    "    s = \"\" if s is None else str(s)\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    return \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "\n",
    "def is_blank(x) -> bool:\n",
    "    if pd.isna(x): return True\n",
    "    return str(x).strip().lower() in {\"\", \"nan\", \"none\", \"<na>\", \"null\"}\n",
    "\n",
    "def key(lat, lon):\n",
    "    try: return round(float(lat), 6), round(float(lon), 6)\n",
    "    except Exception: return None\n",
    "\n",
    "def clean_postcode(p):\n",
    "    m = re.search(r\"\\b(1[0-4]\\d{3})\\b\", str(p))\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "# Ortsteil (suburb) â†’ Bezirk\n",
    "ORTSTEIL_TO_BEZIRK = {\n",
    "    # Mitte\n",
    "    \"mitte\":\"Mitte\",\"tiergarten\":\"Mitte\",\"wedding\":\"Mitte\",\"gesundbrunnen\":\"Mitte\",\n",
    "    # Friedrichshain-Kreuzberg\n",
    "    \"friedrichshain\":\"Friedrichshain-Kreuzberg\",\"kreuzberg\":\"Friedrichshain-Kreuzberg\",\n",
    "    # Pankow\n",
    "    \"pankow\":\"Pankow\",\"prenzlauer berg\":\"Pankow\",\"niederschÃ¶nhausen\":\"Pankow\",\"heinersdorf\":\"Pankow\",\n",
    "    \"blankenburg\":\"Pankow\",\"buch\":\"Pankow\",\"karow\":\"Pankow\",\"franzÃ¶sisch buchholz\":\"Pankow\",\n",
    "    \"weiÃŸensee\":\"Pankow\",\"stadtrandsiedlung malchow\":\"Pankow\",\"borna\":\"Pankow\",\n",
    "    # Charlottenburg-Wilmersdorf\n",
    "    \"charlottenburg\":\"Charlottenburg-Wilmersdorf\",\"wilmersdorf\":\"Charlottenburg-Wilmersdorf\",\n",
    "    \"grunewald\":\"Charlottenburg-Wilmersdorf\",\"halensee\":\"Charlottenburg-Wilmersdorf\",\"westend\":\"Charlottenburg-Wilmersdorf\",\n",
    "    # Spandau\n",
    "    \"spandau\":\"Spandau\",\"haselhorst\":\"Spandau\",\"siemensstadt\":\"Spandau\",\"staaken\":\"Spandau\",\n",
    "    \"gatow\":\"Spandau\",\"kladow\":\"Spandau\",\"hakenfelde\":\"Spandau\",\n",
    "    # Steglitz-Zehlendorf\n",
    "    \"steglitz\":\"Steglitz-Zehlendorf\",\"zehlendorf\":\"Steglitz-Zehlendorf\",\"dahlem\":\"Steglitz-Zehlendorf\",\n",
    "    \"lichterfelde\":\"Steglitz-Zehlendorf\",\"lankwitz\":\"Steglitz-Zehlendorf\",\"nikolassee\":\"Steglitz-Zehlendorf\",\n",
    "    \"wannsee\":\"Steglitz-Zehlendorf\",\n",
    "    # Tempelhof-SchÃ¶neberg\n",
    "    \"tempelhof\":\"Tempelhof-SchÃ¶neberg\",\"mariendorf\":\"Tempelhof-SchÃ¶neberg\",\"marienfelde\":\"Tempelhof-SchÃ¶neberg\",\n",
    "    \"lichtenrade\":\"Tempelhof-SchÃ¶neberg\",\"schÃ¶neberg\":\"Tempelhof-SchÃ¶neberg\",\"friedrichshÃ¶he\":\"Tempelhof-SchÃ¶neberg\",\n",
    "    # NeukÃ¶lln\n",
    "    \"neukÃ¶lln\":\"NeukÃ¶lln\",\"britz\":\"NeukÃ¶lln\",\"buckow\":\"NeukÃ¶lln\",\"rudow\":\"NeukÃ¶lln\",\"gropiusstadt\":\"NeukÃ¶lln\",\n",
    "    # Treptow-KÃ¶penick\n",
    "    \"alt-treptow\":\"Treptow-KÃ¶penick\",\"plÃ¤nterwald\":\"Treptow-KÃ¶penick\",\"baumschulenweg\":\"Treptow-KÃ¶penick\",\n",
    "    \"niederschÃ¶neweide\":\"Treptow-KÃ¶penick\",\"oberschÃ¶neweide\":\"Treptow-KÃ¶penick\",\"kÃ¶penick\":\"Treptow-KÃ¶penick\",\n",
    "    \"friedrichshagen\":\"Treptow-KÃ¶penick\",\"rahnsdorf\":\"Treptow-KÃ¶penick\",\"grÃ¼nau\":\"Treptow-KÃ¶penick\",\n",
    "    \"mÃ¼ggelheim\":\"Treptow-KÃ¶penick\",\"schmÃ¶ckwitz\":\"Treptow-KÃ¶penick\",\"adlershof\":\"Treptow-KÃ¶penick\",\n",
    "    \"altglienicke\":\"Treptow-KÃ¶penick\",\"bohnsdorf\":\"Treptow-KÃ¶penick\",\n",
    "    # Marzahn-Hellersdorf\n",
    "    \"marzahn\":\"Marzahn-Hellersdorf\",\"hellersdorf\":\"Marzahn-Hellersdorf\",\"biesdorf\":\"Marzahn-Hellersdorf\",\n",
    "    \"kaulsdorf\":\"Marzahn-Hellersdorf\",\"mahlsdorf\":\"Marzahn-Hellersdorf\",\n",
    "    # Lichtenberg\n",
    "    \"lichtenberg\":\"Lichtenberg\",\"fennpfuhl\":\"Lichtenberg\",\"friedrichsfelde\":\"Lichtenberg\",\n",
    "    \"rummelsburg\":\"Lichtenberg\",\"karlshorst\":\"Lichtenberg\",\"malchow\":\"Lichtenberg\",\n",
    "    \"wartenberg\":\"Lichtenberg\",\"neu-hohenschÃ¶nhausen\":\"Lichtenberg\",\"alt-hohenschÃ¶nhausen\":\"Lichtenberg\",\n",
    "    # Reinickendorf\n",
    "    \"reinickendorf\":\"Reinickendorf\",\"tegel\":\"Reinickendorf\",\"wittenau\":\"Reinickendorf\",\"hermsdorf\":\"Reinickendorf\",\n",
    "    \"frohnau\":\"Reinickendorf\",\"mÃ¤rkiÂ­sches viertel\":\"Reinickendorf\",\"heiligensee\":\"Reinickendorf\",\n",
    "    \"konradshÃ¶he\":\"Reinickendorf\",\"lÃ¼bars\":\"Reinickendorf\",\"waidmannslust\":\"Reinickendorf\",\n",
    "}\n",
    "district_mapping = {\n",
    "    'Mitte': '01','Friedrichshain-Kreuzberg': '02','Pankow': '03',\n",
    "    'Charlottenburg-Wilmersdorf': '04','Spandau': '05','Steglitz-Zehlendorf': '06',\n",
    "    'Tempelhof-SchÃ¶neberg': '07','NeukÃ¶lln': '08','Treptow-KÃ¶penick': '09',\n",
    "    'Marzahn-Hellersdorf': '10','Lichtenberg': '11','Reinickendorf': '12'\n",
    "}\n",
    "def canonical_district(raw: str) -> str:\n",
    "    s = strip_accents(raw).strip()\n",
    "    s = s.replace(\"Mitte (Berlin)\", \"Mitte\") \\\n",
    "         .replace(\"Friedrichshain - Kreuzberg\",\"Friedrichshain-Kreuzberg\") \\\n",
    "         .replace(\"Charlottenburg - Wilmersdorf\",\"Charlottenburg-Wilmersdorf\") \\\n",
    "         .replace(\"Steglitz - Zehlendorf\",\"Steglitz-Zehlendorf\") \\\n",
    "         .replace(\"Tempelhof - Schoneberg\", \"Tempelhof-SchÃ¶neberg\") \\\n",
    "         .replace(\"Tempelhof - SchÃ¶neberg\",\"Tempelhof-SchÃ¶neberg\") \\\n",
    "         .replace(\"Treptow - Kopenick\",\"Treptow-KÃ¶penick\") \\\n",
    "         .replace(\"Marzahn - Hellersdorf\",\"Marzahn-Hellersdorf\")\n",
    "    s = re.sub(r\"^(Bezirk|District)\\s+\", \"\", s, flags=re.I)\n",
    "    s = re.sub(r\"\\s*-\\s*\", \"-\", s).strip()\n",
    "    return s if s in district_mapping else \"\"\n",
    "\n",
    "# Very light street parser from a free-text \"address\" (legacy)\n",
    "STREET_TOKENS = (\"straÃŸe\",\"str.\",\"strasse\",\"allee\",\"damm\",\"weg\",\"platz\",\"ufer\",\"chaussee\",\"ring\",\"steig\",\"promenade\",\"gasse\",\"pfad\")\n",
    "def street_from_address(addr: str) -> str:\n",
    "    parts = [p.strip() for p in str(addr).split(\",\") if p.strip()]\n",
    "    # pick the first piece that looks like a street\n",
    "    for p in parts:\n",
    "        low = p.lower()\n",
    "        if any(t in low for t in STREET_TOKENS):\n",
    "            return p\n",
    "    # fallback: \"Word Word 123\" pattern\n",
    "    m = re.search(r\"([A-Za-zÃ„Ã–ÃœÃ¤Ã¶Ã¼ÃŸ\\-\\s]+)\\s(\\d+[a-zA-Z]?)\", str(addr))\n",
    "    return m.group(0).strip() if m else \"\"\n",
    "\n",
    "# ------------------ shared cache + geocoder ------------------\n",
    "cache_cols = [\"lat\",\"lon\",\"street\",\"postal_code\",\"district\"]\n",
    "cache = pd.read_csv(CACHE_PATH) if CACHE_PATH.exists() else pd.DataFrame(columns=cache_cols)\n",
    "for c in cache_cols:\n",
    "    if c not in cache.columns: cache[c] = \"\"\n",
    "cache[\"key\"] = cache.apply(lambda r: key(r.get(\"lat\"), r.get(\"lon\")), axis=1)\n",
    "cache_dict = {k: (s, p, d) for k, s, p, d in zip(cache[\"key\"], cache[\"street\"], cache[\"postal_code\"], cache[\"district\"]) if pd.notna(k)}\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"pools_enrichment/1.0 (contact: your_email@example.com)\")\n",
    "reverse = RateLimiter(geolocator.reverse, min_delay_seconds=1.0, max_retries=2, error_wait_seconds=2.0, swallow_exceptions=True)\n",
    "\n",
    "def enrich_table_inplace(df, lat_candidates, lon_candidates, street_col, post_col, dist_col, prefill_from=None, name_hint_col=None, sanity_label=\"\"):\n",
    "    \"\"\"Mutates df to fill street/postal_code/district in-place.\"\"\"\n",
    "    # ensure cols\n",
    "    for c in [street_col, post_col, dist_col]:\n",
    "        if c not in df.columns: df[c] = \"\"\n",
    "        df[c] = df[c].astype(\"string\")\n",
    "    # lat/lon\n",
    "    def pickcol(cands):\n",
    "        for c in cands:\n",
    "            if c in df.columns: return c\n",
    "        return None\n",
    "    lat_c = pickcol(lat_candidates)\n",
    "    lon_c = pickcol(lon_candidates)\n",
    "    if not lat_c or not lon_c:\n",
    "        print(f\"[skip] {sanity_label}: no lat/lon\")\n",
    "        return\n",
    "\n",
    "    # optional prefill\n",
    "    if prefill_from and prefill_from in df.columns:\n",
    "        mask = df[street_col].apply(is_blank)\n",
    "        pf = df[prefill_from].astype(str).map(street_from_address)\n",
    "        df.loc[mask & pf.astype(str).str.strip().ne(\"\"), street_col] = pf[mask & pf.astype(str).str.strip().ne(\"\")]\n",
    "\n",
    "    # who needs?\n",
    "    need_mask = (\n",
    "        df[street_col].apply(is_blank) |\n",
    "        df[post_col].apply(is_blank)   |\n",
    "        df[dist_col].apply(is_blank)\n",
    "    ) & df[lat_c].notna() & df[lon_c].notna()\n",
    "\n",
    "    need = df[need_mask].copy()\n",
    "    print(f\"[info] {sanity_label}: rows needing enrichment:\", int(need_mask.sum()))\n",
    "\n",
    "    filled_rows = []\n",
    "    for idx, r in need.iterrows():\n",
    "        k = key(r[lat_c], r[lon_c])\n",
    "        if not k: \n",
    "            continue\n",
    "        s, p, d = cache_dict.get(k, (\"\",\"\",\"\"))\n",
    "\n",
    "        if is_blank(s) or is_blank(p) or is_blank(d):\n",
    "            loc = reverse((r[lat_c], r[lon_c]), exactly_one=True, addressdetails=True, language=\"de\", zoom=18)\n",
    "            ad  = (loc.raw.get(\"address\") if loc and hasattr(loc, \"raw\") else {}) or {}\n",
    "\n",
    "            # street / house no\n",
    "            if is_blank(s):\n",
    "                road = ad.get(\"road\") or ad.get(\"pedestrian\") or ad.get(\"footway\") or ad.get(\"path\") or \"\"\n",
    "                hn   = ad.get(\"house_number\") or \"\"\n",
    "                s = \" \".join([x for x in [road, hn] if x]).strip()\n",
    "\n",
    "            # postcode\n",
    "            if is_blank(p):\n",
    "                p = clean_postcode(ad.get(\"postcode\", \"\"))\n",
    "\n",
    "            # district\n",
    "            if is_blank(d):\n",
    "                raw_d = ad.get(\"city_district\") or ad.get(\"borough\") or ad.get(\"county\") or \"\"\n",
    "                d_can = canonical_district(raw_d)\n",
    "                if not d_can:\n",
    "                    sub = strip_accents(ad.get(\"suburb\") or ad.get(\"municipality\") or ad.get(\"neighbourhood\") or \"\").lower().strip()\n",
    "                    d_can = ORTSTEIL_TO_BEZIRK.get(sub, \"\")\n",
    "                d = d_can\n",
    "\n",
    "            if k:\n",
    "                filled_rows.append({\"lat\": k[0], \"lon\": k[1], \"street\": s, \"postal_code\": p, \"district\": d, \"key\": k})\n",
    "\n",
    "        # assign back only if blank\n",
    "        if not is_blank(s) and is_blank(df.at[idx, street_col]):\n",
    "            df.at[idx, street_col] = str(s)\n",
    "        if not is_blank(p) and is_blank(df.at[idx, post_col]):\n",
    "            df.at[idx, post_col] = str(p)\n",
    "        if not is_blank(d) and is_blank(df.at[idx, dist_col]):\n",
    "            df.at[idx, dist_col] = str(d)\n",
    "\n",
    "    # persist cache\n",
    "    if filled_rows:\n",
    "        cc = pd.concat([cache, pd.DataFrame(filled_rows)], ignore_index=True)\n",
    "        cc = cc.drop_duplicates(subset=[\"key\"], keep=\"last\")\n",
    "        cc[cache_cols].to_csv(CACHE_PATH, index=False)\n",
    "        # refresh in-memory map\n",
    "        cc[\"key\"] = cc.apply(lambda r: key(r.get(\"lat\"), r.get(\"lon\")), axis=1)\n",
    "        cache_dict.update({k: (s, p, d) for k, s, p, d in zip(cc[\"key\"], cc[\"street\"], cc[\"postal_code\"], cc[\"district\"]) if pd.notna(k)})\n",
    "\n",
    "# ------------------ OSM FILE ------------------\n",
    "osm = pd.read_csv(OSM_PUBLIC_NAMED_CSV)\n",
    "# prefill from OSM-style columns first\n",
    "if \"addr_street\" in osm.columns:\n",
    "    m = osm[\"street\"].apply(is_blank)\n",
    "    osm.loc[m, \"street\"] = osm.loc[m, \"addr_street\"].astype(str)\n",
    "if \"addr_postcode\" in osm.columns:\n",
    "    m = osm[\"postal_code\"].apply(is_blank) if \"postal_code\" in osm.columns else pd.Series(True, index=osm.index)\n",
    "    osm.loc[m, \"postal_code\"] = osm.loc[m, \"addr_postcode\"].astype(str)\n",
    "\n",
    "enrich_table_inplace(\n",
    "    osm,\n",
    "    lat_candidates=(\"lat\",\"latitude\"),\n",
    "    lon_candidates=(\"lon\",\"longitude\"),\n",
    "    street_col=\"street\",\n",
    "    post_col=\"postal_code\",\n",
    "    dist_col=\"district\",\n",
    "    prefill_from=None,           # OSM already handled above\n",
    "    sanity_label=\"OSM\"\n",
    ")\n",
    "# normalize post code as text\n",
    "osm[\"postal_code\"] = osm[\"postal_code\"].astype(str).str.replace(\".0\",\"\",regex=False)\n",
    "osm.to_csv(OSM_PUBLIC_NAMED_CSV, index=False)\n",
    "\n",
    "# ------------------ LEGACY FILE ------------------\n",
    "legacy = pd.read_csv(LEGACY_CSV)\n",
    "# ensure columns exist for enrichment targets\n",
    "for c in [\"street\",\"postal_code\",\"district\"]:\n",
    "    if c not in legacy.columns: legacy[c] = \"\"\n",
    "\n",
    "# try to prefill street from any \"address\" field\n",
    "addr_col = \"address\" if \"address\" in legacy.columns else None\n",
    "enrich_table_inplace(\n",
    "    legacy,\n",
    "    lat_candidates=(\"lat\",\"latitude\"),\n",
    "    lon_candidates=(\"lon\",\"longitude\"),\n",
    "    street_col=\"street\",\n",
    "    post_col=\"postal_code\",\n",
    "    dist_col=\"district\",\n",
    "    prefill_from=addr_col,\n",
    "    sanity_label=\"LEGACY\"\n",
    ")\n",
    "legacy[\"postal_code\"] = legacy[\"postal_code\"].astype(str).str.replace(\".0\",\"\",regex=False)\n",
    "\n",
    "# write legacy back\n",
    "if WRITE_LEGACY_INPLACE:\n",
    "    legacy.to_csv(LEGACY_CSV, index=False)\n",
    "    print(\"[ok] Updated legacy file â†’\", LEGACY_CSV.name)\n",
    "else:\n",
    "    out_legacy = LEGACY_CSV.with_name(LEGACY_CSV.stem + \"_enriched.csv\")\n",
    "    legacy.to_csv(out_legacy, index=False)\n",
    "    print(\"[ok] Wrote legacy enriched â†’\", out_legacy.name)\n",
    "\n",
    "# ------------------ tiny checks ------------------\n",
    "def peek_missing(df, label):\n",
    "    miss_st = int(df[\"street\"].astype(str).str.strip().eq(\"\").sum()) if \"street\" in df.columns else 0\n",
    "    miss_pc = int(df[\"postal_code\"].astype(str).str.strip().eq(\"\").sum()) if \"postal_code\" in df.columns else 0\n",
    "    miss_di = int(df[\"district\"].astype(str).str.strip().eq(\"\").sum()) if \"district\" in df.columns else 0\n",
    "    print(f\"[report] {label} missing â†’ street:{miss_st}  postal_code:{miss_pc}  district:{miss_di}\")\n",
    "\n",
    "peek_missing(osm, \"OSM\")\n",
    "peek_missing(legacy, \"LEGACY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0387b49",
   "metadata": {},
   "source": [
    "issue with sommerbad - no street name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3cb99eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[street-fill] rows needing street: 0\n",
      "[street-fill] still missing after Nominatim: 0\n",
      "        name          street postal_code  district\n",
      "5  Sommerbad  Campus Efeuweg       12351  NeukÃ¶lln\n",
      "[ok] street backfilled where possible\n"
     ]
    }
   ],
   "source": [
    "# Fix Sommerbad (and others in case simillar will appear in the future)\n",
    "# Fill missing streets by (1) another Nominatim pass + (2) nearest named OSM road via OSMnx\n",
    "from pathlib import Path\n",
    "import pandas as pd, re, unicodedata\n",
    "\n",
    "OSM_PUBLIC_NAMED_CSV = Path(\"osm_public_named.csv\")\n",
    "NEAREST_ROAD_CACHE = Path(\"nearest_road_cache.csv\")\n",
    "\n",
    "def is_blank(x):\n",
    "    if pd.isna(x): return True\n",
    "    return str(x).strip().lower() in {\"\", \"nan\", \"none\", \"<na>\", \"null\"}\n",
    "\n",
    "def street_like_piece(s: str) -> str:\n",
    "    # pick a display_name piece that looks like a street\n",
    "    tokens = [\"straÃŸe\",\"strasse\",\"allee\",\"damm\",\"weg\",\"platz\",\"ufer\",\"chaussee\",\"ring\",\"steig\",\"promenade\",\"gasse\",\"pfad\",\"chaussee\"]\n",
    "    for piece in [p.strip() for p in str(s).split(\",\")]:\n",
    "        low = piece.lower()\n",
    "        if any(t in low for t in tokens):\n",
    "            return piece\n",
    "    return \"\"\n",
    "\n",
    "def key(lat, lon):\n",
    "    try: return round(float(lat), 6), round(float(lon), 6)\n",
    "    except Exception: return None\n",
    "\n",
    "df = pd.read_csv(OSM_PUBLIC_NAMED_CSV)\n",
    "\n",
    "# ensure columns + dtypes\n",
    "for c in [\"name\",\"street\",\"postal_code\",\"district\"]:\n",
    "    if c not in df.columns: df[c] = \"\"\n",
    "df[[\"name\",\"street\",\"postal_code\",\"district\"]] = df[[\"name\",\"street\",\"postal_code\",\"district\"]].astype(\"string\")\n",
    "\n",
    "lat_col = \"lat\" if \"lat\" in df.columns else (\"latitude\" if \"latitude\" in df.columns else None)\n",
    "lon_col = \"lon\" if \"lon\" in df.columns else (\"longitude\" if \"longitude\" in df.columns else None)\n",
    "if not lat_col or not lon_col:\n",
    "    raise KeyError(\"Need 'lat'/'lon' (or 'latitude'/'longitude') to find nearest roads.\")\n",
    "\n",
    "need_mask = df[\"street\"].apply(is_blank) & df[lat_col].notna() & df[lon_col].notna()\n",
    "need = df[need_mask].copy()\n",
    "print(f\"[street-fill] rows needing street: {int(need_mask.sum())}\")\n",
    "\n",
    "# 1) Tiny extra Nominatim poke (zoom up + parse display_name) â€” cheap and sometimes enough\n",
    "try:\n",
    "    from geopy.geocoders import Nominatim\n",
    "    from geopy.extra.rate_limiter import RateLimiter\n",
    "    geolocator = Nominatim(user_agent=\"pools_enrichment/1.0 (contact: your_email@example.com)\")\n",
    "    reverse = RateLimiter(geolocator.reverse, min_delay_seconds=1.0, max_retries=2, error_wait_seconds=2.0, swallow_exceptions=True)\n",
    "    for idx, r in need.iterrows():\n",
    "        if not is_blank(df.at[idx, \"street\"]): \n",
    "            continue\n",
    "        loc = reverse((r[lat_col], r[lon_col]), exactly_one=True, addressdetails=True, language=\"de\", zoom=19)\n",
    "        if not loc: \n",
    "            continue\n",
    "        ad = (loc.raw.get(\"address\") if hasattr(loc, \"raw\") else {}) or {}\n",
    "        road = ad.get(\"road\") or ad.get(\"pedestrian\") or ad.get(\"footway\") or ad.get(\"path\") or \"\"\n",
    "        hn   = ad.get(\"house_number\") or \"\"\n",
    "        if road:\n",
    "            df.at[idx, \"street\"] = f\"{road} {hn}\".strip()\n",
    "        elif \"display_name\" in loc.raw:\n",
    "            pick = street_like_piece(loc.raw[\"display_name\"])\n",
    "            if pick:\n",
    "                df.at[idx, \"street\"] = pick\n",
    "except Exception as e:\n",
    "    print(\"[street-fill] Nominatim extra pass skipped:\", repr(e))\n",
    "\n",
    "# 2) Nearest named road via OSMnx (Overpass) â€” robust fallback\n",
    "still = df[df[\"street\"].apply(is_blank) & df[lat_col].notna() & df[lon_col].notna()].copy()\n",
    "print(f\"[street-fill] still missing after Nominatim: {len(still)}\")\n",
    "\n",
    "# Load / init cache for nearest roads\n",
    "if NEAREST_ROAD_CACHE.exists():\n",
    "    road_cache = pd.read_csv(NEAREST_ROAD_CACHE)\n",
    "else:\n",
    "    road_cache = pd.DataFrame(columns=[\"lat\",\"lon\",\"street\",\"key\"])\n",
    "road_cache[\"key\"] = road_cache.apply(lambda r: key(r.get(\"lat\"), r.get(\"lon\")), axis=1)\n",
    "road_cache_dict = {k: s for k, s in zip(road_cache[\"key\"], road_cache[\"street\"]) if pd.notna(k)}\n",
    "\n",
    "def nearest_road_name(lat, lon):\n",
    "    k = key(lat, lon)\n",
    "    if k in road_cache_dict and str(road_cache_dict[k]).strip():\n",
    "        return str(road_cache_dict[k])\n",
    "    try:\n",
    "        import osmnx as ox\n",
    "        # version-agnostic graph builder\n",
    "        if hasattr(ox, \"graph_from_point\"):\n",
    "            G = ox.graph_from_point((lat, lon), dist=250, network_type=\"all_private\", retain_all=True, simplify=True)\n",
    "        else:\n",
    "            # very old OSMnx not supported\n",
    "            return \"\"\n",
    "        # find nearest edge and read its name\n",
    "        try:\n",
    "            from osmnx import distance as oxdist\n",
    "            u, v, kkey = oxdist.nearest_edges(G, X=[float(lon)], Y=[float(lat)])[0]\n",
    "            data = G.get_edge_data(u, v, kkey) or {}\n",
    "        except Exception:\n",
    "            # fallback via nearest node and its incident edges\n",
    "            n = ox.distance.nearest_nodes(G, float(lon), float(lat))\n",
    "            data = next(iter(G[n][list(G[n])[0]].values()), {})\n",
    "        name = data.get(\"name\")\n",
    "        if isinstance(name, list):  # sometimes multiple names\n",
    "            name = name[0] if name else \"\"\n",
    "        if name:\n",
    "            road_cache_dict[k] = name\n",
    "            return str(name)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    return \"\"\n",
    "\n",
    "filled_rows = []\n",
    "for idx, r in still.iterrows():\n",
    "    nm = nearest_road_name(r[lat_col], r[lon_col])\n",
    "    if nm:\n",
    "        df.at[idx, \"street\"] = nm\n",
    "        k = key(r[lat_col], r[lon_col])\n",
    "        if k:\n",
    "            filled_rows.append({\"lat\": k[0], \"lon\": k[1], \"street\": nm, \"key\": k})\n",
    "\n",
    "# persist nearest-road cache\n",
    "if filled_rows:\n",
    "    nc = pd.DataFrame(filled_rows)\n",
    "    if not road_cache.empty:\n",
    "        road_cache = pd.concat([road_cache, nc], ignore_index=True)\n",
    "    else:\n",
    "        road_cache = nc\n",
    "    road_cache.drop_duplicates(subset=[\"key\"], keep=\"last\").to_csv(NEAREST_ROAD_CACHE, index=False)\n",
    "\n",
    "# Save and show Sommerbad\n",
    "df.to_csv(OSM_PUBLIC_NAMED_CSV, index=False)\n",
    "print(df.loc[df[\"name\"].str.contains(\"Sommerbad\", case=False, na=False), [\"name\",\"street\",\"postal_code\",\"district\"]])\n",
    "print(\"[ok] street backfilled where possible\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713ef3b4",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Build **pools_master_minimal.csv**\n",
    "\n",
    "**Why:** consolidate legacy + OSM-only rows into one minimal master table with consistent IDs & districts.\n",
    "\n",
    "**What we ensure:**\n",
    "- **pool_id** present for every row (legacy preserved; OSM generated robustly).\n",
    "- **district_id** filled from district name mapping; if missing, reverseâ€‘geocoded from coordinates (cached).\n",
    "- `open_all_year` filled with `False` where empty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0ac23053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] Wrote pools_master_minimal.csv with 168 rows\n"
     ]
    }
   ],
   "source": [
    "# === Step 6 â€” Build final master (robust) ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# reload inputs\n",
    "legacy = pd.read_csv(LEGACY_MAIN_CSV if LEGACY_MAIN_CSV.exists() else LEGACY_ALT_CSV)\n",
    "enrich = pd.read_csv(\"legacy_enrichment_list.csv\")\n",
    "osm_new = pd.read_csv(OSM_PUBLIC_NAMED_CSV)\n",
    "\n",
    "def pickcol(df, *options):\n",
    "    for c in options:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "# --- district helpers\n",
    "district_mapping = {\n",
    "    'Mitte': '01', 'Friedrichshain-Kreuzberg': '02', 'Pankow': '03',\n",
    "    'Charlottenburg-Wilmersdorf': '04', 'Spandau': '05', 'Steglitz-Zehlendorf': '06',\n",
    "    'Tempelhof-SchÃ¶neberg': '07', 'NeukÃ¶lln': '08', 'Treptow-KÃ¶penick': '09',\n",
    "    'Marzahn-Hellersdorf': '10', 'Lichtenberg': '11', 'Reinickendorf': '12'\n",
    "}\n",
    "\n",
    "import unicodedata, re\n",
    "def strip_accents(s: str) -> str:\n",
    "    s = \"\" if s is None else str(s)\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    return \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "\n",
    "def canonical_district(raw: str) -> str:\n",
    "    if not isinstance(raw, str) or not raw.strip():\n",
    "        return \"\"\n",
    "    s = strip_accents(raw).strip()\n",
    "    s = s.replace(\"Mitte (Berlin)\", \"Mitte\")\n",
    "    s = s.replace(\"Friedrichshain - Kreuzberg\", \"Friedrichshain-Kreuzberg\")\n",
    "    s = s.replace(\"Charlottenburg - Wilmersdorf\", \"Charlottenburg-Wilmersdorf\")\n",
    "    s = s.replace(\"Steglitz - Zehlendorf\", \"Steglitz-Zehlendorf\")\n",
    "    s = s.replace(\"Tempelhof - Schoneberg\", \"Tempelhof-SchÃ¶neberg\").replace(\"Tempelhof - SchÃ¶neberg\", \"Tempelhof-SchÃ¶neberg\")\n",
    "    s = s.replace(\"Treptow - Kopenick\", \"Treptow-KÃ¶penick\")\n",
    "    s = s.replace(\"Marzahn - Hellersdorf\", \"Marzahn-Hellersdorf\")\n",
    "    s = re.sub(r\"^(Bezirk|District)\\s+\", \"\", s, flags=re.I)\n",
    "    s = re.sub(r\"\\s*-\\s*\", \"-\", s).strip()\n",
    "    if s in district_mapping:\n",
    "        return s\n",
    "    # very light fuzzy fallback\n",
    "    toks = set(s.lower().replace(\"-\", \" \").split())\n",
    "    best, overlap = \"\", 0\n",
    "    for official in district_mapping:\n",
    "        otoks = set(official.lower().replace(\"-\", \" \").split())\n",
    "        ov = len(toks & otoks)\n",
    "        if ov > overlap:\n",
    "            best, overlap = official, ov\n",
    "    return best if overlap else \"\"\n",
    "\n",
    "# --- name normalization (for enrichment) ---\n",
    "def normalize_name(s: str) -> str:\n",
    "    s = \"\" if pd.isna(s) else str(s)\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s.lower()).strip()\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "legacy_name_col = pickcol(legacy, \"name\", \"pool_name\") or \"name\"\n",
    "legacy[\"_name_norm\"] = legacy[legacy_name_col].astype(str).map(normalize_name)\n",
    "\n",
    "enrich_name_col = pickcol(enrich, \"name_l\") or \"name_l\"\n",
    "enrich[\"_name_norm\"] = enrich[enrich_name_col].astype(str).map(normalize_name)\n",
    "\n",
    "# take a few OSM attrs to enrich legacy (fill if legacy is blank)\n",
    "osm_add_cols = [c for c in [\"website_o\",\"opening_hours_o\",\"wheelchair_o\",\"phone_o\"] if c in enrich.columns]\n",
    "enrich_small  = enrich[[\"_name_norm\"] + osm_add_cols].drop_duplicates(\"_name_norm\")\n",
    "legacy_enriched = legacy.merge(enrich_small, on=\"_name_norm\", how=\"left\")\n",
    "\n",
    "for lcol, ocol in {\"website\":\"website_o\",\"opening_hours\":\"opening_hours_o\",\"wheelchair\":\"wheelchair_o\",\"phone\":\"phone_o\"}.items():\n",
    "    if lcol in legacy_enriched.columns and ocol in legacy_enriched.columns:\n",
    "        legacy_enriched[lcol] = legacy_enriched[lcol].where(\n",
    "            legacy_enriched[lcol].notna() & (legacy_enriched[lcol].astype(str).str.strip() != \"\"),\n",
    "            legacy_enriched[ocol]\n",
    "        )\n",
    "legacy_enriched.drop(columns=[c for c in [\"_name_norm\",\"website_o\",\"opening_hours_o\",\"wheelchair_o\",\"phone_o\"] if c in legacy_enriched.columns],\n",
    "                     inplace=True, errors=\"ignore\")\n",
    "\n",
    "# --- OSM rows â†’ final schema ---\n",
    "def make_osm_pool_id(df):\n",
    "    id_col = pickcol(df, \"source_id\",\"osm_id\",\"element_id\",\"osmid\")\n",
    "    if id_col:\n",
    "        base = df[id_col].fillna(\"\").astype(str).str.strip()\n",
    "    else:\n",
    "        base = pd.Series([\"\"] * len(df), index=df.index)\n",
    "    latc = pickcol(df, \"lat\",\"latitude\")\n",
    "    lonc = pickcol(df, \"lon\",\"longitude\")\n",
    "    if latc and lonc:\n",
    "        coord_sur = df[latc].round(6).astype(str) + \"_\" + df[lonc].round(6).astype(str)\n",
    "    else:\n",
    "        coord_sur = pd.Series([f\"{i:05d}\" for i in range(len(df))], index=df.index)\n",
    "    base = np.where(base == \"\", coord_sur, base)\n",
    "    return pd.Series(\"OSM_\" + pd.Series(base), index=df.index)\n",
    "\n",
    "osm_final = pd.DataFrame({\n",
    "    \"pool_id\":       make_osm_pool_id(osm_new),\n",
    "    \"district\":      osm_new[pickcol(osm_new, \"district\")] if pickcol(osm_new, \"district\") else \"\",\n",
    "    \"district_id\":   osm_new[pickcol(osm_new, \"district_id\")] if pickcol(osm_new, \"district_id\") else \"\",\n",
    "    \"name\":          osm_new[pickcol(osm_new, \"name\")] if pickcol(osm_new, \"name\") else \"\",\n",
    "    \"pool_type\":     osm_new[pickcol(osm_new, \"pool_type\",\"leisure\",\"sport\")] if pickcol(osm_new, \"pool_type\",\"leisure\",\"sport\") else \"\",\n",
    "    \"street\":        osm_new[pickcol(osm_new, \"street\",\"addr_street\",\"address\")] if pickcol(osm_new, \"street\",\"addr_street\",\"address\") else \"\",\n",
    "    \"postal_code\":   osm_new[pickcol(osm_new, \"postal_code\",\"addr_postcode\")] if pickcol(osm_new, \"postal_code\",\"addr_postcode\") else \"\",\n",
    "    \"latitude\":      pd.to_numeric(osm_new[pickcol(osm_new, \"lat\",\"latitude\")], errors=\"coerce\") if pickcol(osm_new, \"lat\",\"latitude\") else pd.NA,\n",
    "    \"longitude\":     pd.to_numeric(osm_new[pickcol(osm_new, \"lon\",\"longitude\")], errors=\"coerce\") if pickcol(osm_new, \"lon\",\"longitude\") else pd.NA,\n",
    "    \"open_all_year\": pd.NA,\n",
    "})\n",
    "\n",
    "# --- Legacy rows â†’ final schema ---\n",
    "legacy_final = pd.DataFrame({\n",
    "    \"pool_id\":     legacy_enriched[pickcol(legacy_enriched, \"pool_id\",\"id\",\"legacy_id\")] if pickcol(legacy_enriched, \"pool_id\",\"id\",\"legacy_id\") else (\"L_\" + legacy_enriched.index.astype(str)),\n",
    "    \"district\":    legacy_enriched[pickcol(legacy_enriched, \"district\")] if pickcol(legacy_enriched, \"district\") else \"\",\n",
    "    \"district_id\": legacy_enriched[pickcol(legacy_enriched, \"district_id\")] if pickcol(legacy_enriched, \"district_id\") else \"\",\n",
    "    \"name\":        legacy_enriched[pickcol(legacy_enriched, \"name\",\"pool_name\")] if pickcol(legacy_enriched, \"name\",\"pool_name\") else \"\",\n",
    "    \"pool_type\":   legacy_enriched[pickcol(legacy_enriched, \"pool_type\",\"type\")] if pickcol(legacy_enriched, \"pool_type\",\"type\") else \"\",\n",
    "    \"street\":      legacy_enriched[pickcol(legacy_enriched, \"street\",\"address\")] if pickcol(legacy_enriched, \"street\",\"address\") else \"\",\n",
    "    \"postal_code\": legacy_enriched[pickcol(legacy_enriched, \"postal_code\",\"postcode\",\"zip\")] if pickcol(legacy_enriched, \"postal_code\",\"postcode\",\"zip\") else \"\",\n",
    "    \"latitude\":    pd.to_numeric(legacy_enriched[pickcol(legacy_enriched, \"latitude\",\"lat\")], errors=\"coerce\") if pickcol(legacy_enriched, \"latitude\",\"lat\") else pd.NA,\n",
    "    \"longitude\":   pd.to_numeric(legacy_enriched[pickcol(legacy_enriched, \"longitude\",\"lon\")], errors=\"coerce\") if pickcol(legacy_enriched, \"longitude\",\"lon\") else pd.NA,\n",
    "    \"open_all_year\": legacy_enriched[pickcol(legacy_enriched, \"open_all_year\",\"open_all_year_round\",\"open_year_round\")] if pickcol(legacy_enriched, \"open_all_year\",\"open_all_year_round\",\"open_year_round\") else pd.NA,\n",
    "})\n",
    "\n",
    "# --- combine\n",
    "final_master = pd.concat([legacy_final, osm_final], ignore_index=True)\n",
    "\n",
    "# --- ensure dtypes\n",
    "for c in [\"pool_id\",\"district\",\"district_id\",\"name\",\"pool_type\",\"street\",\"postal_code\"]:\n",
    "    if c in final_master.columns:\n",
    "        final_master[c] = final_master[c].astype(\"string\")\n",
    "for c in [\"latitude\",\"longitude\"]:\n",
    "    if c in final_master.columns:\n",
    "        final_master[c] = pd.to_numeric(final_master[c], errors=\"coerce\")\n",
    "\n",
    "# --- fill district via cache-aware reverse geocoding if needed\n",
    "def key(lat, lon):\n",
    "    try:\n",
    "        return round(float(lat), 6), round(float(lon), 6)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "need = (final_master[\"district_id\"].isna() | final_master[\"district_id\"].astype(str).str.strip().eq(\"\")) & \\\n",
    "       final_master[\"latitude\"].notna() & final_master[\"longitude\"].notna()\n",
    "\n",
    "if need.any():\n",
    "    cache_cols = [\"lat\",\"lon\",\"street\",\"postal_code\",\"district\"]\n",
    "    cache = pd.read_csv(CACHE_PATH) if CACHE_PATH.exists() else pd.DataFrame(columns=cache_cols)\n",
    "    if \"district\" not in cache.columns:\n",
    "        cache[\"district\"] = \"\"\n",
    "    cache[\"key\"] = cache.apply(lambda r: key(r.get(\"lat\"), r.get(\"lon\")), axis=1)\n",
    "    cache_dict = {k: d for k, d in zip(cache[\"key\"], cache[\"district\"]) if pd.notna(k)}\n",
    "\n",
    "    from geopy.geocoders import Nominatim\n",
    "    from geopy.extra.rate_limiter import RateLimiter\n",
    "    geolocator = Nominatim(user_agent=\"pools_enrichment/1.0 (contact: your_email@example.com)\")\n",
    "    reverse = RateLimiter(geolocator.reverse, min_delay_seconds=1.0)\n",
    "\n",
    "    filled = []\n",
    "    for idx, row in final_master[need].iterrows():\n",
    "        k = key(row[\"latitude\"], row[\"longitude\"])\n",
    "        dname = cache_dict.get(k, \"\")\n",
    "        if not dname:\n",
    "            try:\n",
    "                loc = reverse((row[\"latitude\"], row[\"longitude\"]), exactly_one=True, addressdetails=True, language=\"de\")\n",
    "                ad = (loc.raw.get(\"address\") if loc and hasattr(loc, \"raw\") else {}) or {}\n",
    "                raw = ad.get(\"city_district\") or ad.get(\"borough\") or ad.get(\"county\") or \"\"\n",
    "                dname = canonical_district(raw)\n",
    "                if k:\n",
    "                    filled.append({\"lat\": k[0], \"lon\": k[1], \"street\": \"\", \"postal_code\": \"\", \"district\": dname, \"key\": k})\n",
    "            except Exception:\n",
    "                dname = \"\"\n",
    "        if dname:\n",
    "            final_master.at[idx, \"district\"] = dname\n",
    "\n",
    "    if filled:\n",
    "        cache = pd.concat([cache, pd.DataFrame(filled)], ignore_index=True)\n",
    "        cache = cache.drop_duplicates(subset=[\"key\"], keep=\"last\")\n",
    "        cache[cache_cols].to_csv(CACHE_PATH, index=False)\n",
    "\n",
    "# map district â†’ id (pad to 2 chars)\n",
    "need_id = final_master[\"district_id\"].isna() | final_master[\"district_id\"].astype(str).str.strip().eq(\"\")\n",
    "final_master.loc[need_id, \"district_id\"] = final_master.loc[need_id, \"district\"].map(district_mapping).astype(\"string\")\n",
    "final_master[\"district_id\"] = final_master[\"district_id\"].fillna(\"\").str.replace(\".0\",\"\",regex=False)\n",
    "mask = final_master[\"district_id\"].ne(\"\")\n",
    "final_master.loc[mask, \"district_id\"] = final_master.loc[mask, \"district_id\"].str.zfill(2)\n",
    "\n",
    "# --- open_all_year: coerce & fill blanks â†’ False (boolean)\n",
    "if \"open_all_year\" in final_master.columns:\n",
    "    s = final_master[\"open_all_year\"].astype(str).str.strip().str.lower()\n",
    "    trueish  = {\"true\",\"1\",\"yes\",\"y\",\"ja\"}\n",
    "    falseish = {\"false\",\"0\",\"no\",\"n\",\"nein\",\"\"}\n",
    "    final_master[\"open_all_year\"] = s.map(lambda x: True if x in trueish else (False if x in falseish else False))\n",
    "\n",
    "# --- ensure unique pool_id (keep first if accidental dupes)\n",
    "dup_ct = int(final_master[\"pool_id\"].duplicated().sum())\n",
    "if dup_ct:\n",
    "    print(f\"[warn] duplicate pool_id found: {dup_ct} â†’ keeping first\")\n",
    "    final_master = final_master.drop_duplicates(subset=[\"pool_id\"], keep=\"first\")\n",
    "\n",
    "# --- guarantee final column order & save\n",
    "final_master = final_master[[\n",
    "    \"pool_id\",\"district_id\",\"name\",\"pool_type\",\"street\",\n",
    "    \"postal_code\",\"latitude\",\"longitude\",\"open_all_year\"\n",
    "]]\n",
    "final_master.to_csv(\"pools_master_minimal.csv\", index=False)\n",
    "print(\"[ok] Wrote pools_master_minimal.csv with\", len(final_master), \"rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2874a895",
   "metadata": {},
   "source": [
    "Final tweek for pool_id column - OSM does not provide it - decided to use numbers 1-400 as the first entry from legacy tool has id number : 472"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d9b5b920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] Replaced 24 surrogate ids. New range: (1, 24)\n",
      "Examples: {'OSM_sur_12': '1', 'OSM_sur_30': '2', 'OSM_sur_122': '3', 'OSM_sur_125': '4', 'OSM_sur_127': '5'}\n"
     ]
    }
   ],
   "source": [
    "# Replace surrogate IDs like \"OSM_sur_123\" with consecutive numbers \"1\", \"2\", ...\n",
    "import pandas as pd, re\n",
    "\n",
    "PATH = \"pools_master_minimal.csv\"\n",
    "\n",
    "df = pd.read_csv(PATH, dtype={\"pool_id\": \"string\"})\n",
    "\n",
    "# rows with OSM surrogate ids\n",
    "mask = df[\"pool_id\"].astype(str).str.fullmatch(r\"OSM_sur_\\d+\")\n",
    "sur_ids = df.loc[mask, \"pool_id\"].astype(str).unique().tolist()\n",
    "\n",
    "# order by numeric suffix so mapping is stable/predictable\n",
    "def sur_num(s): \n",
    "    m = re.search(r\"(\\d+)$\", s)\n",
    "    return int(m.group(1)) if m else 10**9\n",
    "\n",
    "ordered = sorted(sur_ids, key=sur_num)\n",
    "\n",
    "# provisional mapping: OSM_sur_* -> \"1\", \"2\", ...\n",
    "proposed = [str(i+1) for i in range(len(ordered))]\n",
    "\n",
    "# avoid collision with any existing non-surrogate pool_id\n",
    "existing = set(df.loc[~mask, \"pool_id\"].astype(str))\n",
    "start = 1\n",
    "while any(str(start + i) in existing for i in range(len(ordered))):\n",
    "    start += 1  # bump start until there are no collisions\n",
    "\n",
    "mapping = {sid: str(start + i) for i, sid in enumerate(ordered)}\n",
    "\n",
    "# (Optional) sanity: if you *really* want to cap at 400, uncomment next 2 lines\n",
    "# if len(mapping) > 400:\n",
    "#     print(f\"[warn] {len(mapping)} surrogate ids > 400; numbering will continue beyond 400.\")\n",
    "\n",
    "# apply mapping\n",
    "df.loc[mask, \"pool_id\"] = df.loc[mask, \"pool_id\"].map(mapping)\n",
    "\n",
    "# keep pool_id as string/object\n",
    "df[\"pool_id\"] = df[\"pool_id\"].astype(\"string\")\n",
    "\n",
    "df.to_csv(PATH, index=False)\n",
    "\n",
    "# small summary\n",
    "new_vals = list(mapping.values())\n",
    "print(f\"[ok] Replaced {len(mapping)} surrogate ids. New range:\",\n",
    "      (min(map(int, new_vals)), max(map(int, new_vals))) if new_vals else \"n/a\")\n",
    "print(\"Examples:\", dict(list(mapping.items())[:5]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7e96ae",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Validation & Outputs\n",
    "\n",
    "**Why:** quick checks to ensure core fields are present and sensible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3ac75d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 168\n",
      "Missing pool_id: 0\n",
      "Missing district_id: 0\n",
      "Duplicate pool_id: 0\n",
      "Invalid district_id format (not 01â€“12): 138\n",
      "Suspicious postal_code (non-empty but not 10xxxâ€“14xxx): 0\n",
      "Missing latitude: 0 | Missing longitude: 0\n",
      "Out-of-range latitude: 0 | Out-of-range longitude: 0\n",
      "\n",
      "open_all_year dtype: bool\n",
      "open_all_year\n",
      "False    89\n",
      "True     79\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Rows with invalid district_id (showing up to 5):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pool_id</th>\n",
       "      <th>district_id</th>\n",
       "      <th>name</th>\n",
       "      <th>street</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>open_all_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>473</td>\n",
       "      <td>9</td>\n",
       "      <td>Kleine Schwimmhalle Wuhlheide</td>\n",
       "      <td>An der Wuhlheide 161</td>\n",
       "      <td>12459</td>\n",
       "      <td>52.45993</td>\n",
       "      <td>13.53965</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>474</td>\n",
       "      <td>8</td>\n",
       "      <td>Kombibad Mariendorf</td>\n",
       "      <td>Ankogelweg 95</td>\n",
       "      <td>12107</td>\n",
       "      <td>52.41972</td>\n",
       "      <td>13.40154</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>476</td>\n",
       "      <td>2</td>\n",
       "      <td>Stadtbad Kreuzberg - Baerwaldbad</td>\n",
       "      <td>BaerwaldstraÃŸe 64-67</td>\n",
       "      <td>10961</td>\n",
       "      <td>52.49451</td>\n",
       "      <td>13.40432</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>477</td>\n",
       "      <td>3</td>\n",
       "      <td>Strandbad am WeiÃŸen See</td>\n",
       "      <td>Berliner Allee 155</td>\n",
       "      <td>13086</td>\n",
       "      <td>52.55396</td>\n",
       "      <td>13.46583</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>478</td>\n",
       "      <td>5</td>\n",
       "      <td>Sommerbad Staaken-West</td>\n",
       "      <td>BrunsbÃ¼ttler Damm 443</td>\n",
       "      <td>13591</td>\n",
       "      <td>52.53386</td>\n",
       "      <td>13.13123</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pool_id  district_id                              name  \\\n",
       "1      473            9     Kleine Schwimmhalle Wuhlheide   \n",
       "2      474            8               Kombibad Mariendorf   \n",
       "4      476            2  Stadtbad Kreuzberg - Baerwaldbad   \n",
       "5      477            3           Strandbad am WeiÃŸen See   \n",
       "6      478            5            Sommerbad Staaken-West   \n",
       "\n",
       "                  street  postal_code  latitude  longitude  open_all_year  \n",
       "1   An der Wuhlheide 161        12459  52.45993   13.53965           True  \n",
       "2          Ankogelweg 95        12107  52.41972   13.40154           True  \n",
       "4   BaerwaldstraÃŸe 64-67        10961  52.49451   13.40432           True  \n",
       "5     Berliner Allee 155        13086  52.55396   13.46583          False  \n",
       "6  BrunsbÃ¼ttler Damm 443        13591  52.53386   13.13123          False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample (top 10):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pool_id</th>\n",
       "      <th>district_id</th>\n",
       "      <th>name</th>\n",
       "      <th>pool_type</th>\n",
       "      <th>street</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>open_all_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>472</td>\n",
       "      <td>12</td>\n",
       "      <td>Strandbad LÃ¼bars</td>\n",
       "      <td>Naturbad</td>\n",
       "      <td>Am Freibad 9</td>\n",
       "      <td>13469</td>\n",
       "      <td>52.61824</td>\n",
       "      <td>13.33519</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>473</td>\n",
       "      <td>9</td>\n",
       "      <td>Kleine Schwimmhalle Wuhlheide</td>\n",
       "      <td>Hallenbad</td>\n",
       "      <td>An der Wuhlheide 161</td>\n",
       "      <td>12459</td>\n",
       "      <td>52.45993</td>\n",
       "      <td>13.53965</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>474</td>\n",
       "      <td>8</td>\n",
       "      <td>Kombibad Mariendorf</td>\n",
       "      <td>Kombibad</td>\n",
       "      <td>Ankogelweg 95</td>\n",
       "      <td>12107</td>\n",
       "      <td>52.41972</td>\n",
       "      <td>13.40154</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>475</td>\n",
       "      <td>11</td>\n",
       "      <td>Schwimmhalle Anton-Saefkow-Platz</td>\n",
       "      <td>Hallenbad</td>\n",
       "      <td>Anton-Saefkow-Platz 1</td>\n",
       "      <td>10369</td>\n",
       "      <td>52.53093</td>\n",
       "      <td>13.47184</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>476</td>\n",
       "      <td>2</td>\n",
       "      <td>Stadtbad Kreuzberg - Baerwaldbad</td>\n",
       "      <td>Hallenbad</td>\n",
       "      <td>BaerwaldstraÃŸe 64-67</td>\n",
       "      <td>10961</td>\n",
       "      <td>52.49451</td>\n",
       "      <td>13.40432</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>477</td>\n",
       "      <td>3</td>\n",
       "      <td>Strandbad am WeiÃŸen See</td>\n",
       "      <td>Naturbad</td>\n",
       "      <td>Berliner Allee 155</td>\n",
       "      <td>13086</td>\n",
       "      <td>52.55396</td>\n",
       "      <td>13.46583</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>478</td>\n",
       "      <td>5</td>\n",
       "      <td>Sommerbad Staaken-West</td>\n",
       "      <td>Freibad</td>\n",
       "      <td>BrunsbÃ¼ttler Damm 443</td>\n",
       "      <td>13591</td>\n",
       "      <td>52.53386</td>\n",
       "      <td>13.13123</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>479</td>\n",
       "      <td>10</td>\n",
       "      <td>Schwimmhalle Kaulsdorf</td>\n",
       "      <td>Schulbad</td>\n",
       "      <td>Clara-Zetkin-Weg 13</td>\n",
       "      <td>12619</td>\n",
       "      <td>52.52080</td>\n",
       "      <td>13.58541</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>480</td>\n",
       "      <td>8</td>\n",
       "      <td>Sommerbad NeukÃ¶lln</td>\n",
       "      <td>Freibad</td>\n",
       "      <td>Columbiadamm 160-180</td>\n",
       "      <td>10965</td>\n",
       "      <td>52.48025</td>\n",
       "      <td>13.41595</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>481</td>\n",
       "      <td>6</td>\n",
       "      <td>Schwimmhalle Finckensteinallee</td>\n",
       "      <td>Hallenbad</td>\n",
       "      <td>Finckensteinallee 73</td>\n",
       "      <td>12205</td>\n",
       "      <td>52.43225</td>\n",
       "      <td>13.29791</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pool_id  district_id                              name  pool_type  \\\n",
       "0      472           12                  Strandbad LÃ¼bars   Naturbad   \n",
       "1      473            9     Kleine Schwimmhalle Wuhlheide  Hallenbad   \n",
       "2      474            8               Kombibad Mariendorf   Kombibad   \n",
       "3      475           11  Schwimmhalle Anton-Saefkow-Platz  Hallenbad   \n",
       "4      476            2  Stadtbad Kreuzberg - Baerwaldbad  Hallenbad   \n",
       "5      477            3           Strandbad am WeiÃŸen See   Naturbad   \n",
       "6      478            5            Sommerbad Staaken-West    Freibad   \n",
       "7      479           10            Schwimmhalle Kaulsdorf   Schulbad   \n",
       "8      480            8                Sommerbad NeukÃ¶lln    Freibad   \n",
       "9      481            6    Schwimmhalle Finckensteinallee  Hallenbad   \n",
       "\n",
       "                  street  postal_code  latitude  longitude  open_all_year  \n",
       "0           Am Freibad 9        13469  52.61824   13.33519          False  \n",
       "1   An der Wuhlheide 161        12459  52.45993   13.53965           True  \n",
       "2          Ankogelweg 95        12107  52.41972   13.40154           True  \n",
       "3  Anton-Saefkow-Platz 1        10369  52.53093   13.47184           True  \n",
       "4   BaerwaldstraÃŸe 64-67        10961  52.49451   13.40432           True  \n",
       "5     Berliner Allee 155        13086  52.55396   13.46583          False  \n",
       "6  BrunsbÃ¼ttler Damm 443        13591  52.53386   13.13123          False  \n",
       "7    Clara-Zetkin-Weg 13        12619  52.52080   13.58541           True  \n",
       "8   Columbiadamm 160-180        10965  52.48025   13.41595          False  \n",
       "9   Finckensteinallee 73        12205  52.43225   13.29791          False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Step 7 â€” Quick QC on pools_master_minimal.csv ===\n",
    "import pandas as pd, numpy as np, re\n",
    "\n",
    "df = pd.read_csv(\"pools_master_minimal.csv\")\n",
    "\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Missing pool_id:\", int(df[\"pool_id\"].astype(str).str.strip().eq(\"\").sum()))\n",
    "print(\"Missing district_id:\", int(df[\"district_id\"].astype(str).str.strip().eq(\"\").sum()))\n",
    "\n",
    "# --- basic uniqueness & formatting checks\n",
    "dupe_ids = int(df[\"pool_id\"].duplicated().sum())\n",
    "print(\"Duplicate pool_id:\", dupe_ids)\n",
    "\n",
    "# district_id must be 01..12 (2 digits)\n",
    "dist_pat = r\"(0[1-9]|1[0-2])\"\n",
    "bad_dist_mask = ~df[\"district_id\"].astype(str).str.fullmatch(dist_pat)\n",
    "bad_dist_count = int(bad_dist_mask.sum())\n",
    "print(\"Invalid district_id format (not 01â€“12):\", bad_dist_count)\n",
    "\n",
    "# postal code: Berlin 10xxxâ€“14xxx (allow empty)\n",
    "pc = df[\"postal_code\"].astype(str).str.strip()\n",
    "bad_pc_mask = (pc != \"\") & ~pc.str.fullmatch(r\"1[0-4]\\d{3}\")\n",
    "print(\"Suspicious postal_code (non-empty but not 10xxxâ€“14xxx):\", int(bad_pc_mask.sum()))\n",
    "\n",
    "# coordinates sanity\n",
    "lat = pd.to_numeric(df[\"latitude\"], errors=\"coerce\")\n",
    "lon = pd.to_numeric(df[\"longitude\"], errors=\"coerce\")\n",
    "missing_lat = int(lat.isna().sum())\n",
    "missing_lon = int(lon.isna().sum())\n",
    "out_lat = int((~lat.between(-90, 90)).sum())\n",
    "out_lon = int((~lon.between(-180, 180)).sum())\n",
    "print(\"Missing latitude:\", missing_lat, \"| Missing longitude:\", missing_lon)\n",
    "print(\"Out-of-range latitude:\", out_lat, \"| Out-of-range longitude:\", out_lon)\n",
    "\n",
    "# open_all_year should be boolean (from Step 6). Show distribution.\n",
    "print(\"\\nopen_all_year dtype:\", df[\"open_all_year\"].dtype)\n",
    "print(df[\"open_all_year\"].value_counts(dropna=False))\n",
    "\n",
    "# --- Peek at problems (up to 5 rows each) ---\n",
    "def peek(mask, cols, title):\n",
    "    m = df[mask]\n",
    "    if not m.empty:\n",
    "        print(f\"\\n{title} (showing up to 5):\")\n",
    "        display(m[cols].head(5))\n",
    "\n",
    "core_cols = [\"pool_id\",\"district_id\",\"name\",\"street\",\"postal_code\",\"latitude\",\"longitude\",\"open_all_year\"]\n",
    "\n",
    "peek(df[\"pool_id\"].astype(str).str.strip().eq(\"\"), core_cols, \"Rows with missing pool_id\")\n",
    "peek(bad_dist_mask, core_cols, \"Rows with invalid district_id\")\n",
    "peek(bad_pc_mask, core_cols, \"Rows with suspicious postal_code\")\n",
    "peek(lat.isna() | lon.isna(), core_cols, \"Rows with missing coordinates\")\n",
    "peek((~lat.between(-90, 90)) | (~lon.between(-180, 180)), core_cols, \"Rows with out-of-range coordinates\")\n",
    "\n",
    "print(\"\\nSample (top 10):\")\n",
    "display(df.head(10))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a134d50c",
   "metadata": {},
   "source": [
    "# === Finalize schema & save (from CSV) ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a4aa040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pool_id           object\n",
      "name              object\n",
      "pool_type         object\n",
      "street            object\n",
      "postal_code       object\n",
      "latitude         float64\n",
      "longitude        float64\n",
      "open_all_year     object\n",
      "district_id       object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"pools_master_minimal.csv\")\n",
    "\n",
    "need_cols = [\"pool_id\",\"name\",\"pool_type\",\"street\",\"postal_code\",\n",
    "             \"latitude\",\"longitude\",\"open_all_year\",\"district_id\"]\n",
    "for c in need_cols:\n",
    "    if c not in df.columns:\n",
    "        df[c] = \"\" if c not in [\"latitude\",\"longitude\"] else pd.NA\n",
    "\n",
    "df[\"latitude\"]  = pd.to_numeric(df[\"latitude\"], errors=\"coerce\").astype(\"float64\")\n",
    "df[\"longitude\"] = pd.to_numeric(df[\"longitude\"], errors=\"coerce\").astype(\"float64\")\n",
    "\n",
    "obj_cols = [\"pool_id\",\"name\",\"pool_type\",\"street\",\"postal_code\",\"open_all_year\",\"district_id\"]\n",
    "for c in obj_cols:\n",
    "    df[c] = df[c].astype(str).fillna(\"\").astype(\"object\")\n",
    "\n",
    "df.loc[df[\"open_all_year\"].astype(str).str.strip().eq(\"\"), \"open_all_year\"] = \"False\"\n",
    "\n",
    "df = df[need_cols]\n",
    "df.to_csv(\"pools_master_minimal.csv\", index=False)\n",
    "\n",
    "print(df.dtypes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
