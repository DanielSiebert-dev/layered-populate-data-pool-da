{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a21a0aa",
   "metadata": {},
   "source": [
    "# üèä‚Äç‚ôÄÔ∏è Berlin Pools ‚Äî End-to-End Pipeline (Refactored)\n",
    "\n",
    "**Purpose:** run this notebook top-to-bottom to build a unified dataset of Berlin pools from legacy and OSM sources, **adding**:\n",
    "\n",
    "---\n",
    "\n",
    "## What this notebook does\n",
    "\n",
    "1) **Environment & Config**  \n",
    "   - Auto-installs minimal deps (pandas, geopandas, shapely, etc.).  \n",
    "   - Sets input/output paths & simple toggles.  \n",
    "   - Declares the **LOR GeoJSON** path and the 8-digit **district_id** mapping.\n",
    "\n",
    "2) **Run legacy extractor (if needed)**  \n",
    "   - Executes `pool_data_processing.ipynb` to reproduce:  \n",
    "     - `berlin_pools_final_dataset.csv`  \n",
    "     - `pools_data_cleaned.csv`\n",
    "\n",
    "3) **Load LOR polygons & enrich Legacy with districts/ortsteile**  \n",
    "   - Loads `lor_ortsteile.geojson` (WGS84).  \n",
    "   - Normalizes district labels; **maps district_id ‚Üí 8 digits**.  \n",
    "   - Derives **ortsteil_id** from `gml_id`.  \n",
    "   - Spatial join (point-in-polygon) to the legacy CSV ‚Üí  \n",
    "     **`legacy_enriched_with_lor.csv`** (adds `district`, `district_id`, `ortsteil`, `ortsteil_id`).\n",
    "\n",
    "4) **(Optional) OSM wide export + public named**  \n",
    "   - Uses OSMnx to build **`osm_pools_wide.csv`** for Berlin.  \n",
    "   - Writes **`osm_public_named.csv`** (named, non-private OSM features).  \n",
    "   - *Skip if you already have these files.*\n",
    "\n",
    "5) **OSM enrichment with districts/ortsteile**  \n",
    "   - Applies the same LOR spatial join to OSM points ‚Üí  \n",
    "     **`osm_enriched_with_lor.csv`** (adds `district`, `district_id`, `ortsteil`, `ortsteil_id`).\n",
    "\n",
    "6) **Cross-check (from enriched files) ‚Üí pairing & new pools**  \n",
    "   - Filters OSM to pool-like features, de-dups by name+coords.  \n",
    "   - Matches **Legacy vs OSM** by normalized name and ‚â§ **250 m** distance.  \n",
    "   - Outputs:  \n",
    "     - **`legacy_enrichment_list.csv`** ‚Äî candidate pairs to pull attributes from OSM  \n",
    "       (both sides carry `district/ortsteil` from LOR).  \n",
    "     - **`osm_public_named.csv`** ‚Äî OSM pools not matched to legacy (potential new pools).\n",
    "\n",
    "7) **Reverse geocode addresses (cache-aware)**  \n",
    "   - Fills **`street`** & **`postal_code`** in `osm_public_named.csv`.  \n",
    "   - Fills **`street_o`** & **`postal_code_o`** in `legacy_enrichment_list.csv` (OSM side only).  \n",
    "   - Uses `reverse_geocode_cache.csv` to avoid re-querying.  \n",
    "   - **Does not** modify district fields (they come from LOR).\n",
    "\n",
    "8) **Build master** ‚Üí `pools_master_minimal.csv`  \n",
    "   - Combines **legacy_enriched_with_lor** + **unmatched OSM named**.  \n",
    "   - Preserves Legacy `pool_id`; generates OSM `pool_id` (`OSM_<id>` or coordinate surrogate).  \n",
    "   - Carries **`district`, `district_id` (8-digit), `ortsteil`, `ortsteil_id`**.  \n",
    "   - Fills blank `open_all_year` ‚Üí `False`.  \n",
    "   - Ensures unique `pool_id`.\n",
    "\n",
    "9) **Validation & Outputs**  \n",
    "   - QC checks: duplicates, **district_id** format/membership, **ortsteil_id** format, postal codes, coords.  \n",
    "   - Small samples of any problematic rows.\n",
    "\n",
    "10) **Finalize schema & save**  \n",
    "   - Coerces types (`latitude/longitude` ‚Üí float).  \n",
    "   - Pads `district_id` ‚Üí **8 digits**, `ortsteil_id` ‚Üí **4 digits** (when non-empty).  \n",
    "   - Reorders columns and overwrites `pools_master_minimal.csv`.\n",
    "\n",
    "---\n",
    "\n",
    "**Notes**\n",
    "- All steps are **idempotent**; re-running updates outputs in place.  \n",
    "- LOR join is the **single source of truth** for `district`, `district_id`, `ortsteil`, `ortsteil_id`.  \n",
    "- Reverse-geocoding is cached; districts are **not** inferred from reverse geocode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7230ea0",
   "metadata": {},
   "source": [
    "# How to use this notebook / file\n",
    "\n",
    "1. **Pull the most recent data** from [baederleben.de](https://baederleben.de/abfragen/baeder-suche.php) ‚Äî save it as **`baederleben_berlin.csv`** and place it **in the same folder as this notebook**.\n",
    "2. **Place** **`lor_ortsteile.geojson`** **in the same folder** as this notebook.\n",
    "3. **Run the notebook top to bottom.**\n",
    "4. **Final CSV output:** **`pools_master_minimal.csv`** (written to this folder).\n",
    "5. **Database publish:** The notebook uploads the result to the DB table **`berlin_source_data.pools_refactored`** (aka *pools_refactored*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a671214f",
   "metadata": {},
   "source": [
    "\n",
    "## **1) Environment & Config**\n",
    "\n",
    "**Why:** \n",
    "- Make the notebook reproducible and self-contained.\n",
    "\n",
    "- Install/import core geo stack used for our two tasks (8-digit district_id mapping + adding ortsteil & ortsteil_id from LOR).\n",
    "\n",
    "- Centralize all file paths and toggles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbaf49f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "versions ‚Üí pandas 2.3.3 | geopandas 1.1.1\n"
     ]
    }
   ],
   "source": [
    "# --- Minimal deps auto-install (safe no-ops if already installed) ---\n",
    "import sys, subprocess, importlib\n",
    "\n",
    "\n",
    "\n",
    "def ensure(pkg, pip_name=None):\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name or pkg])\n",
    "\n",
    "# Core\n",
    "for mod, pipn in [\n",
    "    (\"pandas\", None),\n",
    "    (\"numpy\", None),\n",
    "    (\"geopy\", None),\n",
    "    (\"nbformat\", None),\n",
    "    (\"nbconvert\", None),\n",
    "    # --- GEO stack for our 2 tasks ---\n",
    "    (\"shapely\", None),        # geometry + point-in-polygon\n",
    "    (\"pyproj\", None),         # CRS handling\n",
    "    (\"rtree\", None),          # spatial index for fast spatial joins\n",
    "    (\"pyogrio\", None),        # fast vector IO backend (optional but nice)\n",
    "    (\"geopandas\", None),      # spatial joins + read geojson\n",
    "]:\n",
    "    ensure(mod, pipn)\n",
    "\n",
    "# OSMnx only needed if MAKE_OSM_WIDE = True\n",
    "try:\n",
    "    import osmnx  # noqa\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Geo imports for later cells (used for ortsteil + district join)\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "\n",
    "print(\"versions ‚Üí\",\n",
    "      \"pandas\", pd.__version__,\n",
    "      \"| geopandas\", gpd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1a6cbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured paths:\n",
      "  LEGACY_NOTEBOOK: C:\\Users\\micha\\Projects VS\\final pull 16.10.2025\\pool_data_processing.ipynb\n",
      "  LEGACY_XLSX    : C:\\Users\\micha\\Projects VS\\final pull 16.10.2025\\baederleben_berlin.xlsx\n",
      "  LEGACY_MAIN_CSV: C:\\Users\\micha\\Projects VS\\final pull 16.10.2025\\berlin_pools_final_dataset.csv\n",
      "  LEGACY_ALT_CSV : C:\\Users\\micha\\Projects VS\\final pull 16.10.2025\\pools_data_cleaned.csv\n",
      "  OSM_WIDE_CSV   : C:\\Users\\micha\\Projects VS\\final pull 16.10.2025\\osm_pools_wide.csv (build: True place: Berlin, Germany )\n",
      "  OSM_PUBLIC_NAMED_CSV: C:\\Users\\micha\\Projects VS\\final pull 16.10.2025\\osm_public_named.csv\n",
      "  DIST_M: 100.0 | STRICT_PUBLIC: False\n",
      "  CACHE_PATH: C:\\Users\\micha\\Projects VS\\final pull 16.10.2025\\reverse_geocode_cache.csv\n",
      "\n",
      "Sanity:\n",
      "  LOR_GEOJSON exists: True\n",
      "  Districts in map: 12\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration knobs & paths (adjust as needed) ---\n",
    "from pathlib import Path\n",
    "\n",
    "def resolve_path(p):\n",
    "    return Path(str(p)).expanduser().resolve()\n",
    "\n",
    "# Input artifacts expected next to this notebook\n",
    "LEGACY_NOTEBOOK = resolve_path(\"pool_data_processing.ipynb\")\n",
    "LEGACY_XLSX     = resolve_path(\"baederleben_berlin.xlsx\")\n",
    "\n",
    "# Outputs produced by legacy extractor\n",
    "LEGACY_MAIN_CSV = resolve_path(\"berlin_pools_final_dataset.csv\")\n",
    "LEGACY_ALT_CSV  = resolve_path(\"pools_data_cleaned.csv\")\n",
    "\n",
    "# OSM artifacts\n",
    "MAKE_OSM_WIDE   = True  # set True to build OSM via OSMnx (heavy + may need internet)\n",
    "OSM_PLACE       = \"Berlin, Germany\"\n",
    "OSM_WIDE_CSV    = resolve_path(\"osm_pools_wide.csv\")   # optional\n",
    "OSM_PUBLIC_NAMED_CSV = resolve_path(\"osm_public_named.csv\")\n",
    "\n",
    "# Matching & filtering knobs\n",
    "DIST_M          = 100.0\n",
    "STRICT_PUBLIC   = False  # keep False unless you want to only keep strictly public pools\n",
    "\n",
    "# Enrichment cache for reverse geocoding\n",
    "CACHE_PATH      = resolve_path(\"reverse_geocode_cache.csv\")\n",
    "\n",
    "print(\"Configured paths:\")\n",
    "print(\"  LEGACY_NOTEBOOK:\", LEGACY_NOTEBOOK)\n",
    "print(\"  LEGACY_XLSX    :\", LEGACY_XLSX)\n",
    "print(\"  LEGACY_MAIN_CSV:\", LEGACY_MAIN_CSV)\n",
    "print(\"  LEGACY_ALT_CSV :\", LEGACY_ALT_CSV)\n",
    "print(\"  OSM_WIDE_CSV   :\", OSM_WIDE_CSV, \"(build:\", MAKE_OSM_WIDE, \"place:\", OSM_PLACE, \")\")\n",
    "print(\"  OSM_PUBLIC_NAMED_CSV:\", OSM_PUBLIC_NAMED_CSV)\n",
    "print(\"  DIST_M:\", DIST_M, \"| STRICT_PUBLIC:\", STRICT_PUBLIC)\n",
    "print(\"  CACHE_PATH:\", CACHE_PATH)\n",
    "\n",
    "# --- NEW: LOR (Ortsteil) source for our enrichment ---\n",
    "# Put the GeoJSON file in the same folder as this notebook (or adjust the path).\n",
    "LOR_GEOJSON = resolve_path(\"lor_ortsteile.geojson\")\n",
    "if not LOR_GEOJSON.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"LOR_GEOJSON not found at {LOR_GEOJSON}. \"\n",
    "        \"Place the GeoJSON locally (e.g., 'lor_ortsteile.geojson') and update the path.\"\n",
    "    )\n",
    "\n",
    "# --- NEW: canonical district_id mapping (your exact IDs) ---\n",
    "DISTRICT_ID_MAP = {\n",
    "    \"Mitte\": \"11001001\",\n",
    "    \"Friedrichshain-Kreuzberg\": \"11002002\",\n",
    "    \"Pankow\": \"11003003\",\n",
    "    \"Charlottenburg-Wilmersdorf\": \"11004004\",\n",
    "    \"Spandau\": \"11005005\",\n",
    "    \"Steglitz-Zehlendorf\": \"11006006\",\n",
    "    \"Tempelhof-Sch√∂neberg\": \"11007007\",\n",
    "    \"Neuk√∂lln\": \"11008008\",\n",
    "    \"Treptow-K√∂penick\": \"11009009\",\n",
    "    \"Marzahn-Hellersdorf\": \"11010010\",\n",
    "    \"Lichtenberg\": \"11011011\",\n",
    "    \"Reinickendorf\": \"11012012\",\n",
    "}\n",
    "\n",
    "# --- NEW: lat/lon column names we‚Äôll use for the spatial join ---\n",
    "LAT_COL_NAME = \"latitude\"\n",
    "LON_COL_NAME = \"longitude\"\n",
    "\n",
    "# --- NEW: output paths for enriched artifacts ---\n",
    "LEGACY_ENRICHED_CSV = resolve_path(\"legacy_enriched_with_lor.csv\")\n",
    "OSM_ENRICHED_CSV    = resolve_path(\"osm_enriched_with_lor.csv\")\n",
    "\n",
    "# Quick sanity prints\n",
    "print(\"\\nSanity:\")\n",
    "print(\"  LOR_GEOJSON exists:\", LOR_GEOJSON.exists())\n",
    "print(\"  Districts in map:\", len(DISTRICT_ID_MAP))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22d9264",
   "metadata": {},
   "source": [
    "\n",
    "## **2) Run legacy extractor (if needed)**\n",
    "\n",
    "**Why:** Regenerate the legacy CSVs (only if they don‚Äôt already exist), then pick the one we‚Äôll use and autodetect its latitude/longitude column names for later spatial joins.\n",
    "This executes `pool_data_processing.ipynb` with its own working directory to generate the CSVs.\n",
    "\n",
    "**Outputs:**\n",
    "- `berlin_pools_final_dataset.csv`\n",
    "- `pools_data_cleaned.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9d4fa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Executing legacy extractor notebook‚Ä¶\n",
      "[use] Legacy CSV -> C:\\Users\\micha\\Projects VS\\final pull 16.10.2025\\berlin_pools_final_dataset.csv\n",
      "[schema/legacy] latitude='latitude', longitude='longitude'\n",
      "[info] OSM_PUBLIC_NAMED_CSV not found yet (we can enrich it later).\n"
     ]
    }
   ],
   "source": [
    "# ---  Ensure legacy CSV exists, pick the file to use, and detect coord columns ---\n",
    "\n",
    "from nbconvert.preprocessors import ExecutePreprocessor\n",
    "import nbformat\n",
    "\n",
    "# 3.1) If BOTH legacy outputs are missing, run the extractor notebook\n",
    "need_legacy = (not LEGACY_MAIN_CSV.exists()) and (not LEGACY_ALT_CSV.exists())\n",
    "if need_legacy:\n",
    "    if not LEGACY_NOTEBOOK.exists():\n",
    "        raise FileNotFoundError(f\"Legacy notebook missing: {LEGACY_NOTEBOOK}\")\n",
    "    if not LEGACY_XLSX.exists():\n",
    "        raise FileNotFoundError(f\"Legacy input Excel missing: {LEGACY_XLSX}\")\n",
    "\n",
    "    print(\"[info] Executing legacy extractor notebook‚Ä¶\")\n",
    "    with open(LEGACY_NOTEBOOK, \"r\", encoding=\"utf-8\") as f:\n",
    "        nb = nbformat.read(f, as_version=4)\n",
    "    ep = ExecutePreprocessor(timeout=1800, kernel_name=\"python3\")\n",
    "    ep.preprocess(nb, resources={\"metadata\": {\"path\": str(LEGACY_NOTEBOOK.parent)}})\n",
    "\n",
    "# 3.2) Choose which legacy CSV we'll use downstream\n",
    "LEGACY_IN_USE = LEGACY_MAIN_CSV if LEGACY_MAIN_CSV.exists() else LEGACY_ALT_CSV\n",
    "if not LEGACY_IN_USE.exists():\n",
    "    raise FileNotFoundError(\"No legacy CSV found. Expected one of: \"\n",
    "                            f\"{LEGACY_MAIN_CSV} or {LEGACY_ALT_CSV}\")\n",
    "\n",
    "print(\"[use] Legacy CSV ->\", LEGACY_IN_USE)\n",
    "\n",
    "# 3.3) Detect coordinate column names for LEGACY\n",
    "_probe_leg = pd.read_csv(LEGACY_IN_USE, nrows=5)\n",
    "leg_cols_lower = {c.lower(): c for c in _probe_leg.columns}\n",
    "LEGACY_LAT_COL = leg_cols_lower.get(\"latitude\") or leg_cols_lower.get(\"lat\")\n",
    "LEGACY_LON_COL = leg_cols_lower.get(\"longitude\") or leg_cols_lower.get(\"lon\") or leg_cols_lower.get(\"lng\")\n",
    "if not LEGACY_LAT_COL or not LEGACY_LON_COL:\n",
    "    raise KeyError(\"Legacy CSV must have latitude/longitude (or lat/lon/lng). \"\n",
    "                   f\"Found: {list(_probe_leg.columns)}\")\n",
    "print(f\"[schema/legacy] latitude='{LEGACY_LAT_COL}', longitude='{LEGACY_LON_COL}'\")\n",
    "\n",
    "# 3.4) (Optional) Detect coordinate column names for OSM, if file exists\n",
    "OSM_LAT_COL = OSM_LON_COL = None\n",
    "if OSM_PUBLIC_NAMED_CSV.exists():\n",
    "    _probe_osm = pd.read_csv(OSM_PUBLIC_NAMED_CSV, nrows=5)\n",
    "    osm_cols_lower = {c.lower(): c for c in _probe_osm.columns}\n",
    "    OSM_LAT_COL = osm_cols_lower.get(\"latitude\") or osm_cols_lower.get(\"lat\")\n",
    "    OSM_LON_COL = osm_cols_lower.get(\"longitude\") or osm_cols_lower.get(\"lon\") or osm_cols_lower.get(\"lng\")\n",
    "    if not OSM_LAT_COL or not OSM_LON_COL:\n",
    "        print(\"[warn] OSM CSV present but missing obvious lat/lon columns. \"\n",
    "              f\"Found: {list(_probe_osm.columns)}\")\n",
    "    else:\n",
    "        print(f\"[schema/osm] latitude='{OSM_LAT_COL}', longitude='{OSM_LON_COL}'\")\n",
    "else:\n",
    "    print(\"[info] OSM_PUBLIC_NAMED_CSV not found yet (we can enrich it later).\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca45bb",
   "metadata": {},
   "source": [
    "## **3) (if 2) was run) Load LOR polygons & enrich LEGACY**\n",
    "\n",
    "**Why:** Attach official 8-digit district IDs and Ortsteil fields (ortsteil, ortsteil_id) to the legacy dataset via a spatial join with the LOR (Ortsteil) polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93aa7e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] LOR polygons loaded: 96 features\n",
      "[ok] Legacy enriched -> legacy_enriched_with_lor.csv | rows=144 | missing district_id=0 | missing ortsteil_id=0\n"
     ]
    }
   ],
   "source": [
    "# === Consolidated: LOR loader + suffix-safe enrichment (district_id + ortsteil + ortsteil_id) ===\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "def _normalize_bezirk_name(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    s = s.strip().replace(\"‚Äì\", \"-\").replace(\"‚Äî\", \"-\").replace(\" - \", \"-\")\n",
    "    return s\n",
    "\n",
    "def load_lor_gdf(geojson_path: Path, district_id_map: dict) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Load Ortsteil polygons, normalize names, derive ortsteil_id, and map district_id.\"\"\"\n",
    "    # Try fast engine first; fall back to Fiona if needed\n",
    "    try:\n",
    "        gdf = gpd.read_file(geojson_path, engine=\"pyogrio\")\n",
    "    except Exception:\n",
    "        gdf = gpd.read_file(geojson_path, engine=\"fiona\")\n",
    "\n",
    "    # Ensure WGS84\n",
    "    if gdf.crs is None:\n",
    "        gdf.set_crs(epsg=4326, inplace=True)\n",
    "    else:\n",
    "        gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "    # GeoJSON props: BEZIRK (district), OTEIL (ortsteil), gml_id (e.g., re_ortsteil.0805)\n",
    "    gdf = gdf.rename(columns={\"BEZIRK\": \"district_label\", \"OTEIL\": \"ortsteil\"})\n",
    "    gdf[\"district_label\"] = gdf[\"district_label\"].astype(str).map(_normalize_bezirk_name)\n",
    "    gdf[\"ortsteil\"] = gdf[\"ortsteil\"].astype(str)\n",
    "\n",
    "    # Derive 4-digit ortsteil_id from the tail of gml_id\n",
    "    gdf[\"ortsteil_id\"] = gdf[\"gml_id\"].astype(str).str.split(\".\").str[-1].str.zfill(4)\n",
    "\n",
    "    # Map your exact district_id codes (TASK #1)\n",
    "    gdf[\"district_id\"] = gdf[\"district_label\"].map(district_id_map).astype(\"string\")\n",
    "\n",
    "    keep = [\"district_label\", \"district_id\", \"ortsteil\", \"ortsteil_id\", \"geometry\"]\n",
    "    return gdf[keep].copy()\n",
    "\n",
    "def attach_lor_fields(df: pd.DataFrame,\n",
    "                      lor_polys: gpd.GeoDataFrame,\n",
    "                      lat_col: str,\n",
    "                      lon_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds: district_label, district_id, ortsteil, ortsteil_id via spatial join (within).\n",
    "    Proactively DROPS any pre-existing enrichment columns to avoid _left/_right suffixes.\n",
    "    \"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        out = df.copy()\n",
    "        for c in [\"district_label\",\"district_id\",\"ortsteil\",\"ortsteil_id\"]:\n",
    "            if c not in out.columns: out[c] = pd.Series(dtype=\"string\")\n",
    "        return out\n",
    "\n",
    "    missing = [c for c in (lat_col, lon_col) if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required columns for spatial join: {missing}. \"\n",
    "                       f\"Available: {list(df.columns)}\")\n",
    "\n",
    "    # Drop any existing enrichment cols to prevent suffixes\n",
    "    base = df.drop(columns=[\"district_label\",\"district_id\",\"ortsteil\",\"ortsteil_id\"], errors=\"ignore\").copy()\n",
    "\n",
    "    # Points + spatial join\n",
    "    points = gpd.GeoDataFrame(\n",
    "        base, geometry=gpd.points_from_xy(base[lon_col], base[lat_col]), crs=\"EPSG:4326\"\n",
    "    )\n",
    "    joined = gpd.sjoin(points, lor_polys, how=\"left\", predicate=\"within\")\n",
    "    joined = joined.drop(columns=[c for c in joined.columns if c.startswith(\"index_\")], errors=\"ignore\")\n",
    "\n",
    "    # Ensure canonical columns exist (no suffixes) and are string dtype\n",
    "    for c in [\"district_label\",\"district_id\",\"ortsteil\",\"ortsteil_id\"]:\n",
    "        if c not in joined.columns:\n",
    "            for alt in (f\"{c}_right\", f\"{c}_left\", f\"{c}_y\", f\"{c}_x\"):\n",
    "                if alt in joined.columns:\n",
    "                    joined[c] = joined[alt]\n",
    "                    break\n",
    "        if c in joined.columns:\n",
    "            joined[c] = joined[c].astype(\"string\")\n",
    "        else:\n",
    "            joined[c] = pd.Series(pd.NA, dtype=\"string\")\n",
    "\n",
    "    return pd.DataFrame(joined.drop(columns=[\"geometry\"]))\n",
    "\n",
    "# --- Load polygons once ---\n",
    "lor_gdf = load_lor_gdf(LOR_GEOJSON, DISTRICT_ID_MAP)\n",
    "print(f\"[ok] LOR polygons loaded: {len(lor_gdf)} features\")\n",
    "\n",
    "# --- Enrich LEGACY using detected coord columns from Step 3 ---\n",
    "legacy_df = pd.read_csv(LEGACY_IN_USE)\n",
    "legacy_enriched = attach_lor_fields(legacy_df, lor_gdf,\n",
    "                                    lat_col=LEGACY_LAT_COL, lon_col=LEGACY_LON_COL)\n",
    "legacy_enriched.to_csv(LEGACY_ENRICHED_CSV, index=False)\n",
    "print(f\"[ok] Legacy enriched -> {LEGACY_ENRICHED_CSV.name} | rows={len(legacy_enriched)} | \"\n",
    "      f\"missing district_id={legacy_enriched['district_id'].isna().sum()} | \"\n",
    "      f\"missing ortsteil_id={legacy_enriched['ortsteil_id'].isna().sum()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bddfeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: []\n",
      "Nulls in district_label: 0\n",
      "Nulls in district_id: 0\n",
      "Nulls in ortsteil: 0\n",
      "Nulls in ortsteil_id: 0\n",
      "district_id mismatches vs mapping: 0\n",
      "unique districts present: 12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>district_label</th>\n",
       "      <th>district_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Charlottenburg-Wilmersdorf</td>\n",
       "      <td>11004004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Friedrichshain-Kreuzberg</td>\n",
       "      <td>11002002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lichtenberg</td>\n",
       "      <td>11011011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Marzahn-Hellersdorf</td>\n",
       "      <td>11010010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mitte</td>\n",
       "      <td>11001001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Neuk√∂lln</td>\n",
       "      <td>11008008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pankow</td>\n",
       "      <td>11003003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reinickendorf</td>\n",
       "      <td>11012012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Spandau</td>\n",
       "      <td>11005005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Steglitz-Zehlendorf</td>\n",
       "      <td>11006006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tempelhof-Sch√∂neberg</td>\n",
       "      <td>11007007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Treptow-K√∂penick</td>\n",
       "      <td>11009009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                district_label  district_id\n",
       "11  Charlottenburg-Wilmersdorf     11004004\n",
       "4     Friedrichshain-Kreuzberg     11002002\n",
       "3                  Lichtenberg     11011011\n",
       "7          Marzahn-Hellersdorf     11010010\n",
       "10                       Mitte     11001001\n",
       "8                     Neuk√∂lln     11008008\n",
       "5                       Pankow     11003003\n",
       "0                Reinickendorf     11012012\n",
       "6                      Spandau     11005005\n",
       "9          Steglitz-Zehlendorf     11006006\n",
       "2         Tempelhof-Sch√∂neberg     11007007\n",
       "1             Treptow-K√∂penick     11009009"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Validation ---\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(LEGACY_ENRICHED_CSV)\n",
    "\n",
    "need_cols = [\"district_label\",\"district_id\",\"ortsteil\",\"ortsteil_id\"]\n",
    "print(\"Missing columns:\", [c for c in need_cols if c not in df.columns])\n",
    "\n",
    "for c in need_cols:\n",
    "    print(f\"Nulls in {c}:\", df[c].isna().sum())\n",
    "\n",
    "mapped = df[\"district_label\"].map(DISTRICT_ID_MAP).astype(\"string\")\n",
    "mism = (df[\"district_id\"].astype(\"string\") != mapped) & df[\"district_label\"].notna()\n",
    "print(\"district_id mismatches vs mapping:\", int(mism.sum()))\n",
    "\n",
    "uniq = df[[\"district_label\",\"district_id\"]].dropna().drop_duplicates().sort_values(\"district_label\")\n",
    "print(\"unique districts present:\", len(uniq))\n",
    "display(uniq.head(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf8f244",
   "metadata": {},
   "source": [
    "## **4) (Optional) Build OSM wide export + osm public named**\n",
    "\n",
    "**Why:** reproducible OSM pull to CSV (osm_pools_wide.csv). This step can be slow and needs internet.\n",
    "\n",
    "**Skip it** if you already have osm_pools_wide.csv / osm_public_named.csv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b478beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Building OSM wide for: Berlin, Germany\n",
      "[ok] fetched 1800 features for leisure=['swimming_pool', 'swimming_area', 'water_park', 'beach_resort', 'sports_centre']\n",
      "[ok] fetched 3 features for amenity=['public_bath']\n",
      "[ok] Wrote osm_pools_wide.csv: 1803 rows\n",
      "[ok] Wrote osm_public_named.csv: 638 rows (named)\n"
     ]
    }
   ],
   "source": [
    "# --- : (Optional) Build OSM wide export for Berlin ---\n",
    "\n",
    "# Ensure osmnx is available (your first cell defined `ensure`)\n",
    "try:\n",
    "    import osmnx as ox\n",
    "except Exception:\n",
    "    ensure(\"osmnx\", \"osmnx\")\n",
    "    import osmnx as ox\n",
    "\n",
    "place = OSM_PLACE\n",
    "print(f\"[info] Building OSM wide for: {place}\")\n",
    "\n",
    "# 7.1) Get boundary polygon (WGS84)\n",
    "boundary = ox.geocode_to_gdf(place).to_crs(epsg=4326)\n",
    "\n",
    "# 7.2) Tags we‚Äôll query; this captures most pool-like places\n",
    "tags = {\n",
    "    \"leisure\": [\"swimming_pool\", \"swimming_area\", \"water_park\", \"beach_resort\", \"sports_centre\"],\n",
    "    \"amenity\": [\"public_bath\"],  # rare but include\n",
    "}\n",
    "\n",
    "frames = []\n",
    "poly = boundary.geometry.iloc[0]\n",
    "\n",
    "for key, values in tags.items():\n",
    "    try:\n",
    "        g = ox.features_from_polygon(poly, {key: values})\n",
    "        if not g.empty:\n",
    "            g = g.to_crs(epsg=4326)\n",
    "            g[\"src_tag_key\"] = key\n",
    "            frames.append(g)\n",
    "            print(f\"[ok] fetched {len(g)} features for {key}={values}\")\n",
    "        else:\n",
    "            print(f\"[info] no features for {key}={values}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] fetch failed for {key}={values}: {e}\")\n",
    "\n",
    "if frames:\n",
    "    gdf = pd.concat(frames, ignore_index=True)\n",
    "    # use centroids to get a single representative point for any geometry\n",
    "    gdf_proj = gdf.to_crs(epsg=25833)              # ETRS89 / UTM zone 33N ‚Äî great for Berlin\n",
    "    centroids_proj = gdf_proj.geometry.centroid\n",
    "    centroids_wgs84 = gpd.GeoSeries(centroids_proj, crs=\"EPSG:25833\").to_crs(epsg=4326)\n",
    "\n",
    "    gdf[\"latitude\"]  = centroids_wgs84.y\n",
    "    gdf[\"longitude\"] = centroids_wgs84.x\n",
    "\n",
    "    # Choose useful columns; create if missing so downstream is stable\n",
    "    keep_cols = [\n",
    "        \"name\", \"latitude\", \"longitude\",\n",
    "        \"addr:street\", \"addr:postcode\",\n",
    "        \"phone\", \"website\", \"opening_hours\", \"wheelchair\",\n",
    "        \"access\", \"leisure\", \"amenity\", \"sport\", \"src_tag_key\"\n",
    "    ]\n",
    "    for c in keep_cols:\n",
    "        if c not in gdf.columns:\n",
    "            gdf[c] = pd.NA\n",
    "\n",
    "    wide = gdf[keep_cols].copy().rename(columns={\"addr:street\":\"street\", \"addr:postcode\":\"postal_code\"})\n",
    "    # Keep named rows for the ‚Äúpublic_named‚Äù output\n",
    "    wide[\"name_norm\"] = wide[\"name\"].fillna(\"\").str.strip()\n",
    "    public_mask = wide[\"name_norm\"] != \"\"\n",
    "    if STRICT_PUBLIC:\n",
    "        public_mask &= ~wide[\"access\"].fillna(\"\").str.contains(\"private\", case=False)\n",
    "\n",
    "    public_named = wide.loc[public_mask].drop(columns=[\"name_norm\"])\n",
    "\n",
    "    # Save outputs\n",
    "    wide.to_csv(OSM_WIDE_CSV, index=False)\n",
    "    public_named.to_csv(OSM_PUBLIC_NAMED_CSV, index=False)\n",
    "\n",
    "    print(f\"[ok] Wrote {OSM_WIDE_CSV.name}: {len(wide)} rows\")\n",
    "    print(f\"[ok] Wrote {OSM_PUBLIC_NAMED_CSV.name}: {len(public_named)} rows (named{' & non-private' if STRICT_PUBLIC else ''})\")\n",
    "else:\n",
    "    print(\"[warn] No OSM features found with the configured tags.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57aa71c",
   "metadata": {},
   "source": [
    "## **5) (if 4) was run) OSM enrichment with districts/ortsteile** \n",
    "\n",
    "**Why:** attach Berlin‚Äôs official LOR attributes (district + Ortsteil) to the OSM points, so every record carries:\n",
    "\n",
    "- district_label (human district name)\n",
    "\n",
    "- district_id (your exact 8-digit codes)\n",
    "\n",
    "- ortsteil and ortsteil_id (neighborhood + 4-digit id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "694c2c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] OSM enriched -> osm_enriched_with_lor.csv | rows=638 | missing ortsteil_id=1\n"
     ]
    }
   ],
   "source": [
    "# ---  Enrich OSM (district_id + ortsteil + ortsteil_id) ---\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# make sure polygons are in memory (in case of kernel restarts)\n",
    "try:\n",
    "    lor_gdf\n",
    "except NameError:\n",
    "    lor_gdf = load_lor_gdf(LOR_GEOJSON, DISTRICT_ID_MAP)\n",
    "\n",
    "if OSM_PUBLIC_NAMED_CSV.exists():\n",
    "    osm_df = pd.read_csv(OSM_PUBLIC_NAMED_CSV)\n",
    "\n",
    "    # normalize coordinate columns if needed (your Step 7 already wrote latitude/longitude)\n",
    "    rename_map = {}\n",
    "    if \"lat\" in osm_df.columns and \"latitude\" not in osm_df.columns:\n",
    "        rename_map[\"lat\"] = \"latitude\"\n",
    "    if \"lon\" in osm_df.columns and \"longitude\" not in osm_df.columns:\n",
    "        rename_map[\"lon\"] = \"longitude\"\n",
    "    if \"lng\" in osm_df.columns and \"longitude\" not in rename_map and \"longitude\" not in osm_df.columns:\n",
    "        rename_map[\"lng\"] = \"longitude\"\n",
    "    if rename_map:\n",
    "        osm_df = osm_df.rename(columns=rename_map)\n",
    "\n",
    "    # drop any pre-existing enrichment cols so we never get suffixes\n",
    "    osm_df = osm_df.drop(columns=[\"district\",\"district_label\",\"district_id\",\"invalid_district\",\n",
    "                                  \"ortsteil\",\"ortsteil_id\"], errors=\"ignore\")\n",
    "\n",
    "    # enrich via our suffix-safe helper\n",
    "    osm_enriched = attach_lor_fields(osm_df, lor_gdf, lat_col=\"latitude\", lon_col=\"longitude\")\n",
    "    osm_enriched.to_csv(OSM_ENRICHED_CSV, index=False)\n",
    "\n",
    "    print(f\"[ok] OSM enriched -> {OSM_ENRICHED_CSV.name} | \"\n",
    "          f\"rows={len(osm_enriched)} | missing ortsteil_id={osm_enriched['ortsteil_id'].isna().sum()}\")\n",
    "else:\n",
    "    print(\"[info] OSM_PUBLIC_NAMED_CSV not found ‚Äî run Step 7 first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbe9d42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing rows: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>street</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>leisure</th>\n",
       "      <th>amenity</th>\n",
       "      <th>access</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>Jahnsportst√§tte Ahrensfelde</td>\n",
       "      <td>52.577805</td>\n",
       "      <td>13.566118</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sports_centre</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            name   latitude  longitude street  postal_code  \\\n",
       "560  Jahnsportst√§tte Ahrensfelde  52.577805  13.566118    NaN          NaN   \n",
       "\n",
       "           leisure amenity access  \n",
       "560  sports_centre     NaN    NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --Inspect the missing row--\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "osm_enriched = pd.read_csv(OSM_ENRICHED_CSV)\n",
    "miss = osm_enriched[\"ortsteil_id\"].isna()\n",
    "\n",
    "print(\"Missing rows:\", miss.sum())\n",
    "display(osm_enriched.loc[miss, [\"name\",\"latitude\",\"longitude\",\"street\",\"postal_code\",\"leisure\",\"amenity\",\"access\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc146b7",
   "metadata": {},
   "source": [
    "Ahrensfelde is outside Berlin (Brandenburg), so it won‚Äôt match any Berlin Ortsteil polygon. That‚Äôs why ortsteil_id is NaN ‚Äî and that‚Äôs correct behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45592cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] Kept Berlin-only rows: 637 | dropped outside: 1\n"
     ]
    }
   ],
   "source": [
    "# Filter OSM_ENRICHED to points strictly inside the Berlin boundary\n",
    "import geopandas as gpd, pandas as pd\n",
    "\n",
    "# ensure boundary is available\n",
    "try:\n",
    "    boundary\n",
    "except NameError:\n",
    "    import osmnx as ox\n",
    "    boundary = ox.geocode_to_gdf(OSM_PLACE).to_crs(epsg=4326)\n",
    "\n",
    "osm = pd.read_csv(OSM_ENRICHED_CSV)\n",
    "gdf_pts = gpd.GeoDataFrame(osm, geometry=gpd.points_from_xy(osm[\"longitude\"], osm[\"latitude\"]), crs=\"EPSG:4326\")\n",
    "\n",
    "berlin_poly = boundary.geometry.iloc[0]\n",
    "inside = gdf_pts.within(berlin_poly)         # strict containment\n",
    "dropped = len(gdf_pts) - int(inside.sum())\n",
    "\n",
    "osm_berlin_only = pd.DataFrame(gdf_pts.loc[inside].drop(columns=\"geometry\"))\n",
    "osm_berlin_only.to_csv(OSM_ENRICHED_CSV, index=False)\n",
    "\n",
    "print(f\"[ok] Kept Berlin-only rows: {len(osm_berlin_only)} | dropped outside: {dropped}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5888aff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls in district_label: 0\n",
      "Nulls in district_id: 0\n",
      "Nulls in ortsteil: 0\n",
      "Nulls in ortsteil_id: 0\n"
     ]
    }
   ],
   "source": [
    "# - Validate\n",
    "df = pd.read_csv(OSM_ENRICHED_CSV)\n",
    "for c in [\"district_label\",\"district_id\",\"ortsteil\",\"ortsteil_id\"]:\n",
    "    print(f\"Nulls in {c}:\", df[c].isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5236f635",
   "metadata": {},
   "source": [
    "## **6) Cross-check (from enriched files) ‚Üí pairing & new pools**  \n",
    "**Why:** find what‚Äôs missing in the legacy tool vs. OSM after adding LOR fields (district + Ortsteil).\n",
    "\n",
    "You‚Äôll get:\n",
    "- `legacy_enrichment_list.csv` ‚Äî candidates to pull extra attributes from OSM\n",
    "- `osm_public_named.csv` ‚Äî OSM pools not matched to legacy (potential new pools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bad3616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefer enriched; fall back to raw paths\n",
    "LEGACY_PATH = (LEGACY_ENRICHED_CSV if LEGACY_ENRICHED_CSV.exists()\n",
    "               else (LEGACY_MAIN_CSV if LEGACY_MAIN_CSV.exists() else LEGACY_ALT_CSV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b168c504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Using legacy (enriched): legacy_enriched_with_lor.csv\n",
      "[info] Using OSM (enriched):    osm_enriched_with_lor.csv\n",
      "{'legacy_rows': 144, 'osm_enriched_rows': 637, 'osm_filtered_like_old_pipeline': 69}\n",
      "[ok] Wrote: legacy_enrichment_list.csv, osm_public_named.csv\n",
      "{'legacy_enrichment_list': 48, 'osm_public_named': 23}\n"
     ]
    }
   ],
   "source": [
    "# === Cross-check using ENRICHED files but OSM filtered like before ===\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, math, unicodedata, re\n",
    "\n",
    "# Inputs (enriched)\n",
    "LEGACY_PATH = LEGACY_ENRICHED_CSV\n",
    "OSM_NAMED_ENRICHED_PATH = OSM_ENRICHED_CSV\n",
    "DIST_M = 250\n",
    "STRICT_PUBLIC = False  # keep consistent with your pipeline\n",
    "\n",
    "print(f\"[info] Using legacy (enriched): {LEGACY_PATH.name}\")\n",
    "print(f\"[info] Using OSM (enriched):    {OSM_NAMED_ENRICHED_PATH.name}\")\n",
    "\n",
    "legacy = pd.read_csv(LEGACY_PATH)\n",
    "osm    = pd.read_csv(OSM_NAMED_ENRICHED_PATH)\n",
    "\n",
    "# --- helpers (same as your pipeline) ---\n",
    "def normalize_ascii(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    return \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "\n",
    "def normalize_name(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    s = normalize_ascii(s).lower().strip()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def is_public_access(v, strict=False) -> bool:\n",
    "    if v is None or (isinstance(v, float) and math.isnan(v)):\n",
    "        return not strict\n",
    "    v = str(v).strip().lower()\n",
    "    if v in {\"private\",\"customers\",\"residents\"}: return False\n",
    "    return (v in {\"yes\",\"public\"} if strict else v in {\"\",\"yes\",\"public\",\"permissive\"} or v is None)\n",
    "\n",
    "def haversine_m(lat1, lon1, lat2, lon2) -> float:\n",
    "    if any(pd.isna([lat1, lon1, lat2, lon2])): return np.nan\n",
    "    R=6371000.0\n",
    "    phi1=math.radians(lat1); phi2=math.radians(lat2)\n",
    "    dphi=math.radians(lat2-lat1); dl=math.radians(lon2-lon1)\n",
    "    a=math.sin(dphi/2)**2+math.cos(phi1)*math.cos(phi2)*math.sin(dl/2)**2\n",
    "    return 2*R*math.asin(math.sqrt(a))\n",
    "\n",
    "def colmap(df): return {c.lower(): c for c in df.columns}\n",
    "def cget(cols, *choices):\n",
    "    for ch in choices:\n",
    "        if ch in cols: return cols[ch]\n",
    "    return None\n",
    "\n",
    "def has_swimming(val):\n",
    "    if val is None or (isinstance(val, float) and np.isnan(val)): return False\n",
    "    s = str(val).lower()\n",
    "    return (\"swimming\" in s) or (\"schwimm\" in s)\n",
    "\n",
    "def detect_lat_lon(df):\n",
    "    c = colmap(df)\n",
    "    lat = cget(c, \"latitude\",\"lat\")\n",
    "    lon = cget(c, \"longitude\",\"lon\",\"lng\")\n",
    "    if not lat or not lon:\n",
    "        raise KeyError(f\"Could not detect lat/lon in columns: {list(df.columns)}\")\n",
    "    return lat, lon\n",
    "\n",
    "# --- 1) Filter OSM (enriched) to pool-like + public + valid coords, then de-dup (as before) ---\n",
    "c = colmap(osm)\n",
    "leisure_c = cget(c, \"leisure\")\n",
    "sport_c   = cget(c, \"sport\")\n",
    "access_c  = cget(c, \"access\")\n",
    "\n",
    "leisure = osm[leisure_c].astype(str).str.lower() if leisure_c else pd.Series([\"\"]*len(osm), index=osm.index)\n",
    "sport   = osm[sport_c].astype(str) if sport_c else pd.Series([\"\"]*len(osm), index=osm.index)\n",
    "\n",
    "is_pool = (\n",
    "    leisure.eq(\"swimming_pool\") |\n",
    "    leisure.eq(\"swimming_area\") |\n",
    "    (leisure.eq(\"sports_centre\") & sport.apply(has_swimming))\n",
    ")\n",
    "\n",
    "pub_mask = osm[access_c].apply(lambda v: is_public_access(v, STRICT_PUBLIC)) if access_c else pd.Series([True]*len(osm), index=osm.index)\n",
    "lat_o, lon_o = detect_lat_lon(osm)\n",
    "coord_mask = pd.to_numeric(osm[lat_o], errors=\"coerce\").notna() & pd.to_numeric(osm[lon_o], errors=\"coerce\").notna()\n",
    "\n",
    "osm_f = osm[is_pool & pub_mask & coord_mask].copy()\n",
    "osm_f[\"name_norm\"] = osm_f.get(\"name\",\"\").astype(str).map(normalize_name)\n",
    "osm_f[\"lat_r\"] = pd.to_numeric(osm_f[lat_o], errors=\"coerce\").round(5)\n",
    "osm_f[\"lon_r\"] = pd.to_numeric(osm_f[lon_o], errors=\"coerce\").round(5)\n",
    "osm_f = osm_f.drop_duplicates(subset=[\"name_norm\",\"lat_r\",\"lon_r\"], keep=\"first\").drop(columns=[\"lat_r\",\"lon_r\"])\n",
    "\n",
    "# --- 2) Legacy side: prepare names/coords ---\n",
    "legacy[\"name_norm\"] = legacy.get(\"name\",\"\").astype(str).map(normalize_name)\n",
    "lat_l, lon_l = detect_lat_lon(legacy)\n",
    "\n",
    "print({\n",
    "    \"legacy_rows\": len(legacy),\n",
    "    \"osm_enriched_rows\": len(osm),\n",
    "    \"osm_filtered_like_old_pipeline\": len(osm_f)\n",
    "})\n",
    "\n",
    "# --- 3) Candidate join and matching ---\n",
    "L = legacy.assign(bucket=legacy[\"name_norm\"].str[:10])\n",
    "O = osm_f.assign(bucket=osm_f[\"name_norm\"].str[:10])\n",
    "\n",
    "cand = L.merge(O, on=\"bucket\", suffixes=(\"_l\",\"_o\"), how=\"inner\")\n",
    "cand[\"name_match\"] = (cand[\"name_norm_l\"].ne(\"\")) & cand[\"name_norm_l\"].eq(cand[\"name_norm_o\"])\n",
    "cand[\"dist_m\"] = cand.apply(lambda r: haversine_m(r[lat_l+\"_l\"], r[lon_l+\"_l\"], r[lat_o+\"_o\"], r[lon_o+\"_o\"]), axis=1)\n",
    "\n",
    "legacy_enrichment_list = cand[(cand[\"name_match\"]) | (cand[\"dist_m\"] <= DIST_M)].copy()\n",
    "\n",
    "keep_cols = [\n",
    "    # legacy side + LOR\n",
    "    \"name_l\", f\"{lat_l}_l\", f\"{lon_l}_l\",\n",
    "    \"district_label_l\",\"district_id_l\",\"ortsteil_l\",\"ortsteil_id_l\",\n",
    "    # osm side + LOR + addr\n",
    "    \"name_o\", f\"{lat_o}_o\", f\"{lon_o}_o\", \"street_o\",\"postal_code_o\",\n",
    "    \"district_label_o\",\"district_id_o\",\"ortsteil_o\",\"ortsteil_id_o\",\n",
    "    \"website_o\",\"opening_hours_o\",\"wheelchair_o\",\"phone_o\",\n",
    "    # matching info\n",
    "    \"dist_m\",\"name_match\"\n",
    "]\n",
    "legacy_enrichment_list = legacy_enrichment_list[[c for c in keep_cols if c in legacy_enrichment_list.columns]]\n",
    "\n",
    "# --- 4) Unmatched OSM named (from filtered set), keep all enriched columns ---\n",
    "def rounded_key(df, lat_col, lon_col):\n",
    "    return df[\"name_norm\"].astype(str) + \"|\" + pd.to_numeric(df[lat_col], errors=\"coerce\").round(5).astype(str) + \"|\" + pd.to_numeric(df[lon_col], errors=\"coerce\").round(5).astype(str)\n",
    "\n",
    "cand[\"_key_o\"] = (\n",
    "    cand[\"name_norm_o\"].astype(str) + \"|\" +\n",
    "    pd.to_numeric(cand[lat_o+\"_o\"], errors=\"coerce\").round(5).astype(str) + \"|\" +\n",
    "    pd.to_numeric(cand[lon_o+\"_o\"], errors=\"coerce\").round(5).astype(str)\n",
    ")\n",
    "matched_keys = set(\n",
    "    cand.loc[(cand[\"name_match\"]) | (cand[\"dist_m\"] <= DIST_M), \"_key_o\"].tolist()\n",
    ")\n",
    "\n",
    "osm_f[\"_key\"] = rounded_key(osm_f, lat_o, lon_o)\n",
    "osm_public_named = osm_f[~osm_f[\"_key\"].isin(matched_keys)].drop(columns=[\"_key\"]).copy()\n",
    "\n",
    "# --- 5) Save exactly the two files you want ---\n",
    "legacy_enrichment_list.to_csv(\"legacy_enrichment_list.csv\", index=False)\n",
    "osm_public_named.to_csv(\"osm_public_named.csv\", index=False)\n",
    "\n",
    "print(\"[ok] Wrote: legacy_enrichment_list.csv, osm_public_named.csv\")\n",
    "print({\n",
    "    \"legacy_enrichment_list\": len(legacy_enrichment_list),\n",
    "    \"osm_public_named\": len(osm_public_named)\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ef699f",
   "metadata": {},
   "source": [
    "\n",
    "## 6.1 Load inputs & prep candidate pairs\n",
    "\n",
    "**Why:** standardize names on both sides so our later fuzzy name + distance matching is stable and reproducible.:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e40fb19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[legacy] sample: [{'name': 'Strandbad L√ºbars', 'name_norm': 'strandbad lubars'}, {'name': 'Kleine Schwimmhalle Wuhlheide', 'name_norm': 'kleine schwimmhalle wuhlheide'}, {'name': 'Kombibad Mariendorf', 'name_norm': 'kombibad mariendorf'}]\n",
      "[osm_f] sample: [{'name': 'Schwimmschule Wassermeloni', 'name_norm': 'schwimmschule wassermeloni'}, {'name': 'Freibad L√ºbars', 'name_norm': 'freibad lubars'}, {'name': '1. Berliner Kinder-Schwimmschule', 'name_norm': '1 berliner kinder schwimmschule'}]\n"
     ]
    }
   ],
   "source": [
    "# --- Normalized names on ENRICHED dataframes (legacy + OSM filtered) ---\n",
    "\n",
    "import unicodedata, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def normalize_name(s: str) -> str:\n",
    "    if s is None or (isinstance(s, float) and np.isnan(s)): \n",
    "        s = \"\"\n",
    "    s = str(s)\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s.lower()).strip()\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def ensure_text_col(df, *candidates, create=\"name\"):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    if create not in df.columns:\n",
    "        df[create] = \"\"\n",
    "    return create\n",
    "\n",
    "def coalesce_cols(df, *cols):\n",
    "    present = [c for c in cols if c in df.columns]\n",
    "    if not present:\n",
    "        return pd.Series([\"\"] * len(df), index=df.index, dtype=object)\n",
    "    out = df[present[0]].astype(str).fillna(\"\")\n",
    "    for c in present[1:]:\n",
    "        nxt = df[c].astype(str).fillna(\"\")\n",
    "        use_nxt = out.str.strip().eq(\"\")\n",
    "        out = out.where(~use_nxt, nxt)\n",
    "    return out.fillna(\"\")\n",
    "\n",
    "# ---- LEGACY (enriched) ----\n",
    "# Use 'legacy' DF if you still have it in memory; otherwise reload:\n",
    "try:\n",
    "    legacy\n",
    "except NameError:\n",
    "    legacy = pd.read_csv(LEGACY_ENRICHED_CSV)\n",
    "\n",
    "legacy_name_col = ensure_text_col(legacy, \"name\", \"pool_name\", create=\"name\")\n",
    "legacy[\"name\"] = legacy[legacy_name_col].astype(str).fillna(\"\").str.strip()\n",
    "legacy[\"name_norm\"] = legacy[\"name\"].map(normalize_name)\n",
    "\n",
    "# ---- OSM (enriched + filtered as in your pipeline: osm_f) ----\n",
    "# If you don't have osm_f in memory, rebuild the filtered view from osm_enriched:\n",
    "try:\n",
    "    osm_f\n",
    "except NameError:\n",
    "    osm = pd.read_csv(OSM_ENRICHED_CSV)\n",
    "    # minimal re-filter (same as earlier): keep valid coords only\n",
    "    lat_o = \"latitude\" if \"latitude\" in osm.columns else \"lat\"\n",
    "    lon_o = \"longitude\" if \"longitude\" in osm.columns else \"lon\"\n",
    "    coord_mask = pd.to_numeric(osm[lat_o], errors=\"coerce\").notna() & pd.to_numeric(osm[lon_o], errors=\"coerce\").notna()\n",
    "    osm_f = osm[coord_mask].copy()\n",
    "\n",
    "# Coalesce common OSM name fields; take first token before ';' (OSM often stores multiple names)\n",
    "osm_f[\"name\"] = coalesce_cols(\n",
    "    osm_f,\n",
    "    \"name\",\"official_name\",\"short_name\",\"alt_name\",\"name:de\",\"name:en\",\"brand\",\"operator\"\n",
    ").astype(str).fillna(\"\").str.split(\";\").str[0].str.strip()\n",
    "\n",
    "osm_f[\"name_norm\"] = osm_f[\"name\"].map(normalize_name)\n",
    "\n",
    "# (Optional) quick peek\n",
    "print(\"[legacy] sample:\", legacy[[\"name\",\"name_norm\"]].head(3).to_dict(orient=\"records\"))\n",
    "print(\"[osm_f] sample:\", osm_f[[\"name\",\"name_norm\"]].head(3).to_dict(orient=\"records\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8ff693",
   "metadata": {},
   "source": [
    "\n",
    "## **7) Reverse geocode addresses (cache-aware)**  \n",
    "\n",
    "**Why:** some OSM points don‚Äôt carry addresses. We fill street and postal_code from Nominatim while keeping districts from LOR (we do not touch district/district_id here). **with a local cache** to avoid repeated lookups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deac484d",
   "metadata": {},
   "source": [
    "### 7.1 Reverse-geocode osm_public_named.csv and legacy (enriched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca521e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] OSM: rows needing street/postcode: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\AppData\\Local\\Temp\\ipykernel_26308\\191172126.py:134: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  cc = pd.concat([cache, new], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] LEGACY: rows needing street/postcode: 3\n",
      "[ok] Updated legacy file ‚Üí legacy_enriched_with_lor.csv\n",
      "[report] OSM missing ‚Üí street:0  postal_code:0  district:0\n",
      "[report] LEGACY missing ‚Üí street:0  postal_code:0  district:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\AppData\\Local\\Temp\\ipykernel_26308\\191172126.py:134: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  cc = pd.concat([cache, new], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# --- Enrich street / postal_code using reverse geocode; district comes from LOR ---\n",
    "from pathlib import Path\n",
    "import pandas as pd, re, unicodedata\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "# --------------------------- CONFIG ---------------------------\n",
    "OSM_PUBLIC_NAMED_CSV = Path(\"osm_public_named.csv\")   # already enriched with LOR\n",
    "CACHE_PATH = Path(\"reverse_geocode_cache.csv\")\n",
    "\n",
    "# Prefer enriched legacy so we keep district_label/district_id/ortsteil/ortsteil_id\n",
    "try:\n",
    "    LEGACY_CSV = LEGACY_ENRICHED_CSV if LEGACY_ENRICHED_CSV.exists() else (\n",
    "        LEGACY_MAIN_CSV if LEGACY_MAIN_CSV.exists() else LEGACY_ALT_CSV\n",
    "    )\n",
    "except Exception:\n",
    "    LEGACY_CSV = Path(\"legacy_enriched_with_lor.csv\") if Path(\"legacy_enriched_with_lor.csv\").exists() else Path(\"berlin_pools_final_dataset.csv\")\n",
    "\n",
    "WRITE_LEGACY_INPLACE = True  # write back to LEGACY_CSV\n",
    "\n",
    "# -------------------------- HELPERS ---------------------------\n",
    "def strip_accents(s: str) -> str:\n",
    "    s = \"\" if s is None else str(s)\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    return \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "\n",
    "def is_blank(x) -> bool:\n",
    "    if pd.isna(x): return True\n",
    "    return str(x).strip().lower() in {\"\", \"nan\", \"none\", \"<na>\", \"null\"}\n",
    "\n",
    "def key(lat, lon):\n",
    "    try: return round(float(lat), 6), round(float(lon), 6)\n",
    "    except Exception: return None\n",
    "\n",
    "def clean_postcode(p):\n",
    "    m = re.search(r\"\\b(1[0-4]\\d{3})\\b\", str(p))\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "def street_from_address(addr: str) -> str:\n",
    "    if not addr: return \"\"\n",
    "    # simple heuristics for \"Word Word 123\" or common street tokens\n",
    "    m = re.search(r\"([A-Za-z√Ñ√ñ√ú√§√∂√º√ü\\-\\s]+)\\s(\\d+[a-zA-Z]?)\", str(addr))\n",
    "    if m: return m.group(0).strip()\n",
    "    tokens = (\"stra√üe\",\"str.\",\"strasse\",\"allee\",\"damm\",\"weg\",\"platz\",\"ufer\",\"chaussee\",\"ring\",\"steig\",\"promenade\",\"gasse\",\"pfad\")\n",
    "    parts = [p.strip() for p in str(addr).split(\",\") if p.strip()]\n",
    "    for p in parts:\n",
    "        if any(t in p.lower() for t in tokens):\n",
    "            return p.strip()\n",
    "    return \"\"\n",
    "\n",
    "# ------------------ shared cache + geocoder ------------------\n",
    "cache_cols = [\"lat\",\"lon\",\"street\",\"postal_code\"]\n",
    "cache = pd.read_csv(CACHE_PATH) if CACHE_PATH.exists() else pd.DataFrame(columns=cache_cols)\n",
    "for c in cache_cols:\n",
    "    if c not in cache.columns: cache[c] = \"\"\n",
    "cache[\"key\"] = cache.apply(lambda r: key(r.get(\"lat\"), r.get(\"lon\")), axis=1)\n",
    "cache_dict = {k: (s, p) for k, s, p in zip(cache[\"key\"], cache[\"street\"], cache[\"postal_code\"]) if pd.notna(k)}\n",
    "\n",
    "# Respect Nominatim usage policy (identify yourself + rate limit)\n",
    "geolocator = Nominatim(user_agent=\"pools_enrichment/1.0 (contact: your_email@example.com)\")\n",
    "reverse = RateLimiter(geolocator.reverse, min_delay_seconds=1.0, max_retries=2, error_wait_seconds=2.0, swallow_exceptions=True)\n",
    "\n",
    "def enrich_table_inplace(df, lat_candidates, lon_candidates, street_col, post_col, sanity_label=\"\"):\n",
    "    \"\"\"\n",
    "    Fills ONLY street/postal_code via reverse geocoding; DOES NOT touch district.\n",
    "    District should come from LOR columns (district_label/district_id) outside this function.\n",
    "    \"\"\"\n",
    "    # ensure cols\n",
    "    for c in [street_col, post_col]:\n",
    "        if c not in df.columns: df[c] = \"\"\n",
    "        df[c] = df[c].astype(\"string\")\n",
    "\n",
    "    # lat/lon pick\n",
    "    def pickcol(cands):\n",
    "        for c in cands:\n",
    "            if c in df.columns: return c\n",
    "        return None\n",
    "    lat_c = pickcol(lat_candidates)\n",
    "    lon_c = pickcol(lon_candidates)\n",
    "    if not lat_c or not lon_c:\n",
    "        print(f(\"[skip] {sanity_label}: no lat/lon\"))\n",
    "        return\n",
    "\n",
    "    # who needs?\n",
    "    need_mask = (\n",
    "        df[street_col].apply(is_blank) |\n",
    "        df[post_col].apply(is_blank)\n",
    "    ) & df[lat_c].notna() & df[lon_c].notna()\n",
    "\n",
    "    need = df[need_mask].copy()\n",
    "    print(f\"[info] {sanity_label}: rows needing street/postcode:\", int(need_mask.sum()))\n",
    "\n",
    "    filled_rows = []\n",
    "    for idx, r in need.iterrows():\n",
    "        k = key(r[lat_c], r[lon_c])\n",
    "        if not k:\n",
    "            continue\n",
    "\n",
    "        s, p = cache_dict.get(k, (\"\",\"\"))\n",
    "\n",
    "        if is_blank(s) or is_blank(p):\n",
    "            loc = reverse((r[lat_c], r[lon_c]), exactly_one=True, addressdetails=True, language=\"de\", zoom=18)\n",
    "            ad  = (loc.raw.get(\"address\") if loc and hasattr(loc, \"raw\") else {}) or {}\n",
    "\n",
    "            # street / house no\n",
    "            if is_blank(s):\n",
    "                road = ad.get(\"road\") or ad.get(\"pedestrian\") or ad.get(\"footway\") or ad.get(\"path\") or \"\"\n",
    "                hn   = ad.get(\"house_number\") or \"\"\n",
    "                s = \" \".join([x for x in [road, hn] if x]).strip()\n",
    "\n",
    "            # postcode\n",
    "            if is_blank(p):\n",
    "                p = clean_postcode(ad.get(\"postcode\", \"\"))\n",
    "\n",
    "            if k:\n",
    "                filled_rows.append({\"lat\": k[0], \"lon\": k[1], \"street\": s, \"postal_code\": p, \"key\": k})\n",
    "\n",
    "        # assign back only if blank\n",
    "        if not is_blank(s) and is_blank(df.at[idx, street_col]):\n",
    "            df.at[idx, street_col] = str(s)\n",
    "        if not is_blank(p) and is_blank(df.at[idx, post_col]):\n",
    "            df.at[idx, post_col] = str(p)\n",
    "\n",
    "    # persist cache (only if we actually added any rows)\n",
    "    if filled_rows:\n",
    "        new = pd.DataFrame(filled_rows)\n",
    "        # ensure consistent columns/order\n",
    "        for c in [\"lat\", \"lon\", \"street\", \"postal_code\", \"key\"]:\n",
    "            if c not in new.columns:\n",
    "                new[c] = pd.NA\n",
    "        new = new[[\"lat\", \"lon\", \"street\", \"postal_code\", \"key\"]]\n",
    "\n",
    "        # concat with existing cache\n",
    "        cc = pd.concat([cache, new], ignore_index=True)\n",
    "        cc = cc.drop_duplicates(subset=[\"key\"], keep=\"last\")\n",
    "\n",
    "        # write and refresh in-memory map\n",
    "        cc[[\"lat\", \"lon\", \"street\", \"postal_code\"]].to_csv(CACHE_PATH, index=False)\n",
    "        cc[\"key\"] = cc.apply(lambda r: key(r.get(\"lat\"), r.get(\"lon\")), axis=1)\n",
    "        cache_dict.update({\n",
    "            k: (s, p)\n",
    "            for k, s, p in zip(cc[\"key\"], cc[\"street\"], cc[\"postal_code\"])\n",
    "            if pd.notna(k)\n",
    "        })\n",
    "\n",
    "def adopt_lor_district(df, district_text_col=\"district\"):\n",
    "    \"\"\"\n",
    "    Ensure the human-readable 'district' column is filled from LOR's district_label.\n",
    "    Never overwrite non-blank 'district' values.\n",
    "    \"\"\"\n",
    "    if \"district_label\" in df.columns:\n",
    "        if district_text_col not in df.columns:\n",
    "            df[district_text_col] = \"\"\n",
    "        mask = df[district_text_col].astype(str).str.strip().eq(\"\")\n",
    "        df.loc[mask, district_text_col] = df.loc[mask, \"district_label\"].astype(\"string\")\n",
    "\n",
    "# ------------------ OSM FILE ------------------\n",
    "osm = pd.read_csv(OSM_PUBLIC_NAMED_CSV)\n",
    "\n",
    "# Pre-fill district text from LOR\n",
    "adopt_lor_district(osm, district_text_col=\"district\")\n",
    "\n",
    "# Optional: prefill from OSM-style address columns\n",
    "if \"addr_street\" in osm.columns:\n",
    "    m = osm[\"street\"].astype(str).str.strip().eq(\"\") if \"street\" in osm.columns else pd.Series(True, index=osm.index)\n",
    "    osm.loc[m, \"street\"] = osm.loc[m, \"addr_street\"].astype(str)\n",
    "if \"addr_postcode\" in osm.columns:\n",
    "    m = osm[\"postal_code\"].astype(str).str.strip().eq(\"\") if \"postal_code\" in osm.columns else pd.Series(True, index=osm.index)\n",
    "    osm.loc[m, \"postal_code\"] = osm.loc[m, \"addr_postcode\"].astype(str)\n",
    "\n",
    "# Reverse geocode only missing street/postal_code\n",
    "enrich_table_inplace(\n",
    "    osm,\n",
    "    lat_candidates=(\"latitude\",\"lat\"),\n",
    "    lon_candidates=(\"longitude\",\"lon\"),\n",
    "    street_col=\"street\",\n",
    "    post_col=\"postal_code\",\n",
    "    sanity_label=\"OSM\"\n",
    ")\n",
    "\n",
    "# normalize post code as text\n",
    "osm[\"postal_code\"] = osm[\"postal_code\"].astype(str).str.replace(\".0\",\"\",regex=False)\n",
    "osm.to_csv(OSM_PUBLIC_NAMED_CSV, index=False)\n",
    "\n",
    "# ------------------ LEGACY FILE ------------------\n",
    "legacy = pd.read_csv(LEGACY_CSV)\n",
    "\n",
    "# Ensure target columns exist\n",
    "for c in [\"street\",\"postal_code\",\"district\"]:\n",
    "    if c not in legacy.columns: legacy[c] = \"\"\n",
    "\n",
    "# Fill district text from LOR first\n",
    "adopt_lor_district(legacy, district_text_col=\"district\")\n",
    "\n",
    "# Try to prefill street from any \"address\" field\n",
    "if \"address\" in legacy.columns:\n",
    "    mask = legacy[\"street\"].astype(str).str.strip().eq(\"\")\n",
    "    legacy.loc[mask, \"street\"] = legacy.loc[mask, \"address\"].astype(str).map(street_from_address)\n",
    "\n",
    "# Reverse geocode only missing street/postal_code\n",
    "enrich_table_inplace(\n",
    "    legacy,\n",
    "    lat_candidates=(\"latitude\",\"lat\"),\n",
    "    lon_candidates=(\"longitude\",\"lon\"),\n",
    "    street_col=\"street\",\n",
    "    post_col=\"postal_code\",\n",
    "    sanity_label=\"LEGACY\"\n",
    ")\n",
    "legacy[\"postal_code\"] = legacy[\"postal_code\"].astype(str).str.replace(\".0\",\"\",regex=False)\n",
    "\n",
    "# write legacy back\n",
    "if WRITE_LEGACY_INPLACE:\n",
    "    legacy.to_csv(LEGACY_CSV, index=False)\n",
    "    print(\"[ok] Updated legacy file ‚Üí\", LEGACY_CSV.name)\n",
    "else:\n",
    "    out_legacy = LEGACY_CSV.with_name(LEGACY_CSV.stem + \"_enriched.csv\")\n",
    "    legacy.to_csv(out_legacy, index=False)\n",
    "    print(\"[ok] Wrote legacy enriched ‚Üí\", out_legacy.name)\n",
    "\n",
    "# ------------------ tiny checks ------------------\n",
    "def peek_missing(df, label):\n",
    "    miss_st = int(df[\"street\"].astype(str).str.strip().eq(\"\").sum()) if \"street\" in df.columns else 0\n",
    "    miss_pc = int(df[\"postal_code\"].astype(str).str.strip().eq(\"\").sum()) if \"postal_code\" in df.columns else 0\n",
    "    miss_di = int(df[\"district\"].astype(str).str.strip().eq(\"\").sum()) if \"district\" in df.columns else 0\n",
    "    print(f\"[report] {label} missing ‚Üí street:{miss_st}  postal_code:{miss_pc}  district:{miss_di}\")\n",
    "\n",
    "peek_missing(osm, \"OSM\")\n",
    "peek_missing(legacy, \"LEGACY\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d072c084",
   "metadata": {},
   "source": [
    "### 7.2 Fill OSM-side addresses in legacy_enrichment_list.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cb99eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] LEGACY_ENRICHMENT_LIST (OSM side): rows needing street/postcode: 13\n",
      "[ok] legacy_enrichment_list updated ‚Üí legacy_enrichment_list.csv\n",
      "  street_o blanks:      0 ‚Üí 0\n",
      "  postal_code_o blanks: 0 ‚Üí 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\AppData\\Local\\Temp\\ipykernel_26308\\191172126.py:134: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  cc = pd.concat([cache, new], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# --- Fill street_o / postal_code_o in legacy_enrichment_list.csv via reverse geocoding ---\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# If you just ran the previous cell, enrich_table_inplace + cache + reverse are already defined.\n",
    "# We only need to call it pointing at the OSM-side columns.\n",
    "\n",
    "lxl_path = \"legacy_enrichment_list.csv\"\n",
    "lxl = pd.read_csv(lxl_path)\n",
    "\n",
    "# Count blanks before\n",
    "def _blanks(s):\n",
    "    return int(s.astype(str).str.strip().eq(\"\").sum())\n",
    "\n",
    "before_st = _blanks(lxl[\"street_o\"]) if \"street_o\" in lxl.columns else None\n",
    "before_pc = _blanks(lxl[\"postal_code_o\"]) if \"postal_code_o\" in lxl.columns else None\n",
    "\n",
    "# Enrich ONLY the OSM side (suffix _o). We pass multiple candidate names to be robust.\n",
    "enrich_table_inplace(\n",
    "    lxl,\n",
    "    lat_candidates=(\"latitude_o\",\"lat_o\"),\n",
    "    lon_candidates=(\"longitude_o\",\"lon_o\",\"lng_o\"),\n",
    "    street_col=\"street_o\",\n",
    "    post_col=\"postal_code_o\",\n",
    "    sanity_label=\"LEGACY_ENRICHMENT_LIST (OSM side)\"\n",
    ")\n",
    "\n",
    "# Tidy postcode type\n",
    "if \"postal_code_o\" in lxl.columns:\n",
    "    lxl[\"postal_code_o\"] = lxl[\"postal_code_o\"].astype(str).str.replace(\".0\",\"\",regex=False)\n",
    "\n",
    "# Save back\n",
    "lxl.to_csv(lxl_path, index=False)\n",
    "\n",
    "# Report after\n",
    "after_st = _blanks(lxl[\"street_o\"]) if \"street_o\" in lxl.columns else None\n",
    "after_pc = _blanks(lxl[\"postal_code_o\"]) if \"postal_code_o\" in lxl.columns else None\n",
    "\n",
    "print(f\"[ok] legacy_enrichment_list updated ‚Üí {lxl_path}\")\n",
    "if before_st is not None and after_st is not None:\n",
    "    print(f\"  street_o blanks:      {before_st} ‚Üí {after_st}\")\n",
    "if before_pc is not None and after_pc is not None:\n",
    "    print(f\"  postal_code_o blanks: {before_pc} ‚Üí {after_pc}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713ef3b4",
   "metadata": {},
   "source": [
    "\n",
    "## **8) Build master ‚Üí pools_master_minimal.csv**\n",
    "\n",
    "**Why:** merge legacy rows and OSM-only rows into one clean, minimal table that carries the two enrichment tasks end-to-end:\n",
    "- ‚úÖ District IDs fixed to you canonical codes.\n",
    "- ‚úÖ ortsteil (neighborhood) + ortsteil_id joined from LOR polygons.\n",
    "\n",
    "**What we ensure:**\n",
    "- **pool_id** present for every row (legacy preserved; OSM generated deterministically).\n",
    "- **district_id** stays aligned with the 1100‚Ä¶ codes\n",
    "- **ortsteil** + **ortsteil_id** present for all rows that spatially fall into an LOR polygon.\n",
    "- `open_all_year` defaults to False when not clearly true-ish.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ac23053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Using legacy (enriched if available): legacy_enriched_with_lor.csv\n",
      "[info] Using OSM (public_named):            osm_public_named.csv\n",
      "[info] Using enrichment list (if present):  legacy_enrichment_list.csv (exists=True)\n",
      "[ok] Wrote pools_master_minimal.csv with 167 rows\n",
      "{'legacy_rows_in': 144, 'osm_public_named_in': 23, 'final_rows_out': 167}\n"
     ]
    }
   ],
   "source": [
    "# ‚Äî Build final master (legacy + legacy_enrichment_list + osm_public_named) ‚Äî self-contained & neighborhood-ready\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import unicodedata, re\n",
    "\n",
    "# ---------- robust path setup (works even if Config cell wasn't run) ----------\n",
    "def _p(x): return Path(str(x)).expanduser().resolve()\n",
    "\n",
    "LEGACY_ENRICHED_CSV = globals().get(\"LEGACY_ENRICHED_CSV\", _p(\"legacy_enriched_with_lor.csv\"))\n",
    "LEGACY_MAIN_CSV     = globals().get(\"LEGACY_MAIN_CSV\",     _p(\"berlin_pools_final_dataset.csv\"))\n",
    "LEGACY_ALT_CSV      = globals().get(\"LEGACY_ALT_CSV\",      _p(\"pools_data_cleaned.csv\"))\n",
    "OSM_PUBLIC_NAMED_CSV= globals().get(\"OSM_PUBLIC_NAMED_CSV\",_p(\"osm_public_named.csv\"))\n",
    "ENRICH_LIST_PATH    = _p(\"legacy_enrichment_list.csv\")\n",
    "\n",
    "def _first_existing(*paths: Path) -> Path | None:\n",
    "    for p in paths:\n",
    "        if p and isinstance(p, Path) and p.exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "LEGACY_IN = _first_existing(LEGACY_ENRICHED_CSV, LEGACY_MAIN_CSV, LEGACY_ALT_CSV)\n",
    "if not LEGACY_IN:\n",
    "    raise FileNotFoundError(\n",
    "        \"No legacy CSV found. Expected one of: \"\n",
    "        f\"{LEGACY_ENRICHED_CSV.name}, {LEGACY_MAIN_CSV.name}, {LEGACY_ALT_CSV.name}\"\n",
    "    )\n",
    "\n",
    "if not Path(OSM_PUBLIC_NAMED_CSV).exists():\n",
    "    raise FileNotFoundError(f\"OSM named CSV not found: {OSM_PUBLIC_NAMED_CSV}\")\n",
    "\n",
    "print(f\"[info] Using legacy (enriched if available): {Path(LEGACY_IN).name}\")\n",
    "print(f\"[info] Using OSM (public_named):            {Path(OSM_PUBLIC_NAMED_CSV).name}\")\n",
    "print(f\"[info] Using enrichment list (if present):  {ENRICH_LIST_PATH.name} (exists={ENRICH_LIST_PATH.exists()})\")\n",
    "\n",
    "legacy    = pd.read_csv(LEGACY_IN)\n",
    "osm_named = pd.read_csv(OSM_PUBLIC_NAMED_CSV)\n",
    "\n",
    "# ---------- helper utilities ----------\n",
    "def pickcol(df, *options):\n",
    "    for c in options:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def normalize_name(s: str) -> str:\n",
    "    s = \"\" if pd.isna(s) else str(s)\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s.lower()).strip()\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "# ---------- optional: enrichment from legacy_enrichment_list.csv ----------\n",
    "if ENRICH_LIST_PATH.exists():\n",
    "    enrich = pd.read_csv(ENRICH_LIST_PATH)\n",
    "    legacy_name_col = pickcol(legacy, \"name\",\"pool_name\") or \"name\"\n",
    "    legacy[\"_name_norm\"] = legacy[legacy_name_col].astype(str).map(normalize_name)\n",
    "\n",
    "    # Be defensive: if name_l column is missing, skip enrichment safely\n",
    "    if \"name_l\" in enrich.columns:\n",
    "        enrich[\"_name_norm\"] = enrich[\"name_l\"].astype(str).map(normalize_name)\n",
    "        osm_add_cols = [c for c in [\"website_o\",\"opening_hours_o\",\"wheelchair_o\",\"phone_o\",\"street_o\",\"postal_code_o\"] if c in enrich.columns]\n",
    "        enrich_small  = enrich[[\"_name_norm\"] + osm_add_cols].drop_duplicates(\"_name_norm\")\n",
    "        legacy_enriched = legacy.merge(enrich_small, on=\"_name_norm\", how=\"left\")\n",
    "\n",
    "        # Fill legacy columns from OSM-side columns only when legacy value is blank\n",
    "        fill_map = {\n",
    "            \"website\": (\"website_o\",),\n",
    "            \"opening_hours\": (\"opening_hours_o\",),\n",
    "            \"wheelchair\": (\"wheelchair_o\",),\n",
    "            \"street\": (\"street_o\",),\n",
    "            \"postal_code\": (\"postal_code_o\",),\n",
    "        }\n",
    "        for lcol, ocands in fill_map.items():\n",
    "            if lcol in legacy_enriched.columns:\n",
    "                for ocol in ocands:\n",
    "                    if ocol in legacy_enriched.columns:\n",
    "                        legacy_enriched[lcol] = legacy_enriched[lcol].where(\n",
    "                            legacy_enriched[lcol].notna() & (legacy_enriched[lcol].astype(str).str.strip() != \"\"),\n",
    "                            legacy_enriched[ocol]\n",
    "                        )\n",
    "        legacy_enriched.drop(columns=[c for c in [\"_name_norm\"] + osm_add_cols if c in legacy_enriched.columns],\n",
    "                             inplace=True, errors=\"ignore\")\n",
    "    else:\n",
    "        print(\"[info] enrichment list present but 'name_l' missing ‚Äî skipping enrich merge.\")\n",
    "        legacy_enriched = legacy.copy()\n",
    "else:\n",
    "    legacy_enriched = legacy.copy()\n",
    "\n",
    "# ---------- ID generation for OSM-only rows ----------\n",
    "def make_osm_pool_id(df):\n",
    "    id_col = pickcol(df, \"source_id\",\"osm_id\",\"element_id\",\"osmid\",\"@id\",\"id\")\n",
    "    if id_col:\n",
    "        base = df[id_col].fillna(\"\").astype(str).str.strip()\n",
    "    else:\n",
    "        base = pd.Series([\"\"] * len(df), index=df.index)\n",
    "    latc = pickcol(df, \"lat\",\"latitude\")\n",
    "    lonc = pickcol(df, \"lon\",\"longitude\")\n",
    "    if latc and lonc:\n",
    "        coord_sur = df[latc].round(6).astype(str) + \"_\" + df[lonc].round(6).astype(str)\n",
    "    else:\n",
    "        coord_sur = pd.Series([f\"{i:05d}\" for i in range(len(df))], index=df.index)\n",
    "    base = np.where(base == \"\", coord_sur, base)\n",
    "    return pd.Series(\"OSM_\" + pd.Series(base), index=df.index)\n",
    "\n",
    "# Helpers to read neighborhood fields (support old/new names)\n",
    "def get_neighborhood_cols(df):\n",
    "    neighborhood    = df.get(\"neighborhood\", df.get(\"ortsteil\", \"\"))\n",
    "    neighborhood_id = df.get(\"neighborhood_id\", df.get(\"ortsteil_id\", \"\"))\n",
    "    return neighborhood, neighborhood_id\n",
    "\n",
    "# ---------- legacy rows (already LOR-enriched) ----------\n",
    "legacy_nei, legacy_nei_id = get_neighborhood_cols(legacy_enriched)\n",
    "legacy_final = pd.DataFrame({\n",
    "    \"pool_id\":         legacy_enriched.get(pickcol(legacy_enriched, \"pool_id\",\"id\",\"legacy_id\"), \"L_\" + legacy_enriched.index.astype(str)),\n",
    "    \"district\":        legacy_enriched.get(\"district_label\", legacy_enriched.get(\"district\",\"\")),\n",
    "    \"district_id\":     legacy_enriched.get(\"district_id\", \"\"),\n",
    "    \"neighborhood\":    legacy_nei,\n",
    "    \"neighborhood_id\": legacy_nei_id,\n",
    "    \"name\":            legacy_enriched.get(pickcol(legacy_enriched, \"name\",\"pool_name\"), \"\"),\n",
    "    \"pool_type\":       legacy_enriched.get(pickcol(legacy_enriched, \"pool_type\",\"type\"), \"\"),\n",
    "    \"street\":          legacy_enriched.get(pickcol(legacy_enriched, \"street\",\"address\"), \"\"),\n",
    "    \"postal_code\":     legacy_enriched.get(pickcol(legacy_enriched, \"postal_code\",\"postcode\",\"zip\"), \"\"),\n",
    "    \"latitude\":        pd.to_numeric(legacy_enriched.get(pickcol(legacy_enriched, \"latitude\",\"lat\"), np.nan), errors=\"coerce\"),\n",
    "    \"longitude\":       pd.to_numeric(legacy_enriched.get(pickcol(legacy_enriched, \"longitude\",\"lon\"), np.nan), errors=\"coerce\"),\n",
    "    \"open_all_year\":   legacy_enriched.get(pickcol(legacy_enriched, \"open_all_year\",\"open_all_year_round\",\"open_year_round\"), pd.NA),\n",
    "})\n",
    "\n",
    "# ---------- OSM-only (unmatched) rows ‚Äî should already be LOR-enriched ----------\n",
    "osm_nei, osm_nei_id = get_neighborhood_cols(osm_named)\n",
    "osm_final = pd.DataFrame({\n",
    "    \"pool_id\":         make_osm_pool_id(osm_named),\n",
    "    \"district\":        osm_named.get(\"district_label\", osm_named.get(\"district\",\"\")),\n",
    "    \"district_id\":     osm_named.get(\"district_id\", \"\"),\n",
    "    \"neighborhood\":    osm_nei,\n",
    "    \"neighborhood_id\": osm_nei_id,\n",
    "    \"name\":            osm_named.get(\"name\", \"\"),\n",
    "    \"pool_type\":       osm_named.get(\"pool_type\", osm_named.get(\"leisure\", \"\")),\n",
    "    \"street\":          osm_named.get(\"street\", osm_named.get(\"addr_street\", \"\")),\n",
    "    \"postal_code\":     osm_named.get(\"postal_code\", osm_named.get(\"addr_postcode\", \"\")),\n",
    "    \"latitude\":        pd.to_numeric(osm_named.get(pickcol(osm_named, \"latitude\",\"lat\"), np.nan), errors=\"coerce\"),\n",
    "    \"longitude\":       pd.to_numeric(osm_named.get(pickcol(osm_named, \"longitude\",\"lon\"), np.nan), errors=\"coerce\"),\n",
    "    \"open_all_year\":   pd.NA,\n",
    "})\n",
    "\n",
    "# ---------- combine & tidy ----------\n",
    "final_master = pd.concat([legacy_final, osm_final], ignore_index=True)\n",
    "\n",
    "for c in [\"pool_id\",\"district\",\"district_id\",\"neighborhood\",\"neighborhood_id\",\"name\",\"pool_type\",\"street\",\"postal_code\"]:\n",
    "    if c in final_master.columns:\n",
    "        final_master[c] = final_master[c].astype(\"string\")\n",
    "for c in [\"latitude\",\"longitude\"]:\n",
    "    if c in final_master.columns:\n",
    "        final_master[c] = pd.to_numeric(final_master[c], errors=\"coerce\")\n",
    "\n",
    "if \"open_all_year\" in final_master.columns:\n",
    "    s = final_master[\"open_all_year\"].astype(str).str.strip().str.lower()\n",
    "    trueish = {\"true\",\"1\",\"yes\",\"y\",\"ja\"}\n",
    "    final_master[\"open_all_year\"] = s.map(lambda x: True if x in trueish else False)\n",
    "\n",
    "dup_ct = int(final_master[\"pool_id\"].duplicated().sum())\n",
    "if dup_ct:\n",
    "    print(f\"[warn] duplicate pool_id found: {dup_ct} ‚Üí keeping first\")\n",
    "    final_master = final_master.drop_duplicates(subset=[\"pool_id\"], keep=\"first\")\n",
    "\n",
    "final_master = final_master[[\n",
    "    \"pool_id\",\"district\",\"district_id\",\"neighborhood\",\"neighborhood_id\",\n",
    "    \"name\",\"pool_type\",\"street\",\"postal_code\",\"latitude\",\"longitude\",\"open_all_year\"\n",
    "]]\n",
    "final_master.to_csv(\"pools_master_minimal.csv\", index=False)\n",
    "print(\"[ok] Wrote pools_master_minimal.csv with\", len(final_master), \"rows\")\n",
    "print({\n",
    "    \"legacy_rows_in\": len(legacy),\n",
    "    \"osm_public_named_in\": len(osm_named),\n",
    "    \"final_rows_out\": len(final_master)\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2874a895",
   "metadata": {},
   "source": [
    "### 8.1) Final tweak ‚Äî renumber OSM-generated pool_ids to 1..400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9b5b920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] Renumbered 23 OSM IDs ‚Üí 1..400 with no collisions.\n",
      "     (coord-style: 23, surrogate: 0)\n",
      "Examples: {'OSM_52.459526_13.31501': '1', 'OSM_52.617812_13.33578': '2', 'OSM_52.454536_13.329503': '3', 'OSM_52.546965_13.555637': '4', 'OSM_52.427549_13.470163': '5'}\n"
     ]
    }
   ],
   "source": [
    "# Replace surrogate/coord OSM IDs with consecutive numbers 1..400 (no extra columns)\n",
    "# Targets:\n",
    "#   ‚Ä¢ OSM_sur_###              (surrogate)\n",
    "#   ‚Ä¢ OSM_<lat>_<lon>          (coordinate-style)\n",
    "import pandas as pd, re\n",
    "from pathlib import Path\n",
    "\n",
    "PATH = Path(\"pools_master_minimal.csv\")\n",
    "MIN_NUM, MAX_NUM = 1, 400   # adjust if you want a different range\n",
    "\n",
    "df = pd.read_csv(PATH, dtype={\"pool_id\": \"string\"})\n",
    "pool = df[\"pool_id\"].astype(\"string\")\n",
    "\n",
    "# Patterns\n",
    "coord_pat = re.compile(r\"^OSM_-?\\d+(?:\\.\\d+)?_-?\\d+(?:\\.\\d+)?$\")\n",
    "sur_pat   = re.compile(r\"^OSM_sur_\\d+$\")\n",
    "\n",
    "# Build target list in order of first appearance\n",
    "is_coord = pool.str.match(coord_pat, na=False)\n",
    "is_sur   = pool.str.match(sur_pat, na=False)\n",
    "targets  = pool[is_coord | is_sur].dropna().astype(str).unique().tolist()\n",
    "\n",
    "if not targets:\n",
    "    print(\"[info] No OSM surrogate/coord IDs found; nothing changed.\")\n",
    "else:\n",
    "    # Collision-avoidance: keep any existing non-target IDs\n",
    "    existing = set(pool[~pool.isin(targets)].astype(str).tolist())\n",
    "\n",
    "    # Available numbers in the chosen range that aren‚Äôt already used\n",
    "    available = [str(i) for i in range(MIN_NUM, MAX_NUM + 1) if str(i) not in existing]\n",
    "\n",
    "    if len(available) < len(targets):\n",
    "        raise RuntimeError(\n",
    "            f\"Not enough free numeric IDs in {MIN_NUM}..{MAX_NUM}. \"\n",
    "            f\"Need {len(targets)}, have {len(available)}. Increase MAX_NUM or free up IDs.\"\n",
    "        )\n",
    "\n",
    "    # Deterministic mapping (first appearance wins)\n",
    "    mapping = {old: new for old, new in zip(targets, available[:len(targets)])}\n",
    "\n",
    "    # Apply mapping\n",
    "    df[\"pool_id\"] = df[\"pool_id\"].astype(\"string\").map(lambda x: mapping.get(str(x), str(x)))\n",
    "    df.to_csv(PATH, index=False)\n",
    "\n",
    "    # Summary\n",
    "    coord_ct = int(is_coord.sum())\n",
    "    sur_ct   = int(is_sur.sum())\n",
    "    print(f\"[ok] Renumbered {len(mapping)} OSM IDs ‚Üí {MIN_NUM}..{MAX_NUM} with no collisions.\")\n",
    "    print(f\"     (coord-style: {coord_ct}, surrogate: {sur_ct})\")\n",
    "    print(\"Examples:\", dict(list(mapping.items())[:5]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7e96ae",
   "metadata": {},
   "source": [
    "\n",
    "## **9) Validation & Outputs**\n",
    "\n",
    "**Why:** quick QC on the final pools_master_minimal.csv to confirm IDs, geography, and basic fields look sane before hand-off.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ac75d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 167\n",
      "Missing pool_id: 0\n",
      "Missing district_id: 0\n",
      "Invalid district_id format (not 8 digits): 167\n",
      "district_id not in allowed set: 167\n",
      "district vs district_id label mismatches: 0\n",
      "Missing neighborhood: 0\n",
      "Invalid neighborhood_id format (not 4 digits): 167\n",
      "Duplicate pool_id: 0\n",
      "Leftover surrogate IDs (OSM_sur_###): 0\n",
      "Leftover coord-style IDs (OSM_<lat>_<lon>): 0\n",
      "Suspicious postal_code (non-empty but not 10xxx‚Äì14xxx): 0\n",
      "Missing latitude: 0 | Missing longitude: 0\n",
      "Out-of-range latitude: 0 | Out-of-range longitude: 0\n",
      "\n",
      "open_all_year dtype: bool\n",
      "open_all_year\n",
      "False    88\n",
      "True     79\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Rows with invalid district_id format (showing up to 5):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pool_id</th>\n",
       "      <th>district</th>\n",
       "      <th>district_id</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>neighborhood_id</th>\n",
       "      <th>name</th>\n",
       "      <th>street</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>open_all_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>472</td>\n",
       "      <td>Reinickendorf</td>\n",
       "      <td>11012012.0</td>\n",
       "      <td>L√ºbars</td>\n",
       "      <td>1208.0</td>\n",
       "      <td>Strandbad L√ºbars</td>\n",
       "      <td>Am Freibad 9</td>\n",
       "      <td>13469</td>\n",
       "      <td>52.61824</td>\n",
       "      <td>13.33519</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>473</td>\n",
       "      <td>Treptow-K√∂penick</td>\n",
       "      <td>11009009.0</td>\n",
       "      <td>Obersch√∂neweide</td>\n",
       "      <td>909.0</td>\n",
       "      <td>Kleine Schwimmhalle Wuhlheide</td>\n",
       "      <td>An der Wuhlheide 161</td>\n",
       "      <td>12459</td>\n",
       "      <td>52.45993</td>\n",
       "      <td>13.53965</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>474</td>\n",
       "      <td>Tempelhof-Sch√∂neberg</td>\n",
       "      <td>11007007.0</td>\n",
       "      <td>Mariendorf</td>\n",
       "      <td>704.0</td>\n",
       "      <td>Kombibad Mariendorf</td>\n",
       "      <td>Ankogelweg 95</td>\n",
       "      <td>12107</td>\n",
       "      <td>52.41972</td>\n",
       "      <td>13.40154</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>475</td>\n",
       "      <td>Lichtenberg</td>\n",
       "      <td>11011011.0</td>\n",
       "      <td>Fennpfuhl</td>\n",
       "      <td>1111.0</td>\n",
       "      <td>Schwimmhalle Anton-Saefkow-Platz</td>\n",
       "      <td>Anton-Saefkow-Platz 1</td>\n",
       "      <td>10369</td>\n",
       "      <td>52.53093</td>\n",
       "      <td>13.47184</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>476</td>\n",
       "      <td>Friedrichshain-Kreuzberg</td>\n",
       "      <td>11002002.0</td>\n",
       "      <td>Kreuzberg</td>\n",
       "      <td>202.0</td>\n",
       "      <td>Stadtbad Kreuzberg - Baerwaldbad</td>\n",
       "      <td>Baerwaldstra√üe 64-67</td>\n",
       "      <td>10961</td>\n",
       "      <td>52.49451</td>\n",
       "      <td>13.40432</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pool_id                  district  district_id     neighborhood  \\\n",
       "0      472             Reinickendorf   11012012.0           L√ºbars   \n",
       "1      473          Treptow-K√∂penick   11009009.0  Obersch√∂neweide   \n",
       "2      474      Tempelhof-Sch√∂neberg   11007007.0       Mariendorf   \n",
       "3      475               Lichtenberg   11011011.0        Fennpfuhl   \n",
       "4      476  Friedrichshain-Kreuzberg   11002002.0        Kreuzberg   \n",
       "\n",
       "   neighborhood_id                              name                 street  \\\n",
       "0           1208.0                  Strandbad L√ºbars           Am Freibad 9   \n",
       "1            909.0     Kleine Schwimmhalle Wuhlheide   An der Wuhlheide 161   \n",
       "2            704.0               Kombibad Mariendorf          Ankogelweg 95   \n",
       "3           1111.0  Schwimmhalle Anton-Saefkow-Platz  Anton-Saefkow-Platz 1   \n",
       "4            202.0  Stadtbad Kreuzberg - Baerwaldbad   Baerwaldstra√üe 64-67   \n",
       "\n",
       "   postal_code  latitude  longitude  open_all_year  \n",
       "0        13469  52.61824   13.33519          False  \n",
       "1        12459  52.45993   13.53965           True  \n",
       "2        12107  52.41972   13.40154           True  \n",
       "3        10369  52.53093   13.47184           True  \n",
       "4        10961  52.49451   13.40432           True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows with district_id not in allowed set (showing up to 5):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pool_id</th>\n",
       "      <th>district</th>\n",
       "      <th>district_id</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>neighborhood_id</th>\n",
       "      <th>name</th>\n",
       "      <th>street</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>open_all_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>472</td>\n",
       "      <td>Reinickendorf</td>\n",
       "      <td>11012012.0</td>\n",
       "      <td>L√ºbars</td>\n",
       "      <td>1208.0</td>\n",
       "      <td>Strandbad L√ºbars</td>\n",
       "      <td>Am Freibad 9</td>\n",
       "      <td>13469</td>\n",
       "      <td>52.61824</td>\n",
       "      <td>13.33519</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>473</td>\n",
       "      <td>Treptow-K√∂penick</td>\n",
       "      <td>11009009.0</td>\n",
       "      <td>Obersch√∂neweide</td>\n",
       "      <td>909.0</td>\n",
       "      <td>Kleine Schwimmhalle Wuhlheide</td>\n",
       "      <td>An der Wuhlheide 161</td>\n",
       "      <td>12459</td>\n",
       "      <td>52.45993</td>\n",
       "      <td>13.53965</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>474</td>\n",
       "      <td>Tempelhof-Sch√∂neberg</td>\n",
       "      <td>11007007.0</td>\n",
       "      <td>Mariendorf</td>\n",
       "      <td>704.0</td>\n",
       "      <td>Kombibad Mariendorf</td>\n",
       "      <td>Ankogelweg 95</td>\n",
       "      <td>12107</td>\n",
       "      <td>52.41972</td>\n",
       "      <td>13.40154</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>475</td>\n",
       "      <td>Lichtenberg</td>\n",
       "      <td>11011011.0</td>\n",
       "      <td>Fennpfuhl</td>\n",
       "      <td>1111.0</td>\n",
       "      <td>Schwimmhalle Anton-Saefkow-Platz</td>\n",
       "      <td>Anton-Saefkow-Platz 1</td>\n",
       "      <td>10369</td>\n",
       "      <td>52.53093</td>\n",
       "      <td>13.47184</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>476</td>\n",
       "      <td>Friedrichshain-Kreuzberg</td>\n",
       "      <td>11002002.0</td>\n",
       "      <td>Kreuzberg</td>\n",
       "      <td>202.0</td>\n",
       "      <td>Stadtbad Kreuzberg - Baerwaldbad</td>\n",
       "      <td>Baerwaldstra√üe 64-67</td>\n",
       "      <td>10961</td>\n",
       "      <td>52.49451</td>\n",
       "      <td>13.40432</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pool_id                  district  district_id     neighborhood  \\\n",
       "0      472             Reinickendorf   11012012.0           L√ºbars   \n",
       "1      473          Treptow-K√∂penick   11009009.0  Obersch√∂neweide   \n",
       "2      474      Tempelhof-Sch√∂neberg   11007007.0       Mariendorf   \n",
       "3      475               Lichtenberg   11011011.0        Fennpfuhl   \n",
       "4      476  Friedrichshain-Kreuzberg   11002002.0        Kreuzberg   \n",
       "\n",
       "   neighborhood_id                              name                 street  \\\n",
       "0           1208.0                  Strandbad L√ºbars           Am Freibad 9   \n",
       "1            909.0     Kleine Schwimmhalle Wuhlheide   An der Wuhlheide 161   \n",
       "2            704.0               Kombibad Mariendorf          Ankogelweg 95   \n",
       "3           1111.0  Schwimmhalle Anton-Saefkow-Platz  Anton-Saefkow-Platz 1   \n",
       "4            202.0  Stadtbad Kreuzberg - Baerwaldbad   Baerwaldstra√üe 64-67   \n",
       "\n",
       "   postal_code  latitude  longitude  open_all_year  \n",
       "0        13469  52.61824   13.33519          False  \n",
       "1        12459  52.45993   13.53965           True  \n",
       "2        12107  52.41972   13.40154           True  \n",
       "3        10369  52.53093   13.47184           True  \n",
       "4        10961  52.49451   13.40432           True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows with bad neighborhood_id (showing up to 5):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pool_id</th>\n",
       "      <th>district</th>\n",
       "      <th>district_id</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>neighborhood_id</th>\n",
       "      <th>name</th>\n",
       "      <th>street</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>open_all_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>472</td>\n",
       "      <td>Reinickendorf</td>\n",
       "      <td>11012012.0</td>\n",
       "      <td>L√ºbars</td>\n",
       "      <td>1208.0</td>\n",
       "      <td>Strandbad L√ºbars</td>\n",
       "      <td>Am Freibad 9</td>\n",
       "      <td>13469</td>\n",
       "      <td>52.61824</td>\n",
       "      <td>13.33519</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>473</td>\n",
       "      <td>Treptow-K√∂penick</td>\n",
       "      <td>11009009.0</td>\n",
       "      <td>Obersch√∂neweide</td>\n",
       "      <td>909.0</td>\n",
       "      <td>Kleine Schwimmhalle Wuhlheide</td>\n",
       "      <td>An der Wuhlheide 161</td>\n",
       "      <td>12459</td>\n",
       "      <td>52.45993</td>\n",
       "      <td>13.53965</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>474</td>\n",
       "      <td>Tempelhof-Sch√∂neberg</td>\n",
       "      <td>11007007.0</td>\n",
       "      <td>Mariendorf</td>\n",
       "      <td>704.0</td>\n",
       "      <td>Kombibad Mariendorf</td>\n",
       "      <td>Ankogelweg 95</td>\n",
       "      <td>12107</td>\n",
       "      <td>52.41972</td>\n",
       "      <td>13.40154</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>475</td>\n",
       "      <td>Lichtenberg</td>\n",
       "      <td>11011011.0</td>\n",
       "      <td>Fennpfuhl</td>\n",
       "      <td>1111.0</td>\n",
       "      <td>Schwimmhalle Anton-Saefkow-Platz</td>\n",
       "      <td>Anton-Saefkow-Platz 1</td>\n",
       "      <td>10369</td>\n",
       "      <td>52.53093</td>\n",
       "      <td>13.47184</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>476</td>\n",
       "      <td>Friedrichshain-Kreuzberg</td>\n",
       "      <td>11002002.0</td>\n",
       "      <td>Kreuzberg</td>\n",
       "      <td>202.0</td>\n",
       "      <td>Stadtbad Kreuzberg - Baerwaldbad</td>\n",
       "      <td>Baerwaldstra√üe 64-67</td>\n",
       "      <td>10961</td>\n",
       "      <td>52.49451</td>\n",
       "      <td>13.40432</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pool_id                  district  district_id     neighborhood  \\\n",
       "0      472             Reinickendorf   11012012.0           L√ºbars   \n",
       "1      473          Treptow-K√∂penick   11009009.0  Obersch√∂neweide   \n",
       "2      474      Tempelhof-Sch√∂neberg   11007007.0       Mariendorf   \n",
       "3      475               Lichtenberg   11011011.0        Fennpfuhl   \n",
       "4      476  Friedrichshain-Kreuzberg   11002002.0        Kreuzberg   \n",
       "\n",
       "   neighborhood_id                              name                 street  \\\n",
       "0           1208.0                  Strandbad L√ºbars           Am Freibad 9   \n",
       "1            909.0     Kleine Schwimmhalle Wuhlheide   An der Wuhlheide 161   \n",
       "2            704.0               Kombibad Mariendorf          Ankogelweg 95   \n",
       "3           1111.0  Schwimmhalle Anton-Saefkow-Platz  Anton-Saefkow-Platz 1   \n",
       "4            202.0  Stadtbad Kreuzberg - Baerwaldbad   Baerwaldstra√üe 64-67   \n",
       "\n",
       "   postal_code  latitude  longitude  open_all_year  \n",
       "0        13469  52.61824   13.33519          False  \n",
       "1        12459  52.45993   13.53965           True  \n",
       "2        12107  52.41972   13.40154           True  \n",
       "3        10369  52.53093   13.47184           True  \n",
       "4        10961  52.49451   13.40432           True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample (top 10):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pool_id</th>\n",
       "      <th>district</th>\n",
       "      <th>district_id</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>neighborhood_id</th>\n",
       "      <th>name</th>\n",
       "      <th>pool_type</th>\n",
       "      <th>street</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>open_all_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>472</td>\n",
       "      <td>Reinickendorf</td>\n",
       "      <td>11012012.0</td>\n",
       "      <td>L√ºbars</td>\n",
       "      <td>1208.0</td>\n",
       "      <td>Strandbad L√ºbars</td>\n",
       "      <td>Naturbad</td>\n",
       "      <td>Am Freibad 9</td>\n",
       "      <td>13469</td>\n",
       "      <td>52.61824</td>\n",
       "      <td>13.33519</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>473</td>\n",
       "      <td>Treptow-K√∂penick</td>\n",
       "      <td>11009009.0</td>\n",
       "      <td>Obersch√∂neweide</td>\n",
       "      <td>909.0</td>\n",
       "      <td>Kleine Schwimmhalle Wuhlheide</td>\n",
       "      <td>Hallenbad</td>\n",
       "      <td>An der Wuhlheide 161</td>\n",
       "      <td>12459</td>\n",
       "      <td>52.45993</td>\n",
       "      <td>13.53965</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>474</td>\n",
       "      <td>Tempelhof-Sch√∂neberg</td>\n",
       "      <td>11007007.0</td>\n",
       "      <td>Mariendorf</td>\n",
       "      <td>704.0</td>\n",
       "      <td>Kombibad Mariendorf</td>\n",
       "      <td>Kombibad</td>\n",
       "      <td>Ankogelweg 95</td>\n",
       "      <td>12107</td>\n",
       "      <td>52.41972</td>\n",
       "      <td>13.40154</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>475</td>\n",
       "      <td>Lichtenberg</td>\n",
       "      <td>11011011.0</td>\n",
       "      <td>Fennpfuhl</td>\n",
       "      <td>1111.0</td>\n",
       "      <td>Schwimmhalle Anton-Saefkow-Platz</td>\n",
       "      <td>Hallenbad</td>\n",
       "      <td>Anton-Saefkow-Platz 1</td>\n",
       "      <td>10369</td>\n",
       "      <td>52.53093</td>\n",
       "      <td>13.47184</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>476</td>\n",
       "      <td>Friedrichshain-Kreuzberg</td>\n",
       "      <td>11002002.0</td>\n",
       "      <td>Kreuzberg</td>\n",
       "      <td>202.0</td>\n",
       "      <td>Stadtbad Kreuzberg - Baerwaldbad</td>\n",
       "      <td>Hallenbad</td>\n",
       "      <td>Baerwaldstra√üe 64-67</td>\n",
       "      <td>10961</td>\n",
       "      <td>52.49451</td>\n",
       "      <td>13.40432</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>477</td>\n",
       "      <td>Pankow</td>\n",
       "      <td>11003003.0</td>\n",
       "      <td>Wei√üensee</td>\n",
       "      <td>302.0</td>\n",
       "      <td>Strandbad am Wei√üen See</td>\n",
       "      <td>Naturbad</td>\n",
       "      <td>Berliner Allee 155</td>\n",
       "      <td>13086</td>\n",
       "      <td>52.55396</td>\n",
       "      <td>13.46583</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>478</td>\n",
       "      <td>Spandau</td>\n",
       "      <td>11005005.0</td>\n",
       "      <td>Staaken</td>\n",
       "      <td>504.0</td>\n",
       "      <td>Sommerbad Staaken-West</td>\n",
       "      <td>Freibad</td>\n",
       "      <td>Brunsb√ºttler Damm 443</td>\n",
       "      <td>13591</td>\n",
       "      <td>52.53386</td>\n",
       "      <td>13.13123</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>479</td>\n",
       "      <td>Marzahn-Hellersdorf</td>\n",
       "      <td>11010010.0</td>\n",
       "      <td>Hellersdorf</td>\n",
       "      <td>1005.0</td>\n",
       "      <td>Schwimmhalle Kaulsdorf</td>\n",
       "      <td>Schulbad</td>\n",
       "      <td>Clara-Zetkin-Weg 13</td>\n",
       "      <td>12619</td>\n",
       "      <td>52.52080</td>\n",
       "      <td>13.58541</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>480</td>\n",
       "      <td>Neuk√∂lln</td>\n",
       "      <td>11008008.0</td>\n",
       "      <td>Neuk√∂lln</td>\n",
       "      <td>801.0</td>\n",
       "      <td>Sommerbad Neuk√∂lln</td>\n",
       "      <td>Freibad</td>\n",
       "      <td>Columbiadamm 160-180</td>\n",
       "      <td>10965</td>\n",
       "      <td>52.48025</td>\n",
       "      <td>13.41595</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>481</td>\n",
       "      <td>Steglitz-Zehlendorf</td>\n",
       "      <td>11006006.0</td>\n",
       "      <td>Lichterfelde</td>\n",
       "      <td>602.0</td>\n",
       "      <td>Schwimmhalle Finckensteinallee</td>\n",
       "      <td>Hallenbad</td>\n",
       "      <td>Finckensteinallee 73</td>\n",
       "      <td>12205</td>\n",
       "      <td>52.43225</td>\n",
       "      <td>13.29791</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pool_id                  district  district_id     neighborhood  \\\n",
       "0      472             Reinickendorf   11012012.0           L√ºbars   \n",
       "1      473          Treptow-K√∂penick   11009009.0  Obersch√∂neweide   \n",
       "2      474      Tempelhof-Sch√∂neberg   11007007.0       Mariendorf   \n",
       "3      475               Lichtenberg   11011011.0        Fennpfuhl   \n",
       "4      476  Friedrichshain-Kreuzberg   11002002.0        Kreuzberg   \n",
       "5      477                    Pankow   11003003.0        Wei√üensee   \n",
       "6      478                   Spandau   11005005.0          Staaken   \n",
       "7      479       Marzahn-Hellersdorf   11010010.0      Hellersdorf   \n",
       "8      480                  Neuk√∂lln   11008008.0         Neuk√∂lln   \n",
       "9      481       Steglitz-Zehlendorf   11006006.0     Lichterfelde   \n",
       "\n",
       "   neighborhood_id                              name  pool_type  \\\n",
       "0           1208.0                  Strandbad L√ºbars   Naturbad   \n",
       "1            909.0     Kleine Schwimmhalle Wuhlheide  Hallenbad   \n",
       "2            704.0               Kombibad Mariendorf   Kombibad   \n",
       "3           1111.0  Schwimmhalle Anton-Saefkow-Platz  Hallenbad   \n",
       "4            202.0  Stadtbad Kreuzberg - Baerwaldbad  Hallenbad   \n",
       "5            302.0           Strandbad am Wei√üen See   Naturbad   \n",
       "6            504.0            Sommerbad Staaken-West    Freibad   \n",
       "7           1005.0            Schwimmhalle Kaulsdorf   Schulbad   \n",
       "8            801.0                Sommerbad Neuk√∂lln    Freibad   \n",
       "9            602.0    Schwimmhalle Finckensteinallee  Hallenbad   \n",
       "\n",
       "                  street  postal_code  latitude  longitude  open_all_year  \n",
       "0           Am Freibad 9        13469  52.61824   13.33519          False  \n",
       "1   An der Wuhlheide 161        12459  52.45993   13.53965           True  \n",
       "2          Ankogelweg 95        12107  52.41972   13.40154           True  \n",
       "3  Anton-Saefkow-Platz 1        10369  52.53093   13.47184           True  \n",
       "4   Baerwaldstra√üe 64-67        10961  52.49451   13.40432           True  \n",
       "5     Berliner Allee 155        13086  52.55396   13.46583          False  \n",
       "6  Brunsb√ºttler Damm 443        13591  52.53386   13.13123          False  \n",
       "7    Clara-Zetkin-Weg 13        12619  52.52080   13.58541           True  \n",
       "8   Columbiadamm 160-180        10965  52.48025   13.41595          False  \n",
       "9   Finckensteinallee 73        12205  52.43225   13.29791          False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ‚Äî Quick QC on pools_master_minimal.csv (8-digit district_id + neighborhood) ===\n",
    "import pandas as pd, numpy as np, re\n",
    "from pathlib import Path\n",
    "\n",
    "PATH = Path(\"pools_master_minimal.csv\")\n",
    "df = pd.read_csv(PATH)\n",
    "\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Missing pool_id:\", int(df[\"pool_id\"].astype(str).str.strip().eq(\"\").sum()))\n",
    "\n",
    "# --- district_id: 8-digit codes, membership + label consistency\n",
    "# Prefer the mapping we defined earlier; otherwise fall back to a literal set\n",
    "if 'DISTRICT_ID_MAP' in globals():\n",
    "    allowed_ids = set(DISTRICT_ID_MAP.values())\n",
    "    id_to_label = {v: k for k, v in DISTRICT_ID_MAP.items()}\n",
    "else:\n",
    "    allowed_ids = {\n",
    "        \"11001001\",\"11002002\",\"11003003\",\"11004004\",\"11005005\",\"11006006\",\n",
    "        \"11007007\",\"11008008\",\"11009009\",\"11010010\",\"11011011\",\"11012012\"\n",
    "    }\n",
    "    id_to_label = {\n",
    "        \"11001001\":\"Mitte\",\"11002002\":\"Friedrichshain-Kreuzberg\",\"11003003\":\"Pankow\",\n",
    "        \"11004004\":\"Charlottenburg-Wilmersdorf\",\"11005005\":\"Spandau\",\"11006006\":\"Steglitz-Zehlendorf\",\n",
    "        \"11007007\":\"Tempelhof-Sch√∂neberg\",\"11008008\":\"Neuk√∂lln\",\"11009009\":\"Treptow-K√∂penick\",\n",
    "        \"11010010\":\"Marzahn-Hellersdorf\",\"11011011\":\"Lichtenberg\",\"11012012\":\"Reinickendorf\",\n",
    "    }\n",
    "\n",
    "did = df[\"district_id\"].astype(str).str.strip()\n",
    "print(\"Missing district_id:\", int(did.eq(\"\").sum()))\n",
    "\n",
    "bad_format_mask = ~did.str.fullmatch(r\"\\d{8}\") & did.ne(\"\")\n",
    "print(\"Invalid district_id format (not 8 digits):\", int(bad_format_mask.sum()))\n",
    "\n",
    "bad_value_mask = ~did.isin(list(allowed_ids)) & did.ne(\"\")\n",
    "print(\"district_id not in allowed set:\", int(bad_value_mask.sum()))\n",
    "\n",
    "# district label vs id consistency (if present)\n",
    "if \"district\" in df.columns:\n",
    "    dlabel = df[\"district\"].astype(str).str.strip()\n",
    "    expected_from_id = did.map(id_to_label).fillna(\"\")\n",
    "    mism_label = (dlabel.ne(\"\")) & (expected_from_id.ne(\"\")) & (dlabel != expected_from_id)\n",
    "    print(\"district vs district_id label mismatches:\", int(mism_label.sum()))\n",
    "else:\n",
    "    mism_label = pd.Series(False, index=df.index)\n",
    "\n",
    "# --- neighborhood checks (renamed from ortsteil)\n",
    "neigh_missing = int(df.get(\"neighborhood\", pd.Series([\"\"]*len(df))).astype(str).str.strip().eq(\"\").sum())\n",
    "print(\"Missing neighborhood:\", neigh_missing)\n",
    "\n",
    "neigh_id = df.get(\"neighborhood_id\", pd.Series([\"\"]*len(df))).astype(str).str.strip()\n",
    "bad_neigh_id = (neigh_id != \"\") & ~neigh_id.str.fullmatch(r\"\\d{4}\")\n",
    "print(\"Invalid neighborhood_id format (not 4 digits):\", int(bad_neigh_id.sum()))\n",
    "\n",
    "# --- basic uniqueness & formatting checks for pool_id\n",
    "dupe_ids = int(df[\"pool_id\"].duplicated().sum())\n",
    "print(\"Duplicate pool_id:\", dupe_ids)\n",
    "\n",
    "# leftovers: surrogate or coordinate-style OSM ids\n",
    "pool = df[\"pool_id\"].astype(str)\n",
    "left_sur = pool.str.fullmatch(r\"OSM_sur_\\d+\").sum()\n",
    "left_coord = pool.str.fullmatch(r\"OSM_-?\\d+(?:\\.\\d+)?_-?\\d+(?:\\.\\d+)?\").sum()\n",
    "print(\"Leftover surrogate IDs (OSM_sur_###):\", int(left_sur))\n",
    "print(\"Leftover coord-style IDs (OSM_<lat>_<lon>):\", int(left_coord))\n",
    "\n",
    "# --- postal code: Berlin 10xxx‚Äì14xxx (allow empty)\n",
    "pc = df[\"postal_code\"].astype(str).str.strip()\n",
    "bad_pc_mask = (pc != \"\") & ~pc.str.fullmatch(r\"1[0-4]\\d{3}\")\n",
    "print(\"Suspicious postal_code (non-empty but not 10xxx‚Äì14xxx):\", int(bad_pc_mask.sum()))\n",
    "\n",
    "# --- coordinates sanity\n",
    "lat = pd.to_numeric(df[\"latitude\"], errors=\"coerce\")\n",
    "lon = pd.to_numeric(df[\"longitude\"], errors=\"coerce\")\n",
    "missing_lat = int(lat.isna().sum())\n",
    "missing_lon = int(lon.isna().sum())\n",
    "out_lat = int((~lat.between(-90, 90)).sum())\n",
    "out_lon = int((~lon.between(-180, 180)).sum())\n",
    "print(\"Missing latitude:\", missing_lat, \"| Missing longitude:\", missing_lon)\n",
    "print(\"Out-of-range latitude:\", out_lat, \"| Out-of-range longitude:\", out_lon)\n",
    "\n",
    "# --- open_all_year should be boolean/text. Show distribution.\n",
    "print(\"\\nopen_all_year dtype:\", df[\"open_all_year\"].dtype)\n",
    "print(df[\"open_all_year\"].value_counts(dropna=False))\n",
    "\n",
    "# --- Peek at problems (up to 5 rows each) ---\n",
    "def peek(mask, cols, title):\n",
    "    m = df[mask]\n",
    "    if not m.empty:\n",
    "        print(f\"\\n{title} (showing up to 5):\")\n",
    "        display(m[cols].head(5))\n",
    "\n",
    "core_cols = [\"pool_id\",\"district\",\"district_id\",\"neighborhood\",\"neighborhood_id\",\n",
    "             \"name\",\"street\",\"postal_code\",\"latitude\",\"longitude\",\"open_all_year\"]\n",
    "\n",
    "peek(did.eq(\"\"), core_cols, \"Rows with missing district_id\")\n",
    "peek(bad_format_mask, core_cols, \"Rows with invalid district_id format\")\n",
    "peek(bad_value_mask, core_cols, \"Rows with district_id not in allowed set\")\n",
    "peek(mism_label, core_cols, \"district label vs id mismatches\")\n",
    "\n",
    "peek(df.get(\"neighborhood\",\"\").astype(str).str.strip().eq(\"\"), core_cols, \"Rows with missing neighborhood\")\n",
    "peek(bad_neigh_id, core_cols, \"Rows with bad neighborhood_id\")\n",
    "\n",
    "peek(pool.str.fullmatch(r\"OSM_sur_\\d+\"), core_cols, \"Leftover surrogate pool_id\")\n",
    "peek(pool.str.fullmatch(r\"OSM_-?\\d+(?:\\.\\d+)?_-?\\d+(?:\\.\\d+)?\"), core_cols, \"Leftover coord-style pool_id\")\n",
    "\n",
    "peek(bad_pc_mask, core_cols, \"Rows with suspicious postal_code\")\n",
    "peek(lat.isna() | lon.isna(), core_cols, \"Rows with missing coordinates\")\n",
    "peek((~lat.between(-90, 90)) | (~lon.between(-180, 180)), core_cols, \"Rows with out-of-range coordinates\")\n",
    "\n",
    "print(\"\\nSample (top 10):\")\n",
    "display(df.head(10))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a134d50c",
   "metadata": {},
   "source": [
    "# **10) === Finalize schema & save (from CSV) ===**\n",
    "\n",
    "**Why:** lock the final table into a clean, predictable schema (columns present, types coerced, IDs padded) before hand-off or DB import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a4aa040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pool_id             object\n",
      "name                object\n",
      "pool_type           object\n",
      "street              object\n",
      "postal_code         object\n",
      "latitude           float64\n",
      "longitude          float64\n",
      "open_all_year       object\n",
      "district            object\n",
      "district_id         object\n",
      "neighborhood        object\n",
      "neighborhood_id     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"pools_master_minimal.csv\")\n",
    "\n",
    "# --- Backward-compat: if old columns exist, rename to new ones ---\n",
    "rename_map = {}\n",
    "if \"ortsteil\" in df.columns and \"neighborhood\" not in df.columns:\n",
    "    rename_map[\"ortsteil\"] = \"neighborhood\"\n",
    "if \"ortsteil_id\" in df.columns and \"neighborhood_id\" not in df.columns:\n",
    "    rename_map[\"ortsteil_id\"] = \"neighborhood_id\"\n",
    "if rename_map:\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "# --- required columns (now includes district, neighborhood, neighborhood_id) ---\n",
    "need_cols = [\n",
    "    \"pool_id\",\"name\",\"pool_type\",\"street\",\"postal_code\",\n",
    "    \"latitude\",\"longitude\",\"open_all_year\",\n",
    "    \"district\",\"district_id\",\"neighborhood\",\"neighborhood_id\",\n",
    "]\n",
    "\n",
    "# ensure all columns exist\n",
    "for c in need_cols:\n",
    "    if c not in df.columns:\n",
    "        df[c] = pd.NA if c in [\"latitude\",\"longitude\"] else \"\"\n",
    "\n",
    "# coords ‚Üí float64\n",
    "df[\"latitude\"]  = pd.to_numeric(df[\"latitude\"], errors=\"coerce\").astype(\"float64\")\n",
    "df[\"longitude\"] = pd.to_numeric(df[\"longitude\"], errors=\"coerce\").astype(\"float64\")\n",
    "\n",
    "# object/text columns (same style as your code, but null-safe)\n",
    "obj_cols = [\n",
    "    \"pool_id\",\"name\",\"pool_type\",\"street\",\"postal_code\",\n",
    "    \"open_all_year\",\"district\",\"district_id\",\"neighborhood\",\"neighborhood_id\"\n",
    "]\n",
    "for c in obj_cols:\n",
    "    df[c] = df[c].astype(\"string\").fillna(\"\").astype(\"object\")\n",
    "\n",
    "# default blanks for open_all_year -> \"False\" (keep as text, same as your original)\n",
    "df.loc[df[\"open_all_year\"].astype(str).str.strip().eq(\"\"), \"open_all_year\"] = \"False\"\n",
    "\n",
    "# pad IDs only when non-empty; strip accidental \".0\"\n",
    "mask_did = df[\"district_id\"].astype(str).str.strip().ne(\"\")\n",
    "df.loc[mask_did, \"district_id\"] = (\n",
    "    df.loc[mask_did, \"district_id\"].astype(str).str.replace(\".0\",\"\",regex=False).str.zfill(8)\n",
    ")\n",
    "\n",
    "mask_nei = df[\"neighborhood_id\"].astype(str).str.strip().ne(\"\")\n",
    "df.loc[mask_nei, \"neighborhood_id\"] = (\n",
    "    df.loc[mask_nei, \"neighborhood_id\"].astype(str).str.replace(\".0\",\"\",regex=False).str.zfill(4)\n",
    ")\n",
    "\n",
    "# reorder & save\n",
    "df = df[need_cols]\n",
    "df.to_csv(\"pools_master_minimal.csv\", index=False)\n",
    "\n",
    "print(df.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8741a1c",
   "metadata": {},
   "source": [
    "# **10.2 Populate pools_refactored in Postgres** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2996676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[max string lengths]\n",
      "  pool_id          ‚Üí 5\n",
      "  name             ‚Üí 49\n",
      "  pool_type        ‚Üí 21\n",
      "  street           ‚Üí 28\n",
      "  postal_code      ‚Üí 5\n",
      "  district         ‚Üí 26\n",
      "  district_id      ‚Üí 8\n",
      "  neighborhood     ‚Üí 20\n",
      "  neighborhood_id  ‚Üí 4\n",
      "Table 'pools_refactored' created or already exists.\n",
      "DataFrame sent to PostgreSQL (.to_sql append).\n"
     ]
    }
   ],
   "source": [
    "# 10.2 ‚Äî Create + populate berlin_source_data.pools_refactored\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# --- Load your final CSV with stable dtypes ---\n",
    "csv_path = \"pools_master_minimal.csv\"\n",
    "df = pd.read_csv(\n",
    "    csv_path,\n",
    "    dtype={\n",
    "        \"pool_id\": \"string\",\n",
    "        \"name\": \"string\",\n",
    "        \"pool_type\": \"string\",\n",
    "        \"street\": \"string\",\n",
    "        \"postal_code\": \"string\",\n",
    "        \"district\": \"string\",\n",
    "        \"district_id\": \"string\",\n",
    "        \"neighborhood\": \"string\",      # <-- new\n",
    "        \"neighborhood_id\": \"string\",   # <-- new\n",
    "    }\n",
    ")\n",
    "\n",
    "# Backward-compat (if older CSV still had 'ortsteil*')\n",
    "if \"neighborhood\" not in df.columns and \"ortsteil\" in df.columns:\n",
    "    df[\"neighborhood\"] = df[\"ortsteil\"].astype(\"string\")\n",
    "if \"neighborhood_id\" not in df.columns and \"ortsteil_id\" in df.columns:\n",
    "    df[\"neighborhood_id\"] = df[\"ortsteil_id\"].astype(\"string\")\n",
    "\n",
    "# Coerce coords + boolean\n",
    "df[\"latitude\"]  = pd.to_numeric(df.get(\"latitude\"), errors=\"coerce\")\n",
    "df[\"longitude\"] = pd.to_numeric(df.get(\"longitude\"), errors=\"coerce\")\n",
    "\n",
    "trueish = {\"true\",\"1\",\"yes\",\"y\",\"ja\",\"wahr\"}\n",
    "s = df.get(\"open_all_year\", False).astype(str).str.strip().str.lower()\n",
    "df[\"open_all_year\"] = s.map(lambda x: True if x in trueish else False)\n",
    "\n",
    "# Drop duplicate PKs to avoid insert errors\n",
    "if \"pool_id\" in df.columns:\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates(subset=[\"pool_id\"], keep=\"first\")\n",
    "    after = len(df)\n",
    "    if before != after:\n",
    "        print(f\"[info] dropped {before-after} duplicate rows by pool_id\")\n",
    "\n",
    "# (Optional) peek max string lengths to spot columns that might need larger VARCHARs\n",
    "print(\"\\n[max string lengths]\")\n",
    "for col in [\"pool_id\",\"name\",\"pool_type\",\"street\",\"postal_code\",\n",
    "            \"district\",\"district_id\",\"neighborhood\",\"neighborhood_id\"]:\n",
    "    if col in df.columns:\n",
    "        m = int(df[col].fillna(\"\").astype(str).str.len().max())\n",
    "        print(f\"  {col:16s} ‚Üí {m}\")\n",
    "\n",
    "# --- Connect to Postgres (your exact block) ---\n",
    "# Connect to postgres DB\n",
    "user_name='michalina_pacholska'\n",
    "password='9iqk3zPUATp43zVl'\n",
    "# Conection\n",
    "host = 'localhost'\n",
    "port = '5433'\n",
    "database = 'layereddb'\n",
    "schema='berlin_source_data'\n",
    "\n",
    "#connection to db after you opened tunnel\n",
    "engine = create_engine(f'postgresql+psycopg2://{user_name}:{password}@{host}:{port}/{database}')\n",
    "\n",
    "# --- Create target table (if not exists) ---\n",
    "create_table_query = f\"\"\"\n",
    "CREATE SCHEMA IF NOT EXISTS {schema};\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS {schema}.pools_refactored (\n",
    "    pool_id           VARCHAR(32) PRIMARY KEY,\n",
    "    name              VARCHAR(200) NOT NULL,\n",
    "    pool_type         VARCHAR(100),\n",
    "    street            VARCHAR(200),\n",
    "    postal_code       VARCHAR(10),\n",
    "    latitude          DECIMAL(9,6),\n",
    "    longitude         DECIMAL(9,6),\n",
    "    open_all_year     BOOLEAN NOT NULL DEFAULT FALSE,\n",
    "    district          VARCHAR(100),\n",
    "    district_id       VARCHAR(8),\n",
    "    neighborhood      VARCHAR(100),\n",
    "    neighborhood_id   VARCHAR(4),\n",
    "    CONSTRAINT pools_refactored_district_fk\n",
    "        FOREIGN KEY (district_id)\n",
    "        REFERENCES {schema}.districts(district_id)\n",
    "        ON DELETE RESTRICT\n",
    "        ON UPDATE CASCADE\n",
    ");\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS pools_refactored_district_id_idx\n",
    "  ON {schema}.pools_refactored(district_id);\n",
    "CREATE INDEX IF NOT EXISTS pools_refactored_neighborhood_id_idx\n",
    "  ON {schema}.pools_refactored(neighborhood_id);\n",
    "\"\"\"\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    for stmt in create_table_query.strip().split(\";\\n\\n\"):\n",
    "        if stmt.strip():\n",
    "            conn.execute(text(stmt))\n",
    "print(\"Table 'pools_refactored' created or already exists.\")\n",
    "\n",
    "# --- Insert data ---\n",
    "# If you want to fully refresh, uncomment:\n",
    "# with engine.begin() as conn:\n",
    "#     conn.execute(text(f\"TRUNCATE TABLE {schema}.pools_refactored\"))\n",
    "\n",
    "df.to_sql(\n",
    "    'pools_refactored',\n",
    "    engine,\n",
    "    schema=schema,\n",
    "    if_exists='append',  # keep table, just insert rows\n",
    "    index=False,\n",
    "    method='multi',\n",
    "    chunksize=2000,\n",
    ")\n",
    "print(\"DataFrame sent to PostgreSQL (.to_sql append).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2798d379",
   "metadata": {},
   "source": [
    "# **10.2 --- Quick verification query (top 10) ---** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ff7ec05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pool_id</th>\n",
       "      <th>name</th>\n",
       "      <th>district</th>\n",
       "      <th>district_id</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>neighborhood_id</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>open_all_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Schwimmschule Wassermeloni</td>\n",
       "      <td>Steglitz-Zehlendorf</td>\n",
       "      <td>11006006</td>\n",
       "      <td>Steglitz</td>\n",
       "      <td>0601</td>\n",
       "      <td>12165</td>\n",
       "      <td>52.459527</td>\n",
       "      <td>13.315011</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Seebad Friedrichshagen</td>\n",
       "      <td>Treptow-K√∂penick</td>\n",
       "      <td>11009009</td>\n",
       "      <td>Friedrichshagen</td>\n",
       "      <td>0911</td>\n",
       "      <td>12587</td>\n",
       "      <td>52.445950</td>\n",
       "      <td>13.630620</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10223</td>\n",
       "      <td>DRK Kliniken Berlin</td>\n",
       "      <td>Tempelhof-Sch√∂neberg</td>\n",
       "      <td>11007007</td>\n",
       "      <td>Mariendorf</td>\n",
       "      <td>0704</td>\n",
       "      <td>12109</td>\n",
       "      <td>52.439756</td>\n",
       "      <td>13.397093</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10224</td>\n",
       "      <td>Wannseeschule</td>\n",
       "      <td>Steglitz-Zehlendorf</td>\n",
       "      <td>11006006</td>\n",
       "      <td>Wannsee</td>\n",
       "      <td>0607</td>\n",
       "      <td>14109</td>\n",
       "      <td>52.430556</td>\n",
       "      <td>13.161794</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1023</td>\n",
       "      <td>Strandbad Jungfernheide</td>\n",
       "      <td>Charlottenburg-Wilmersdorf</td>\n",
       "      <td>11004004</td>\n",
       "      <td>Charlottenburg-Nord</td>\n",
       "      <td>0406</td>\n",
       "      <td>13629</td>\n",
       "      <td>52.543940</td>\n",
       "      <td>13.274310</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10231</td>\n",
       "      <td>Evangelisches Hubertuskrankenhaus</td>\n",
       "      <td>Steglitz-Zehlendorf</td>\n",
       "      <td>11006006</td>\n",
       "      <td>Nikolassee</td>\n",
       "      <td>0606</td>\n",
       "      <td>14129</td>\n",
       "      <td>52.431208</td>\n",
       "      <td>13.219199</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10232</td>\n",
       "      <td>Schwimmbad Berlin Steglitz</td>\n",
       "      <td>Steglitz-Zehlendorf</td>\n",
       "      <td>11006006</td>\n",
       "      <td>Steglitz</td>\n",
       "      <td>0601</td>\n",
       "      <td>12165</td>\n",
       "      <td>52.459351</td>\n",
       "      <td>13.314782</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10238</td>\n",
       "      <td>Badestelle Reiherwerder am Forsthaus</td>\n",
       "      <td>Reinickendorf</td>\n",
       "      <td>11012012</td>\n",
       "      <td>Tegel</td>\n",
       "      <td>1202</td>\n",
       "      <td>13505</td>\n",
       "      <td>52.585398</td>\n",
       "      <td>13.255380</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1024</td>\n",
       "      <td>Strandbad Halensee</td>\n",
       "      <td>Charlottenburg-Wilmersdorf</td>\n",
       "      <td>11004004</td>\n",
       "      <td>Grunewald</td>\n",
       "      <td>0404</td>\n",
       "      <td>14193</td>\n",
       "      <td>52.493990</td>\n",
       "      <td>13.283080</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1025</td>\n",
       "      <td>Stadtbad Charlottenburg - Alte Halle</td>\n",
       "      <td>Charlottenburg-Wilmersdorf</td>\n",
       "      <td>11004004</td>\n",
       "      <td>Charlottenburg</td>\n",
       "      <td>0401</td>\n",
       "      <td>10585</td>\n",
       "      <td>52.514360</td>\n",
       "      <td>13.309560</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pool_id                                  name                    district  \\\n",
       "0       1            Schwimmschule Wassermeloni         Steglitz-Zehlendorf   \n",
       "1      10                Seebad Friedrichshagen            Treptow-K√∂penick   \n",
       "2   10223                   DRK Kliniken Berlin        Tempelhof-Sch√∂neberg   \n",
       "3   10224                         Wannseeschule         Steglitz-Zehlendorf   \n",
       "4    1023               Strandbad Jungfernheide  Charlottenburg-Wilmersdorf   \n",
       "5   10231     Evangelisches Hubertuskrankenhaus         Steglitz-Zehlendorf   \n",
       "6   10232            Schwimmbad Berlin Steglitz         Steglitz-Zehlendorf   \n",
       "7   10238  Badestelle Reiherwerder am Forsthaus               Reinickendorf   \n",
       "8    1024                    Strandbad Halensee  Charlottenburg-Wilmersdorf   \n",
       "9    1025  Stadtbad Charlottenburg - Alte Halle  Charlottenburg-Wilmersdorf   \n",
       "\n",
       "  district_id         neighborhood neighborhood_id postal_code   latitude  \\\n",
       "0    11006006             Steglitz            0601       12165  52.459527   \n",
       "1    11009009      Friedrichshagen            0911       12587  52.445950   \n",
       "2    11007007           Mariendorf            0704       12109  52.439756   \n",
       "3    11006006              Wannsee            0607       14109  52.430556   \n",
       "4    11004004  Charlottenburg-Nord            0406       13629  52.543940   \n",
       "5    11006006           Nikolassee            0606       14129  52.431208   \n",
       "6    11006006             Steglitz            0601       12165  52.459351   \n",
       "7    11012012                Tegel            1202       13505  52.585398   \n",
       "8    11004004            Grunewald            0404       14193  52.493990   \n",
       "9    11004004       Charlottenburg            0401       10585  52.514360   \n",
       "\n",
       "   longitude  open_all_year  \n",
       "0  13.315011          False  \n",
       "1  13.630620          False  \n",
       "2  13.397093          False  \n",
       "3  13.161794          False  \n",
       "4  13.274310          False  \n",
       "5  13.219199           True  \n",
       "6  13.314782           True  \n",
       "7  13.255380          False  \n",
       "8  13.283080          False  \n",
       "9  13.309560           True  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with engine.connect() as conn:\n",
    "    preview = pd.read_sql(\n",
    "        text(f\"\"\"\n",
    "            SELECT pool_id, name, district, district_id, neighborhood, neighborhood_id,\n",
    "                   postal_code, latitude, longitude, open_all_year\n",
    "            FROM {schema}.pools_refactored\n",
    "            ORDER BY pool_id\n",
    "            LIMIT 10\n",
    "        \"\"\"),\n",
    "        conn\n",
    "    )\n",
    "preview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbfa393",
   "metadata": {},
   "source": [
    "# Berlin Pools ‚Äî Refactoring Outcome (Legacy ‚ÜîÔ∏é OSM)\n",
    "\n",
    "**Final stance**  \n",
    "- **OSM does not fully replace the legacy source today** (names too sparse; specs like length/depth largely missing).  \n",
    "- **Best choice: Hybrid enrichment.** Keep **legacy** as the backbone; use **OSM** to **fill** contacts/websites (fallback), **opening_hours** (when present), **accessibility (*wheelchair*)**, and **address fixes**.  \n",
    "- **Specs (length/depth/lanes)** stay **separate/parked** until we secure a reliable source.\n",
    "\n",
    "**Coverage reality check (OSM, current snapshot)**  \n",
    "- Most enrichment tags are empty or ~**‚â§1%** filled.  \n",
    "- **wheelchair:** ~**14%** coverage ‚Üí useful as an accessibility flag.  \n",
    "- **website:** ~**9%** coverage, but **primary** website data comes from our original source ([baederleben.de](https://baederleben.de/abfragen/baeder-suche.php) ‚Üí official B√§der pages); OSM is a **secondary fallback**.  \n",
    "- Decision remains aligned with my colleague‚Äôs recommendation; **most relevant columns remain unchanged**.\n",
    "\n",
    "**What stays in the backbone (from legacy and lor_ortsteile.geojson)**  \n",
    "- Canonical **name**, **pool_type**, **street**, **postal_code**, **district / neighborhood IDs**, and core **geos**.  \n",
    "- Stable dtypes; normalized IDs; consistent Bezirk/Ortsteil mapping.\n",
    "\n",
    "**What OSM adds (when present)**  \n",
    "- **Website** (fallback only, if legacy lacks it or confirms the same URL).  \n",
    "- **opening_hours** (store as provided; do not rely on completeness).  \n",
    "- **wheelchair** (capture as accessibility flag).  \n",
    "- **Address nits & alt names** (use to fix obvious errors or fill blanks).  \n",
    "- **OSM identifier** saved for traceability.\n",
    "\n",
    "**What we deliberately keep separate / defer**  \n",
    "- **length, depth, lane counts, indoor/outdoor specs** ‚Üí **not reliable in OSM now**; keep in a **separate specs table** (or backlog) until a trustworthy upstream is identified.\n",
    "\n",
    "**Quality & rules applied**  \n",
    "- Deduped by **normalized names** + **proximity**; excluded **access=private**.  \n",
    "- Address sanity checks; fixed residual **district/neighborhood IDs**; enforced **string** dtypes for IDs.  \n",
    "- Merge logic favors **legacy** when both exist; OSM used to **fill gaps or correct obvious issues**.  \n",
    "- Stored **OSM_id** for auditing; scripted Overpass pull limited to Berlin; added caching to avoid churn.\n",
    "\n",
    "**Risks & mitigations**  \n",
    "- **OSM sparsity/volatility:** Keep OSM as enrichment only; log deltas on refresh.  \n",
    "- **Website consistency:** Prefer **official B√§derbetrieb** URLs; use OSM only if missing in legacy.  \n",
    "- **Specs reliability:** Parked until a curated or official specs source is identified.\n",
    "\n",
    "**Next steps**  \n",
    "1. Run the **hybrid end_to_end pipeline** to publish the enriched layer (legacy backbone + OSM add-ons).  \n",
    "2. Open a **mini-backlog** to source **specs** (e.g., structured scrape/API from official B√§der pages).  \n",
    "3. Schedule **periodic OSM refresh** (quarterly) with a coverage report (*wheelchair/website/hours*) to revisit the decision when fill rates improve.\n",
    "\n",
    "**TL;DR:** Keep **legacy as truth**; add **OSM** where it‚Äôs strong (wheelchair, some websites/hours, address fixes). **Park specs.**\n",
    "\"\"\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
