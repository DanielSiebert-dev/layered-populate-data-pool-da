{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93a51fdb",
   "metadata": {},
   "source": [
    "## Cleaning & parsing\n",
    "    * Cleans data from berlin_venues_raw created by the venues_craper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a784169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaded variable 'df' from URI\n",
    "\n",
    "df = pd.read_csv(r'/Users/giovanigoltara/Documents/webeet/layered-populate-data-pool-da/venues/sources/berlin_venues_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab3cde20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing data in columns: 'name', 'district'\n",
    "df = df.dropna(subset=['name', 'district'])\n",
    "\n",
    "# Drop duplicate rows across all columns\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39bb2987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening Hours parser code\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "DAY_ORDER = [\"Mo\", \"Tu\", \"We\", \"Th\", \"Fr\", \"Sa\", \"Su\"]\n",
    "\n",
    "def _normalize_day_text(s: str) -> str:\n",
    "    t = (s or \"\").strip()\n",
    "    t = t.replace(\"–\", \"-\").replace(\"—\", \"-\").replace(\"−\", \"-\").replace(\" to \", \"-\")\n",
    "    low = t.lower()\n",
    "    repl = [\n",
    "        (r'\\bpublic\\s*holidays?\\b', 'PH'), (r'\\bph\\b', 'PH'),\n",
    "        (r'\\bmonday\\b', 'Mo'), (r'\\bmon\\b', 'Mo'), (r'\\bmo\\b', 'Mo'),\n",
    "        (r'\\btuesday\\b', 'Tu'), (r'\\btues\\b', 'Tu'), (r'\\btue\\b', 'Tu'), (r'\\btu\\b', 'Tu'),\n",
    "        (r'\\bwednesday\\b', 'We'), (r'\\bweds\\b', 'We'), (r'\\bwed\\b', 'We'), (r'\\bwe\\b', 'We'),\n",
    "        (r'\\bthursday\\b', 'Th'), (r'\\bthurs\\b', 'Th'), (r'\\bthur\\b', 'Th'), (r'\\bthu\\b', 'Th'),\n",
    "        (r'\\bfriday\\b', 'Fr'), (r'\\bfri\\b', 'Fr'), (r'\\bfr\\b', 'Fr'),\n",
    "        (r'\\bsaturday\\b', 'Sa'), (r'\\bsat\\b', 'Sa'), (r'\\bsa\\b', 'Sa'),\n",
    "        (r'\\bsunday\\b', 'Su'), (r'\\bsun\\b', 'Su'), (r'\\bsu\\b', 'Su'),\n",
    "    ]\n",
    "    for pat, rep in repl:\n",
    "        low = re.sub(pat, rep, low)\n",
    "    return re.sub(r'\\s+', ' ', low).strip()\n",
    "\n",
    "def _expand_days_token(tok: str):\n",
    "    tok = tok.strip()\n",
    "    if not tok: return []\n",
    "    if tok == \"PH\": return [\"PH\"]\n",
    "    if \"-\" in tok:\n",
    "        a, b = [x.strip() for x in tok.split(\"-\", 1)]\n",
    "        if a in DAY_ORDER and b in DAY_ORDER:\n",
    "            ai, bi = DAY_ORDER.index(a), DAY_ORDER.index(b)\n",
    "            return DAY_ORDER[ai:bi+1] if ai <= bi else DAY_ORDER[ai:] + DAY_ORDER[:bi+1]\n",
    "        return []\n",
    "    return [tok] if tok in DAY_ORDER else []\n",
    "\n",
    "def _parse_time_value(t: str) -> str:\n",
    "    t = t.strip()\n",
    "    if t.lower() in {\"midnight\", \"24\", \"24:00\"}: return \"00:00\"\n",
    "    if t.lower() in {\"noon\", \"12pm\"}: return \"12:00\"\n",
    "    if re.match(r'^\\d{1,2}:\\d{1,2}$', t):\n",
    "        h, m = t.split(\":\"); return f\"{int(h):02d}:{int(m):02d}\"\n",
    "    if re.match(r'^\\d{1,2}$', t):\n",
    "        return f\"{int(t):02d}:00\"\n",
    "    return t\n",
    "\n",
    "def _parse_segment(seg: str):\n",
    "    seg = _normalize_day_text(seg)\n",
    "    mdig = re.search(r'\\d', seg)\n",
    "    day_part = seg[:mdig.start()].strip().rstrip(\",\") if mdig else seg\n",
    "    times_part = seg[mdig.start():].strip() if mdig else \"\"\n",
    "    if day_part:\n",
    "        day_tokens = [p.strip() for p in day_part.split(\",\") if p.strip()]\n",
    "        days = []\n",
    "        for tok in day_tokens:\n",
    "            days += _expand_days_token(tok)\n",
    "    else:\n",
    "        days = DAY_ORDER[:]\n",
    "    times = []\n",
    "    if times_part:\n",
    "        for tseg in [x.strip() for x in re.split(r',|\\s*/\\s*', times_part) if x.strip()]:\n",
    "            if tseg.endswith(\"+\"):\n",
    "                times.append([_parse_time_value(tseg[:-1]), \"late\"])\n",
    "            elif \"-\" in tseg:\n",
    "                a, b = tseg.split(\"-\", 1)\n",
    "                times.append([_parse_time_value(a), _parse_time_value(b)])\n",
    "            else:\n",
    "                tok = tseg.lower()\n",
    "                if tok in {\"closed\", \"off\"}:\n",
    "                    times.append([\"closed\", \"closed\"])\n",
    "                else:\n",
    "                    times.append([_parse_time_value(tseg), \"\"])\n",
    "    return days, times\n",
    "\n",
    "def opening_hours_to_dict(text: str):\n",
    "    if not isinstance(text, str) or text.strip() == \"\" or \"missing\" in text.lower():\n",
    "        return None\n",
    "    result = {}\n",
    "    segments = [s.strip() for s in re.split(r';|\\||·', text) if s.strip()]\n",
    "    if not segments: segments = [text.strip()]\n",
    "    for seg in segments:\n",
    "        days, times = _parse_segment(seg)\n",
    "        if not times: continue\n",
    "        for d in days:\n",
    "            result.setdefault(d, []).extend(times)\n",
    "    return result\n",
    "\n",
    "# --- APPLY transformation directly to df ---\n",
    "df[\"opening_hours_dict\"] = df[\"opening_hours\"].apply(opening_hours_to_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "037d2f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleans empty spaces on the phone column\n",
    "df[\"phone\"] = (\n",
    "    df[\"phone\"]\n",
    "    .str.strip()                         # remove leading/trailing\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)  # collapse multiple spaces\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d686cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df.to_csv(r'/Users/giovanigoltara/Documents/webeet/layered-populate-data-pool-da/venues/sources/berlin_venues_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
