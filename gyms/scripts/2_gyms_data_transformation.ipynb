{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "874a86c2",
   "metadata": {},
   "source": [
    "# Gym Data Transformation\n",
    "\n",
    "This notebook provides a step-by-step workflow to clean and map OpenStreetMap (OSM) gym data for further analysis or database import. It includes:\n",
    "- Loading the most recent OSM gym export (CSV)\n",
    "- Cleaning and standardizing the data\n",
    "- Mapping gyms to Berlin districts via spatial join (GeoPandas)\n",
    "- Exporting the cleaned and mapped dataset\n",
    "\n",
    "> **All steps are self-contained and annotated in English.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ecc854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Import Required Libraries ===\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# === 2. Load data ===\n",
    "# - load the most recent OSM gym export\n",
    "osm_path = os.path.join('..', 'sources', 'gyms_osm_berlin_*.csv')\n",
    "\n",
    "# - load cleaned gym data\n",
    "gyms_path = os.path.join('..', 'sources', 'gyms_cleaned_for_db.csv')\n",
    "\n",
    "# - load Berlin districts GeoJSON\n",
    "districts_path = os.path.join('..', 'sources', 'berlin_districts.geojson')\n",
    "\n",
    "# - load Berlin neighborhoods GeoJSON\n",
    "neighborhoods_path = os.path.join('..', 'sources', 'berlin_neighborhood.geojson')\n",
    "\n",
    "# - final CSV path\n",
    "final_csv_path = os.path.join('..', 'sources', 'gyms_ready_for_db.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd97492",
   "metadata": {},
   "source": [
    "# 2. Load the Latest Exported OSM Data\n",
    "\n",
    "This code block performs the following steps:\n",
    "\n",
    "- **Searches** for all OSM gym export files in the `gyms/sources` directory.  \n",
    "- **Identifies** the most recent file by extracting the date from each filename.\n",
    "- **Loads** the latest file as a pandas DataFrame for further processing.\n",
    "- **Raises an error** if no matching file is found.\n",
    "\n",
    "This ensures your analysis always uses the most up-to-date OSM data export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8379ff86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OSM export: ../sources/gyms_osm_berlin_2025-09-26.csv\n"
     ]
    }
   ],
   "source": [
    "osm_files = glob.glob(osm_path)\n",
    "if not osm_files:\n",
    "    raise FileNotFoundError(\"No OSM gym export file found in ../sources/\")\n",
    "\n",
    "def extract_date(fname):\n",
    "    basename = os.path.basename(fname)\n",
    "    date_str = basename.replace('gyms_osm_berlin_', '').replace('.csv', '')\n",
    "    return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "\n",
    "osm_files_sorted = sorted(osm_files, key=extract_date)\n",
    "raw_file = osm_files_sorted[-1]  # most recent file\n",
    "print(f\"Loading OSM export: {raw_file}\")\n",
    "df = pd.read_csv(raw_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9d76ae",
   "metadata": {},
   "source": [
    "## 3. Clean and Standardize Gym Data\n",
    "\n",
    "This code block prepares the OSM data for further analysis by:\n",
    "\n",
    "- **Renaming columns** to a unified naming scheme.  \n",
    "  For example:  \n",
    "    - `leisure` → `type` (main type, e.g., fitness_centre)  \n",
    "    - `sport` → `type_alt` (alternative type, e.g., yoga)\n",
    "\n",
    "- **Merging gym types:**  \n",
    "  If the main type (`type`) is missing, it uses the value from `type_alt`.\n",
    "\n",
    "- **Cleaning and filling missing values:**  \n",
    "  - Fills missing names with \"Unknown Gym\"\n",
    "  - Fills missing addresses, postcodes, and other details with sensible defaults\n",
    "  - Sets city to \"Berlin\" if missing\n",
    "  - Normalizes website and phone fields\n",
    "  - Ensures all latitude and longitude values are numeric\n",
    "  - Fills unknown wheelchair access info with \"unknown\"\n",
    "  - Ensures all `osm_id` values are strings\n",
    "\n",
    "- **Prepares for future steps:**  \n",
    "  Adds an empty `district_id` column as a placeholder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15bac735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Column Renaming (adjust if your source has different names) ---\n",
    "df = df.rename(columns={\n",
    "    'name': 'name',\n",
    "    'leisure': 'type',           # Main type (e.g. fitness_centre)\n",
    "    'sport': 'type_alt',         # Backup type (e.g. yoga)\n",
    "    'street': 'street',\n",
    "    'housenumber': 'housenumber',\n",
    "    'postcode': 'postcode',\n",
    "    'city': 'city',\n",
    "    'opening_hours': 'opening_hours',\n",
    "    'phone': 'phone',\n",
    "    'website': 'website',\n",
    "    'wheelchair': 'wheelchair',\n",
    "    'latitude': 'latitude',\n",
    "    'longitude': 'longitude',\n",
    "    'osm_id': 'osm_id',\n",
    "    'osm_type': 'osm_type',\n",
    "    'source': 'source'\n",
    "})\n",
    "\n",
    "# --- Type (merge type and type_alt if main is missing) ---\n",
    "df['type'] = df['type'].fillna('')\n",
    "df['type'] = np.where(df['type'] != '', df['type'], df['type_alt'])\n",
    "df.drop(columns=['type_alt'], inplace=True)\n",
    "\n",
    "# --- Fill other fields ---\n",
    "df['name'] = df['name'].fillna('Unknown Gym')\n",
    "df['street'] = df['street'].fillna('')\n",
    "df['housenumber'] = df['housenumber'].fillna('')\n",
    "df['postcode'] = df['postcode'].fillna('')\n",
    "df['city'] = df['city'].fillna('Berlin')  # Default: Berlin\n",
    "\n",
    "df['website'] = df['website'].fillna('').str.lower().str.strip()\n",
    "df['phone'] = df['phone'].fillna('').str.strip()\n",
    "\n",
    "df['opening_hours'] = df['opening_hours'].fillna('')\n",
    "df['wheelchair'] = df['wheelchair'].fillna('unknown')\n",
    "\n",
    "df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')\n",
    "df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')\n",
    "\n",
    "df['osm_id'] = df['osm_id'].fillna('').astype(str)\n",
    "df['osm_type'] = df['osm_type'].fillna('')\n",
    "df['source'] = df['source'].fillna('OSM Overpass')\n",
    "\n",
    "# --- Placeholder for future district_id assignment --- \n",
    "df['district_id'] = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aa0600",
   "metadata": {},
   "source": [
    "## 4. Save Cleaned Data for Further Processing\n",
    "\n",
    "This code block does the following:\n",
    "\n",
    "- **Defines the output file path** for the cleaned data (`gyms_cleaned_for_db.csv`) in the `../sources` directory.\n",
    "- **Exports the cleaned DataFrame** to a CSV file at the specified location, without row indices.\n",
    "- **Prints a confirmation message** with the file path and the number of rows saved.\n",
    "\n",
    "This step stores your cleaned and standardized gym data for further processing or database import.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44383c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to ../sources/gyms_cleaned_for_db.csv (444 rows)\n"
     ]
    }
   ],
   "source": [
    "gyms_df = os.path.join('..', 'sources', 'gyms_cleaned_for_db.csv')\n",
    "df.to_csv(gyms_df, index=False)\n",
    "print(f\"Cleaned data saved to {gyms_df} ({len(df)} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ab4547",
   "metadata": {},
   "source": [
    "# 5. Spatial Join: Assign Districts to Gyms\n",
    "\n",
    "This code performs the following steps:\n",
    "\n",
    "- **Imports required libraries** for working with tabular and spatial data.\n",
    "- **Loads the cleaned gym data** from CSV into a pandas DataFrame.\n",
    "- **Loads Berlin district boundaries** from a GeoJSON file into a GeoDataFrame.\n",
    "- **Creates spatial Point geometries** for each gym using their longitude and latitude coordinates.\n",
    "- **Performs a spatial join** between the gyms (as points) and the districts (as polygons), \n",
    "  assigning each gym to the district in which it is located.\n",
    "- **Adds district information** to the gyms DataFrame:\n",
    "  - `district_id`: the unique identifier for the district (`Schluessel_gesamt`)\n",
    "  - `district`: the district name (`Gemeinde_name`)\n",
    "\n",
    "This step enriches the gym data with spatial context, allowing further analysis by Berlin district.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ff16670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighborhoods columns: Index(['gml_id', 'spatial_name', 'spatial_alias', 'spatial_type', 'OTEIL',\n",
      "       'BEZIRK', 'FLAECHE_HA', 'geometry'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# ---- 1. Load cleaned gym data ----\n",
    "gyms_df = pd.read_csv(gyms_path)\n",
    "\n",
    "# ---- 2. Load Berlin districts (GeoJSON) ----\n",
    "districts_gdf = gpd.read_file(districts_path)\n",
    "neighborhoods_gdf = gpd.read_file(neighborhoods_path)\n",
    "\n",
    "# ---- 3. Build Point geometries for gyms ----\n",
    "gyms_gdf = gpd.GeoDataFrame(\n",
    "    gyms_df,\n",
    "    geometry=[Point(xy) for xy in zip(gyms_df.longitude, gyms_df.latitude)],\n",
    "    crs='EPSG:4326'\n",
    ")\n",
    "\n",
    "# ---- 4. Spatial join gyms with districts ----\n",
    "gyms_with_district = gpd.sjoin(\n",
    "    gyms_gdf,\n",
    "    districts_gdf,\n",
    "    how=\"left\",\n",
    "    predicate='within'\n",
    ")\n",
    "\n",
    "# ---- 5. Assign district_id and district name from joined data ----\n",
    "gyms_with_district['district_id'] = gyms_with_district['Schluessel_gesamt']\n",
    "gyms_with_district['district'] = gyms_with_district['Gemeinde_name']\n",
    "\n",
    "# ---- 6. Load Berlin neighborhoods (GeoJSON) ----\n",
    "neighborhoods_gdf = gpd.read_file(neighborhoods_path)\n",
    "\n",
    "# Print columns for debugging\n",
    "print(\"Neighborhoods columns:\", neighborhoods_gdf.columns)\n",
    "\n",
    "# ---- 7. Spatial join gyms_with_district with neighborhoods ----\n",
    "gyms_with_all = gpd.sjoin(\n",
    "    gyms_with_district,\n",
    "    neighborhoods_gdf,\n",
    "    how=\"left\",\n",
    "    predicate='within',\n",
    "    lsuffix='_gym',\n",
    "    rsuffix='_neigh'\n",
    ")\n",
    "\n",
    "# ---- 8. Assign neighborhood_id and neighborhood from joined data ----\n",
    "for neigh_id_col in ['neighborhood_id_neigh', 'neighborhood_id']:\n",
    "    if neigh_id_col in gyms_with_all.columns:\n",
    "        gyms_with_all['neighborhood_id'] = gyms_with_all[neigh_id_col]\n",
    "        break\n",
    "for neigh_col in ['neighborhood_neigh', 'neighborhood']:\n",
    "    if neigh_col in gyms_with_all.columns:\n",
    "        gyms_with_all['neighborhood'] = gyms_with_all[neigh_col]\n",
    "        break\n",
    "    \n",
    "# ---- 9. Clean up: Remove duplicates and unwanted columns ----\n",
    "unwanted_columns = [col for col in gyms_with_all.columns \n",
    "                    if col.endswith('.1') or col.endswith('_gym') or col.endswith('_neigh') \n",
    "                    or col in ['Schluessel_gesamt', 'Gemeinde_name', 'index_right', 'index_left']]\n",
    "gyms_with_all = gyms_with_all.drop(columns=unwanted_columns, errors='ignore')\n",
    "\n",
    "# district_id as str and without \".0\"\n",
    "gyms_with_all['district_id'] = gyms_with_all['district_id'].apply(lambda x: str(int(float(x))) if pd.notnull(x) and x != '' else None)\n",
    "\n",
    "final_columns = [\n",
    "    'gym_id', 'district_id', 'name', 'address', 'postal_code', 'phone_number', 'email',\n",
    "    'coordinates', 'latitude', 'longitude', 'neighborhood_id', 'neighborhood', 'district'\n",
    "]\n",
    "cols_existing = [col for col in final_columns if col in gyms_with_all.columns]\n",
    "\n",
    "gyms_with_district = gyms_with_all[cols_existing]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f614e38",
   "metadata": {},
   "source": [
    "# 6. Prepare Columns for Database Import\n",
    "\n",
    "This code block prepares the DataFrame for database import by:\n",
    "\n",
    "- **Renaming and creating columns** to match the target SQL table structure.\n",
    "  - Combines street name and house number into a single address field.\n",
    "  - Extracts latitude and longitude.\n",
    "  - Sets empty values for columns not available in the source (e.g., email, neighborhood).\n",
    "\n",
    "- **Defining the exact column order** to match the SQL schema:\n",
    "  - Ensures the exported data will align perfectly with the database table, minimizing import issues.\n",
    "\n",
    "- **Reordering the DataFrame columns** according to the SQL table.\n",
    "  - This step also removes any unwanted or duplicate columns.\n",
    "\n",
    "This ensures your data is cleanly formatted, named, and ordered for a smooth transition into the SQL database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1036d0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'osm_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/webeet_internship/Repositories/layered-populate-data-pool-da/venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'osm_id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Reorder and rename columns to match the SQL table structure ---\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Create new columns or adjust as needed to match SQL schema\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m gyms_with_district[\u001b[33m'\u001b[39m\u001b[33mgym_id\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mgyms_with_district\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mosm_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      5\u001b[39m gyms_with_district[\u001b[33m'\u001b[39m\u001b[33maddress\u001b[39m\u001b[33m'\u001b[39m] = gyms_with_district[\u001b[33m'\u001b[39m\u001b[33mstreet\u001b[39m\u001b[33m'\u001b[39m].fillna(\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m) + \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m + gyms_with_district[\u001b[33m'\u001b[39m\u001b[33mhousenumber\u001b[39m\u001b[33m'\u001b[39m].fillna(\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m gyms_with_district[\u001b[33m'\u001b[39m\u001b[33mpostal_code\u001b[39m\u001b[33m'\u001b[39m] = gyms_with_district[\u001b[33m'\u001b[39m\u001b[33mpostcode\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/webeet_internship/Repositories/layered-populate-data-pool-da/venv/lib/python3.12/site-packages/pandas/core/frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/webeet_internship/Repositories/layered-populate-data-pool-da/venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'osm_id'"
     ]
    }
   ],
   "source": [
    "# --- Reorder and rename columns to match the SQL table structure ---\n",
    "\n",
    "# Create new columns or adjust as needed to match SQL schema\n",
    "gyms_with_district['gym_id'] = gyms_with_district['osm_id']\n",
    "gyms_with_district['address'] = gyms_with_district['street'].fillna('') + ' ' + gyms_with_district['housenumber'].fillna('')\n",
    "gyms_with_district['postal_code'] = gyms_with_district['postcode']\n",
    "gyms_with_district['phone_number'] = gyms_with_district['phone']\n",
    "gyms_with_district['email'] = ''  # No email in source; set empty or fill if available\n",
    "gyms_with_district['coordinates'] = gyms_with_district['geometry'].apply(lambda geom: str(geom) if geom else '')\n",
    "gyms_with_district['neighborhood'] = ''  # No neighborhood info; set empty or fill if available\n",
    "\n",
    "# Define final column order (matching SQL table)\n",
    "final_columns = [\n",
    "    'gym_id',        # VARCHAR(20) PRIMARY KEY\n",
    "    'district_id',   # VARCHAR(2)\n",
    "    'name',          # VARCHAR(200)\n",
    "    'address',       # VARCHAR(200)\n",
    "    'postal_code',   # VARCHAR(10)\n",
    "    'phone_number',  # VARCHAR(50)\n",
    "    'email',         # VARCHAR(100)\n",
    "    'coordinates',   # VARCHAR(200)\n",
    "    'latitude',      # DECIMAL(9,6)\n",
    "    'longitude',     # DECIMAL(9,6)\n",
    "    'neighborhood',  # VARCHAR(100)\n",
    "    'district'       # VARCHAR(100)\n",
    "]\n",
    "\n",
    "# Reorder DataFrame columns to match the SQL schema\n",
    "gyms_final = gyms_with_district[final_columns].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7150a359",
   "metadata": {},
   "source": [
    "# 7. Export Final DataFrame with Districts\n",
    "\n",
    "This code block saves the final, cleaned, and enriched DataFrame to a CSV file:\n",
    "\n",
    "- **Defines the output path** using `os.path.join` for consistency and portability.\n",
    "- **Exports the DataFrame** to a CSV file (`gyms_with_district.csv`) in the `../sources` directory, without row indices.\n",
    "- **Prints a confirmation message** with the file path, confirming successful export.\n",
    "\n",
    "This ensures you have a ready-to-import CSV file containing all gym and district information, structured for database import or further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f22031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns reordered and renamed for SQL import. CSV file saved as '../sources/gyms_with_district.csv'.\n"
     ]
    }
   ],
   "source": [
    "# --- Save the final DataFrame to CSV ---\n",
    "CSV_PATH = os.path.join('..', 'sources', 'gyms_with_district.csv')\n",
    "gyms_final.to_csv(CSV_PATH, index=False)\n",
    "print(f\"Columns reordered and renamed for SQL import. CSV file saved as '{CSV_PATH}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac40641",
   "metadata": {},
   "source": [
    "# **Done!**\n",
    "\n",
    "- The OSM gym data is now cleaned and mapped to Berlin districts.\n",
    "- Next steps: The CSV can now be used for database import or further analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
