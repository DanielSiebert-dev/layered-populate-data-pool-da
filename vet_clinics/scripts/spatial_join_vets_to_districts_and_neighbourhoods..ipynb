{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4893ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/martinsvitek/layered-populate-data-pool-da/layered-populate-data-pool-da/vet_clinics/sources/vets_with_districts_neighborhoods.csv (178 rows)\n",
      "Added columns present: ['district_id', 'district', 'neighbourhood_id', 'neighbourhood']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "VETS_CSV = \"/Users/martinsvitek/layered-populate-data-pool-da/layered-populate-data-pool-da/vet_clinics/sources/vet_clinics/sources/vets_osm_berlin_cleaned_for_db.csv\"\n",
    "LOR_GEOJSON = \"/Users/martinsvitek/layered-populate-data-pool-da/layered-populate-data-pool-da/vet_clinics/sources/vet_clinics/sources/lor_ortsteile.geojson\"\n",
    "OUT_CSV = \"/Users/martinsvitek/layered-populate-data-pool-da/layered-populate-data-pool-da/vet_clinics/sources/vets_with_districts_neighborhoods.csv\"\n",
    "\n",
    "# 1) Load vets and build points\n",
    "v = pd.read_csv(VETS_CSV)\n",
    "v[\"latitude\"]  = pd.to_numeric(v[\"latitude\"], errors=\"coerce\")\n",
    "v[\"longitude\"] = pd.to_numeric(v[\"longitude\"], errors=\"coerce\")\n",
    "v = v.dropna(subset=[\"latitude\",\"longitude\"]).copy()\n",
    "\n",
    "v_gdf = gpd.GeoDataFrame(\n",
    "    v,\n",
    "    geometry=gpd.points_from_xy(v[\"longitude\"], v[\"latitude\"]),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# 2) Load LOR polygons and pick the actual columns you have\n",
    "lor = gpd.read_file(LOR_GEOJSON)\n",
    "\n",
    "# Ensure CRS matches\n",
    "if lor.crs is None:\n",
    "    lor = lor.set_crs(4326, allow_override=True)\n",
    "elif lor.crs.to_epsg() != 4326:\n",
    "    lor = lor.to_crs(4326)\n",
    "\n",
    "# Find name columns (your file shows BEZIRK and OTEIL)\n",
    "def first_present(cands, df):\n",
    "    for c in cands:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "district_name_col = first_present([\"BEZIRK\", \"Bezirk\", \"district\", \"DISTRICT\"], lor)\n",
    "neigh_name_col    = first_present([\"OTEIL\", \"Ortsteil\", \"ORTSTEIL\", \"neighbourhood\", \"NEIGHBOURHOOD\"], lor)\n",
    "\n",
    "if not district_name_col or not neigh_name_col:\n",
    "    raise KeyError(f\"Couldnâ€™t find district/neighbourhood name columns in GeoJSON. \"\n",
    "                   f\"Columns present: {sorted(lor.columns)}\")\n",
    "\n",
    "lor_min = lor[[district_name_col, neigh_name_col, \"geometry\"]].copy()\n",
    "lor_min.rename(columns={district_name_col: \"district\", neigh_name_col: \"neighbourhood\"}, inplace=True)\n",
    "\n",
    "# If official numeric IDs exist, pick them up; else create IDs from names\n",
    "district_id_col = first_present(\n",
    "    [\"Bezirk_ID\",\"BEZIRKS_ID\",\"BEZIRKSNR\",\"BEZIRK_ID\",\"Schluessel_gesamt\",\"BEZIRK_NR\",\"district_id\"],\n",
    "    lor\n",
    ")\n",
    "neigh_id_col = first_present(\n",
    "    [\"Ortsteil_ID\",\"ORTSTEIL_ID\",\"ORTSTEIL_NR\",\"Ortsteil_Nr\",\"neighbourhood_id\"],\n",
    "    lor\n",
    ")\n",
    "\n",
    "if district_id_col and district_id_col in lor.columns:\n",
    "    lor_min[\"district_id\"] = lor[district_id_col].values\n",
    "else:\n",
    "    lor_min[\"district_id\"] = lor_min[\"district\"].astype(\"category\").cat.codes + 1\n",
    "\n",
    "if neigh_id_col and neigh_id_col in lor.columns:\n",
    "    lor_min[\"neighbourhood_id\"] = lor[neigh_id_col].values\n",
    "else:\n",
    "    lor_min[\"neighbourhood_id\"] = lor_min[\"neighbourhood\"].astype(\"category\").cat.codes + 1\n",
    "\n",
    "# 3) Spatial join (point-in-polygon). If many points lie on borders, try predicate=\"intersects\".\n",
    "joined = gpd.sjoin(v_gdf, lor_min, how=\"left\", predicate=\"within\")\n",
    "if \"index_right\" in joined.columns:\n",
    "    joined.drop(columns=[\"index_right\"], inplace=True)\n",
    "\n",
    "# 4) Guarantee the columns exist (derive IDs from names if still missing)\n",
    "if \"district\" in joined.columns and \"district_id\" not in joined.columns:\n",
    "    joined[\"district_id\"] = joined[\"district\"].astype(\"category\").cat.codes + 1\n",
    "if \"neighbourhood\" in joined.columns and \"neighbourhood_id\" not in joined.columns:\n",
    "    joined[\"neighbourhood_id\"] = joined[\"neighbourhood\"].astype(\"category\").cat.codes + 1\n",
    "\n",
    "# 5) Save CSV (only columns that actually exist)\n",
    "base_cols = list(v.columns)  # original vets columns\n",
    "extra_cols = [\"district_id\", \"district\", \"neighbourhood_id\", \"neighbourhood\"]\n",
    "present_extras = [c for c in extra_cols if c in joined.columns]\n",
    "out_cols = base_cols + present_extras\n",
    "\n",
    "os.makedirs(os.path.dirname(OUT_CSV), exist_ok=True)\n",
    "joined[out_cols].to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Saved: {OUT_CSV} ({len(joined)} rows)\")\n",
    "print(\"Added columns present:\", present_extras)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
