{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99d0d3d9",
   "metadata": {},
   "source": [
    "cleaning osm data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95e5fbc",
   "metadata": {},
   "source": [
    "step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7be1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "def _ensure_osmid_column(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Make sure we have an 'osmid' column (OSMnx can put it in the index).\"\"\"\n",
    "    if 'osmid' in gdf.columns:\n",
    "        return gdf\n",
    "    if isinstance(gdf.index, pd.MultiIndex):\n",
    "        if 'osmid' in gdf.index.names:\n",
    "            return gdf.reset_index()\n",
    "        gdf = gdf.reset_index()\n",
    "    else:\n",
    "        gdf = gdf.reset_index()\n",
    "    # Fallbacks\n",
    "    if 'osmid' not in gdf.columns:\n",
    "        if 'id' in gdf.columns:\n",
    "            gdf = gdf.rename(columns={'id': 'osmid'})\n",
    "        else:\n",
    "            gdf['osmid'] = gdf.index.astype(str)\n",
    "    return gdf\n",
    "\n",
    "def _coalesce_cols(df: pd.DataFrame, cols):\n",
    "    \"\"\"Row-wise first-non-null across a list of optional columns.\"\"\"\n",
    "    out = pd.Series(pd.NA, index=df.index, dtype=\"object\")\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            out = out.fillna(df[c])\n",
    "    return out\n",
    "\n",
    "def normalize_clinic_geometries(gdf: gpd.GeoDataFrame, keep_footprint=True) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Set CRS to EPSG:4326, convert non-points to centroids; optionally keep original footprint.\"\"\"\n",
    "    # CRS → 4326\n",
    "    if gdf.crs is None:\n",
    "        gdf = gdf.set_crs(4326, allow_override=True)\n",
    "    else:\n",
    "        gdf = gdf.to_crs(4326)\n",
    "\n",
    "    if keep_footprint:\n",
    "        gdf['geometry_footprint'] = gdf.geometry\n",
    "\n",
    "    # Point or centroid\n",
    "    geomtype = gdf.geometry.geom_type\n",
    "    centroids = gdf.geometry.centroid\n",
    "    new_geom = np.where(geomtype.eq('Point'), gdf.geometry, centroids)\n",
    "    gdf = gdf.set_geometry(new_geom)\n",
    "    gdf = gdf.set_crs(4326, allow_override=True)\n",
    "\n",
    "    # Lat/Lon from point\n",
    "    gdf['latitude']  = gdf.geometry.y\n",
    "    gdf['longitude'] = gdf.geometry.x\n",
    "    return gdf\n",
    "\n",
    "def prune_and_rename_clinics(gdf: gpd.GeoDataFrame, keep_footprint=True) -> gpd.GeoDataFrame:\n",
    "    gdf = _ensure_osmid_column(gdf).copy()\n",
    "\n",
    "    # Filter to amenity=veterinary (if available)\n",
    "    if 'amenity' in gdf.columns:\n",
    "        gdf = gdf[gdf['amenity'].astype(str).str.lower().eq('veterinary')].copy()\n",
    "\n",
    "    # Geometry normalization (point + lat/lon, keep footprint optionally)\n",
    "    gdf = normalize_clinic_geometries(gdf, keep_footprint=keep_footprint)\n",
    "\n",
    "    # Coalesce contact fields\n",
    "    phone   = _coalesce_cols(gdf, ['contact:phone', 'phone'])\n",
    "    website = _coalesce_cols(gdf, ['contact:website', 'website'])\n",
    "    email   = _coalesce_cols(gdf, ['contact:email', 'email'])\n",
    "\n",
    "    # Build full address\n",
    "    def _full_address(row):\n",
    "        a = \" \".join([str(row.get('addr:street') or \"\").strip(),\n",
    "                      str(row.get('addr:housenumber') or \"\").strip()]).strip()\n",
    "        b = \" \".join([str(row.get('addr:postcode') or \"\").strip(),\n",
    "                      str(row.get('addr:city') or \"\").strip()]).strip()\n",
    "        parts = [p for p in [a, b] if p]\n",
    "        return \", \".join(parts) if parts else pd.NA\n",
    "\n",
    "    gdf['full_address'] = gdf.apply(_full_address, axis=1)\n",
    "\n",
    "    # Target schema mapping (rename/pass-through)\n",
    "    rename_map = {\n",
    "        'osmid': 'clinic_id',\n",
    "        'name': 'clinic_name',\n",
    "        'addr:street': 'street',\n",
    "        'addr:housenumber': 'house_number',\n",
    "        'addr:postcode': 'postcode',\n",
    "        'addr:city': 'city',\n",
    "        'veterinary:speciality': 'speciality',\n",
    "        'wheelchair': 'wheelchair_acces',  # as requested (one 's')\n",
    "        # keep: amenity, opening_hours, operator, brand, geometry\n",
    "    }\n",
    "    for old, new in rename_map.items():\n",
    "        if old in gdf.columns:\n",
    "            gdf = gdf.rename(columns={old: new})\n",
    "\n",
    "    # Inject coalesced contact fields to target names\n",
    "    gdf['phone_number'] = phone\n",
    "    gdf['website']      = website\n",
    "    gdf['email']        = email\n",
    "\n",
    "    # Add district/neighbourhood placeholders (to be enriched later)\n",
    "    gdf['district_id']      = pd.NA\n",
    "    gdf['neighbourhood_id'] = pd.NA\n",
    "\n",
    "    # Final column order\n",
    "    final_cols = [\n",
    "        'clinic_id', 'clinic_name',\n",
    "        'street', 'house_number', 'postcode', 'city',\n",
    "        'district_id', 'neighbourhood_id',\n",
    "        'phone_number', 'website', 'email',\n",
    "        'opening_hours', 'operator', 'brand',\n",
    "        'speciality', 'wheelchair_acces',\n",
    "        'full_address', 'latitude', 'longitude',\n",
    "        'amenity', 'geometry'\n",
    "    ]\n",
    "\n",
    "    # Keep only requested columns that actually exist\n",
    "    existing = [c for c in final_cols if c in gdf.columns]\n",
    "    out = gdf[existing].copy()\n",
    "\n",
    "    # Ensure proper dtypes for id/postcode if desired (keep as text here)\n",
    "    if 'clinic_id' in out.columns:\n",
    "        out['clinic_id'] = out['clinic_id'].astype(str)\n",
    "    if 'postcode' in out.columns:\n",
    "        out['postcode'] = out['postcode'].astype(str)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36a5699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) geting raw OSM features (point/polygon geometries)\n",
    "from osmnx.features import features_from_place as get_features  # OSMnx ≥2\n",
    "gdf_raw = get_features(\"Berlin, Germany\", {\"amenity\": \"veterinary\"}).reset_index()\n",
    "\n",
    "# 2) running the pruning/renaming + geometry normalization\n",
    "clinics_gdf = prune_and_rename_clinics(gdf_raw, keep_footprint=True)\n",
    "\n",
    "# 3) quick sanity checks\n",
    "clinics_gdf.head(), clinics_gdf.crs, clinics_gdf.shape\n",
    "\n",
    "clinics_gdf.to_file(\"berlin_vet_clinics_normalized.geojson\", driver=\"GeoJSON\")\n",
    "clinics_gdf.drop(columns=\"geometry\").to_csv(\"berlin_vet_clinics_normalized.csv\", index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b5b2fc",
   "metadata": {},
   "source": [
    "step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35641a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Apply cleaning helpers to the target-schema GeoDataFrame ---\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "def _geo_latlon(g):\n",
    "    \"\"\"Return (lat, lon) from geometry; centroid fallback; None if missing.\"\"\"\n",
    "    if g is None or g.is_empty:\n",
    "        return (None, None)\n",
    "    p = g\n",
    "    if not isinstance(p, Point):\n",
    "        try:\n",
    "            p = g.centroid\n",
    "        except Exception:\n",
    "            return (None, None)\n",
    "    return (float(p.y), float(p.x))\n",
    "\n",
    "def clean_clinics_target_schema(clinics_gdf, country=\"DE\", postcode_as_text=True):\n",
    "    \"\"\"\n",
    "    Use your clean_vet_df() (which expects OSM-style column names),\n",
    "    then map cleaned values back to the target schema on clinics_gdf.\n",
    "    \"\"\"\n",
    "    gdf = clinics_gdf.copy()\n",
    "\n",
    "    # 1) Build a temporary OSM-style frame the cleaner expects\n",
    "    tmp = pd.DataFrame({\n",
    "        \"name\": gdf.get(\"clinic_name\"),\n",
    "        \"addr:street\": gdf.get(\"street\"),\n",
    "        \"addr:housenumber\": gdf.get(\"house_number\"),\n",
    "        \"addr:city\": gdf.get(\"city\"),\n",
    "        \"addr:postcode\": gdf.get(\"postcode\"),\n",
    "        \"phone\": gdf.get(\"phone_number\"),\n",
    "        \"website\": gdf.get(\"website\"),\n",
    "        \"email\": gdf.get(\"email\"),\n",
    "        \"opening_hours\": gdf.get(\"opening_hours\"),\n",
    "        \"operator\": gdf.get(\"operator\"),\n",
    "        \"brand\": gdf.get(\"brand\"),\n",
    "        \"wheelchair\": gdf.get(\"wheelchair_acces\", gdf.get(\"wheelchair_access\")),\n",
    "        \"emergency\": gdf.get(\"emergency\"),\n",
    "    })\n",
    "\n",
    "    # 2) Clean using helper\n",
    "    tmp_clean = clean_vet_df(tmp, country=country, postcode_as_text=postcode_as_text)\n",
    "\n",
    "    # 3) Write cleaned values back to target-schema columns\n",
    "    gdf[\"clinic_name\"]   = tmp_clean[\"name\"]\n",
    "    gdf[\"street\"]        = tmp_clean[\"addr:street\"]\n",
    "    gdf[\"house_number\"]  = tmp_clean[\"addr:housenumber\"]\n",
    "    gdf[\"city\"]          = tmp_clean[\"addr:city\"]\n",
    "    gdf[\"postcode\"]      = tmp_clean[\"addr:postcode\"]\n",
    "    gdf[\"phone_number\"]  = tmp_clean.get(\"phone\")\n",
    "    gdf[\"website\"]       = tmp_clean.get(\"website\")\n",
    "    gdf[\"email\"]         = tmp_clean.get(\"email\")\n",
    "    gdf[\"opening_hours\"] = tmp_clean.get(\"opening_hours\")\n",
    "    gdf[\"operator\"]      = tmp_clean.get(\"operator\")\n",
    "    gdf[\"wheelchair_acces\"] = tmp_clean.get(\"wheelchair\")  # keep your column name\n",
    "    gdf[\"emergency\"]        = tmp_clean.get(\"emergency\")\n",
    "\n",
    "    # Optional: bring over the validity flag from your helper\n",
    "    if \"opening_hours_valid\" in tmp_clean.columns:\n",
    "        gdf[\"opening_hours_valid\"] = tmp_clean[\"opening_hours_valid\"]\n",
    "\n",
    "    # 4) Rebuild full_address (street + house + postcode + city) when available\n",
    "    def _addr_row(r):\n",
    "        parts = [r.get(\"street\"), r.get(\"house_number\"), r.get(\"postcode\"), r.get(\"city\")]\n",
    "        parts = [p for p in parts if isinstance(p, str) and p.strip() != \"\"]\n",
    "        return \", \".join(parts) if parts else None\n",
    "    gdf[\"full_address\"] = gdf.apply(_addr_row, axis=1)\n",
    "\n",
    "    # 5) Ensure geometry CRS and lat/lon columns exist (point or centroid)\n",
    "    if getattr(gdf, \"crs\", None) is None:\n",
    "        try:\n",
    "            gdf.set_crs(epsg=4326, inplace=True, allow_override=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "    elif gdf.crs.to_epsg() != 4326:\n",
    "        gdf = gdf.to_crs(4326)\n",
    "\n",
    "    if \"latitude\" not in gdf.columns or \"longitude\" not in gdf.columns:\n",
    "        lats, lons = zip(*gdf.geometry.map(_geo_latlon))\n",
    "        gdf[\"latitude\"] = lats\n",
    "        gdf[\"longitude\"] = lons\n",
    "\n",
    "    # 6) (Safety) keep only veterinary amenity rows if column present\n",
    "    if \"amenity\" in gdf.columns:\n",
    "        gdf = gdf[gdf[\"amenity\"].fillna(\"\").astype(str).str.lower().eq(\"veterinary\")]\n",
    "\n",
    "    return gdf\n",
    "\n",
    "# --- run it ---\n",
    "clinics_clean = clean_clinics_target_schema(clinics_gdf, country=\"DE\", postcode_as_text=True)\n",
    "\n",
    "# quick peek / save\n",
    "display(clinics_clean.head())\n",
    "clinics_clean.drop(columns=\"geometry\").to_csv(\"berlin_vet_clinics_clean.csv\", index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd282fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dedupe on raw OSM features\n",
    "gdf_raw = _ensure_osmid_column(gdf_raw)\n",
    "before = len(gdf_raw)\n",
    "gdf_raw = gdf_raw.drop_duplicates(subset=[\"osmid\"], keep=\"first\").copy()\n",
    "print(f\"Dropped {before - len(gdf_raw)} exact duplicates on osmid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f548079",
   "metadata": {},
   "outputs": [],
   "source": [
    "before = len(clinics_gdf)\n",
    "clinics_gdf = clinics_gdf.drop_duplicates(subset=[\"clinic_id\"], keep=\"first\").copy()\n",
    "print(f\"Dropped {before - len(clinics_gdf)} exact duplicates on clinic_id\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca9973d",
   "metadata": {},
   "source": [
    "alligning bpt and taek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9392d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# ---------- light utilities ----------\n",
    "def pick_first_col(df: pd.DataFrame, candidates):\n",
    "    \"\"\"Return the first existing column (Series) from candidates or an NA Series.\"\"\"\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return df[c]\n",
    "    return pd.Series(pd.NA, index=df.index, dtype=\"object\")\n",
    "\n",
    "SYNONYMS = {\n",
    "    \"clinic_id\":        [\"clinic_id\",\"osmid\",\"osm_id\",\"id\",\"uid\",\"facility_id\"],\n",
    "    \"clinic_name\":      [\"clinic_name\",\"name\",\"practice\",\"praxis\",\"facility_name\",\"betrieb\"],\n",
    "    \"street\":           [\"street\",\"addr:street\",\"addr_street\",\"address_street\",\"strasse\",\"straße\"],\n",
    "    \"house_number\":     [\"house_number\",\"addr:housenumber\",\"housenumber\",\"hnr\",\"nr\"],\n",
    "    \"postcode\":         [\"postcode\",\"addr:postcode\",\"zip\",\"plz\",\"postleitzahl\"],\n",
    "    \"city\":             [\"city\",\"addr:city\",\"ort\",\"stadt\",\"gemeinde\"],\n",
    "    \"phone_number\":     [\"phone_number\",\"contact:phone\",\"phone\",\"telefon\",\"tel\"],\n",
    "    \"website\":          [\"website\",\"contact:website\",\"url\",\"homepage\",\"web\"],\n",
    "    \"email\":            [\"email\",\"contact:email\",\"mail\",\"e-mail\"],\n",
    "    \"opening_hours\":    [\"opening_hours\",\"hours\",\"sprechzeiten\",\"openinghours\"],\n",
    "    \"operator\":         [\"operator\",\"betreiber\",\"owner\"],\n",
    "    \"brand\":            [\"brand\",\"kette\"],\n",
    "    \"speciality\":       [\"speciality\",\"specialty\",\"veterinary:speciality\",\"fachrichtung\"],\n",
    "    \"wheelchair_acces\": [\"wheelchair_acces\",\"wheelchair_access\",\"wheelchair\",\"barrierefrei\",\"barrierefreiheit\"],\n",
    "    \"amenity\":          [\"amenity\",\"type\",\"category\"],\n",
    "    \"latitude\":         [\"latitude\",\"lat\",\"y\"],\n",
    "    \"longitude\":        [\"longitude\",\"lon\",\"lng\",\"x\"],\n",
    "}\n",
    "\n",
    "def standardize_from_csv(df: pd.DataFrame, source_name: str, explicit_map: dict | None = None,\n",
    "                         default_amenity: str = \"veterinary\") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Map arbitrary CSV columns into your target schema.\n",
    "    - `explicit_map`: optional dict {target_col: \"source_col\"} to override auto-mapping.\n",
    "    - Adds a `source` column to track provenance.\n",
    "    - Creates geometry from lat/lon if present; CRS = EPSG:4326.\n",
    "    \"\"\"\n",
    "    explicit_map = explicit_map or {}\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # 1) map all target fields\n",
    "    for target, candidates in SYNONYMS.items():\n",
    "        if target in explicit_map and explicit_map[target] in df.columns:\n",
    "            out[target] = df[explicit_map[target]]\n",
    "        else:\n",
    "            out[target] = pick_first_col(df, candidates)\n",
    "\n",
    "    # Defaults / fixes\n",
    "    if out[\"amenity\"].isna().all():\n",
    "        out[\"amenity\"] = default_amenity\n",
    "\n",
    "    # Guarantee an id (if missing), prefix with source to avoid cross-source collisions\n",
    "    if out[\"clinic_id\"].isna().any():\n",
    "        out[\"clinic_id\"] = out[\"clinic_id\"].fillna(pd.Series([f\"{source_name}_{i}\" for i in range(len(out))], index=out.index))\n",
    "\n",
    "    # Build full_address (simple concatenation; will be rebuilt again after cleaning)\n",
    "    def _addr_row(r):\n",
    "        parts = [r.get(\"street\"), r.get(\"house_number\"), r.get(\"postcode\"), r.get(\"city\")]\n",
    "        parts = [p for p in parts if isinstance(p, str) and p.strip() != \"\"]\n",
    "        return \", \".join(parts) if parts else pd.NA\n",
    "    out[\"full_address\"] = out.apply(_addr_row, axis=1)\n",
    "\n",
    "    # District placeholders\n",
    "    out[\"district_id\"] = pd.NA\n",
    "    out[\"neighbourhood_id\"] = pd.NA\n",
    "\n",
    "    # 2) to GeoDataFrame (geometry from lat/lon if available)\n",
    "    has_latlon = out[\"latitude\"].notna() & out[\"longitude\"].notna()\n",
    "    geom = [Point(float(lon), float(lat)) if (pd.notna(lat) and pd.notna(lon)) else None\n",
    "            for lat, lon in zip(out[\"latitude\"], out[\"longitude\"])]\n",
    "    gdf = gpd.GeoDataFrame(out, geometry=geom, crs=\"EPSG:4326\")\n",
    "\n",
    "    # provenance\n",
    "    gdf[\"source\"] = source_name\n",
    "\n",
    "    return gdf\n",
    "\n",
    "# ---------- ALIGN TWO CSVs ----------\n",
    "csv_a = \"/Users/martinsvitek/layered-populate-data-pool-da/layered-populate-data-pool-da/vet_clinics/sources/taek_berlin_emergency_parsed_v3.csv\"\n",
    "csv_b = \"/Users/martinsvitek/layered-populate-data-pool-da/layered-populate-data-pool-da/vet_clinics/sources/bpt_tierarztsuche_berlin.csv\"\n",
    "\n",
    "df_a = pd.read_csv(csv_a)\n",
    "df_b = pd.read_csv(csv_b)\n",
    "\n",
    "\n",
    "\n",
    "gdf_a_raw = standardize_from_csv(df_a, source_name=\"A\", explicit_map=colmap_a)\n",
    "gdf_b_raw = standardize_from_csv(df_b, source_name=\"B\", explicit_map=colmap_b)\n",
    "\n",
    "# ---------- CLEAN  ----------\n",
    "gdf_a = clean_clinics_target_schema(gdf_a_raw, country=\"DE\", postcode_as_text=True)\n",
    "gdf_b = clean_clinics_target_schema(gdf_b_raw, country=\"DE\", postcode_as_text=True)\n",
    "\n",
    "# ---------- DEDUPE WITHIN EACH SOURCE (exact ID) ----------\n",
    "gdf_a = gdf_a.drop_duplicates(subset=[\"clinic_id\"], keep=\"first\").copy()\n",
    "gdf_b = gdf_b.drop_duplicates(subset=[\"clinic_id\"], keep=\"first\").copy()\n",
    "\n",
    "# ---------- MERGE SOURCES ----------\n",
    "combined = pd.concat([gdf_a, gdf_b], ignore_index=True)\n",
    "\n",
    "# If both sources may share the same ids, keep first occurrence:\n",
    "combined = combined.drop_duplicates(subset=[\"clinic_id\"], keep=\"first\").copy()\n",
    "\n",
    "# ---------- SOFT DEDUPE (no/weak IDs): name + address key ----------\n",
    "def _norm(s):\n",
    "    from unicodedata import normalize\n",
    "    import re\n",
    "    if pd.isna(s): return \"\"\n",
    "    s = normalize(\"NFKC\", str(s)).strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "combined[\"__name_key\"] = combined[\"clinic_name\"].map(_norm)\n",
    "combined[\"__addr_key\"] = (combined[\"street\"].map(_norm) + \" \" +\n",
    "                          combined[\"house_number\"].map(_norm) + \"|\" +\n",
    "                          combined[\"postcode\"].map(_norm))\n",
    "\n",
    "before_soft = len(combined)\n",
    "combined = combined.drop_duplicates(subset=[\"__name_key\",\"__addr_key\"], keep=\"first\").copy()\n",
    "print(f\"Soft de-dupe (name+address): {before_soft - len(combined)} rows dropped\")\n",
    "\n",
    "# ---------- OPTIONAL: SPATIAL + NAME MERGE (node vs polygon–like) ----------\n",
    "# Reuse your `dedupe_clinics` if you have (it prefers polygons, keeps point geom).\n",
    "# Only run if we have some geometries.\n",
    "if \"geometry\" in combined and combined.geometry.notna().any():\n",
    "    try:\n",
    "        combined = dedupe_clinics(combined, distance_m=30, name_col=\"clinic_name\", id_col=\"clinic_id\")\n",
    "    except Exception as e:\n",
    "        print(\"Spatial dedupe skipped (error):\", e)\n",
    "\n",
    "# ---------- FINALIZE ----------\n",
    "# Clean up helper keys\n",
    "combined = combined.drop(columns=[c for c in [\"__name_key\",\"__addr_key\"] if c in combined.columns])\n",
    "\n",
    "# Ensure CRS and lat/lon present\n",
    "if getattr(combined, \"crs\", None) is None:\n",
    "    combined = combined.set_crs(4326, allow_override=True)\n",
    "elif combined.crs.to_epsg() != 4326:\n",
    "    combined = combined.to_crs(4326)\n",
    "\n",
    "if \"latitude\" not in combined.columns or \"longitude\" not in combined.columns:\n",
    "    combined[\"latitude\"]  = combined.geometry.y\n",
    "    combined[\"longitude\"] = combined.geometry.x\n",
    "\n",
    "# Save\n",
    "combined.to_file(\"vet_clinics_aligned_merged.geojson\", driver=\"GeoJSON\")\n",
    "combined.drop(columns=\"geometry\").to_csv(\"vet_clinics_aligned_merged.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "combined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f8ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "one mopre step in cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe00c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# --- Load only if the variable doesn't already exist ---\n",
    "try:\n",
    "    vet_clinics_aligned_merged\n",
    "except NameError:\n",
    "    vet_clinics_aligned_merged = pd.read_csv(\"vet_clinics_aligned_merged.csv\")\n",
    "\n",
    "# --- Split helpers ---\n",
    "_HN_TRAIL_RE = re.compile(r\"\"\"\n",
    "    ^\\s*(?P<street>.*?\\S)[,\\s]*(?P<hn>\\d{1,5}(?:\\s?[A-Za-z])?(?:\\s?[\\/\\-–—]\\s?\\d{1,5}\\s?[A-Za-z]?)?)\\s*$\n",
    "\"\"\", re.X)\n",
    "\n",
    "_HN_LEAD_RE = re.compile(r\"\"\"\n",
    "    ^\\s*(?P<hn>\\d{1,5}(?:\\s?[A-Za-z])?(?:\\s?[\\/\\-–—]\\s?\\d{1,5}\\s?[A-Za-z]?)?)[,\\s]+(?P<street>.+?)\\s*$\n",
    "\"\"\", re.X)\n",
    "\n",
    "def _split_street_hn(s: str):\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return (s, None)\n",
    "    t = re.sub(r\"[–—−]\", \"-\", s.strip())\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    m = _HN_TRAIL_RE.match(t) or _HN_LEAD_RE.match(t)\n",
    "    if m:\n",
    "        street = m.group(\"street\").strip().rstrip(\",\")\n",
    "        hn = re.sub(r\"\\s+\", \"\", m.group(\"hn\"))\n",
    "        return (street, hn)\n",
    "    return (t, None)\n",
    "\n",
    "def clean_street_and_house_number(df: pd.DataFrame,\n",
    "                                  street_col=\"street\",\n",
    "                                  house_col=\"house_number\") -> pd.DataFrame:\n",
    "    if street_col not in df.columns:\n",
    "        raise KeyError(f\"'{street_col}' column not found\")\n",
    "\n",
    "    # split\n",
    "    split_series = df[street_col].map(_split_street_hn)\n",
    "    new_street   = split_series.map(lambda t: t[0])\n",
    "    extracted_hn = split_series.map(lambda t: t[1])\n",
    "\n",
    "    # ensure house_number exists\n",
    "    if house_col not in df.columns:\n",
    "        df[house_col] = pd.NA\n",
    "\n",
    "    # fill only where current house_number is empty\n",
    "    has_existing = df[house_col].astype(str).str.strip().ne(\"\").fillna(False)\n",
    "    df.loc[~has_existing, house_col] = extracted_hn\n",
    "\n",
    "    # update street\n",
    "    df[street_col] = new_street\n",
    "    return df\n",
    "\n",
    "# --- Apply ONLY to this dataframe ---\n",
    "vet_clinics_aligned_merged = clean_street_and_house_number(\n",
    "    vet_clinics_aligned_merged,\n",
    "    street_col=\"street\",\n",
    "    house_col=\"house_number\"\n",
    ")\n",
    "\n",
    "# (Optional) save a cleaned copy\n",
    "vet_clinics_aligned_merged.to_csv(\"vet_clinics_aligned_merged_cleaned.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# quick peek\n",
    "vet_clinics_aligned_merged[[\"street\", \"house_number\"]].head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefc065e",
   "metadata": {},
   "source": [
    "join toghether for end file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e80e156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# --- Pick where to load LEFT from (tries a few common filenames) ---\n",
    "CANDIDATES = [\n",
    "    \"berlin_vet_clinics_clean.csv\",\n",
    "    \"berlin_vet_clinics_clean.geojson\",\n",
    "]\n",
    "\n",
    "def _load_any(path):\n",
    "    if path.lower().endswith((\".geojson\", \".gpkg\", \".shp\", \".json\")):\n",
    "        return gpd.read_file(path)\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "# Use in-memory variable if it already exists\n",
    "left = globals().get(\"berlin_vet_clinics_clean\", None)\n",
    "if left is None:\n",
    "    for p in CANDIDATES:\n",
    "        if os.path.exists(p):\n",
    "            try:\n",
    "                left = _load_any(p)\n",
    "                print(f\"Loaded LEFT from: {p} → {type(left).__name__}, shape={left.shape}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {p}: {e}\")\n",
    "\n",
    "if left is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Couldn't load LEFT dataset. Put one of these files next to the notebook: \"\n",
    "        + \", \".join(CANDIDATES)\n",
    "    )\n",
    "\n",
    "# RIGHT dataset (your merged CSV)\n",
    "right_path = \"vet_clinics_aligned_merged_cleaned.csv\"\n",
    "right = pd.read_csv(right_path)\n",
    "print(f\"Loaded RIGHT from: {right_path} → DataFrame, shape={right.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d806eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install rapidfuzz\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import unicodedata, re, importlib.util as iu\n",
    "\n",
    "# optional fuzzy matcher (no try/except)\n",
    "if iu.find_spec(\"rapidfuzz\") is not None:\n",
    "    from rapidfuzz import fuzz, process\n",
    "else:\n",
    "    fuzz = process = None\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def normalize_name(s):\n",
    "    \"\"\"Robust key for clinic names (case/accents/punct/stopwords stripped).\"\"\"\n",
    "    if s is None or (isinstance(s, float) and pd.isna(s)):\n",
    "        return None\n",
    "    s = str(s).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    # de-accent → lowercase\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch)).lower()\n",
    "    # remove common fillers/legal forms\n",
    "    s = re.sub(r\"(tierarztpraxis|tierklinik|tieraerztliche praxis|tierärztliche praxis|praxis|vet|veterinaer|veterinär|gmbh|ag|kg|ug|e\\.k\\.)\", \" \", s)\n",
    "    s = re.sub(r\"[\\s\\-/_,.&+()'’`]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s or None\n",
    "\n",
    "def _as_df(path):\n",
    "    \"\"\"Load GeoJSON/GeoPackage as GeoDataFrame, else CSV as DataFrame.\"\"\"\n",
    "    try:\n",
    "        return gpd.read_file(path)\n",
    "    except Exception:\n",
    "        return pd.read_csv(path)\n",
    "\n",
    "# ---------- load data (no try/except) ----------\n",
    "left = globals().get(\"berlin_vet_clinics_clean\", None)\n",
    "\n",
    "\n",
    "right = pd.read_csv(\"vet_clinics_aligned_merged_cleaned.csv\")\n",
    "\n",
    "# ensure clinic_name column\n",
    "for df in (left, right):\n",
    "    if \"clinic_name\" not in df.columns and \"name\" in df.columns:\n",
    "        df.rename(columns={\"name\": \"clinic_name\"}, inplace=True)\n",
    "\n",
    "if \"clinic_name\" not in left.columns or \"clinic_name\" not in right.columns:\n",
    "    raise KeyError(\"Both datasets must have a 'clinic_name' (or 'name') column.\")\n",
    "\n",
    "# ---------- normalized name keys ----------\n",
    "left[\"name_key\"]  = left[\"clinic_name\"].map(normalize_name)\n",
    "right[\"name_key\"] = right[\"clinic_name\"].map(normalize_name)\n",
    "\n",
    "l_key = left.dropna(subset=[\"name_key\"]).copy()\n",
    "r_key = right.dropna(subset=[\"name_key\"]).copy()\n",
    "\n",
    "# ---------- schema diffs ----------\n",
    "cols_left, cols_right = set(left.columns), set(right.columns)\n",
    "new_cols_in_right     = sorted(cols_right - cols_left)\n",
    "missing_cols_in_right = sorted(cols_left - cols_right)\n",
    "\n",
    "print(\"=== Column differences ===\")\n",
    "print(\"New columns in RIGHT:\", new_cols_in_right)\n",
    "print(\"Columns missing in RIGHT (present in LEFT):\", missing_cols_in_right)\n",
    "\n",
    "# ---------- duplicates by normalized name ----------\n",
    "dups_left  = l_key[l_key.duplicated(\"name_key\", keep=False)].sort_values([\"name_key\",\"clinic_name\"])\n",
    "dups_right = r_key[r_key.duplicated(\"name_key\", keep=False)].sort_values([\"name_key\",\"clinic_name\"])\n",
    "\n",
    "print(\"\\n=== Duplicates (by normalized clinic_name) ===\")\n",
    "print(f\"LEFT duplicates: {dups_left['name_key'].nunique()} names, {len(dups_left)} rows\")\n",
    "print(f\"RIGHT duplicates: {dups_right['name_key'].nunique()} names, {len(dups_right)} rows\")\n",
    "\n",
    "# ---------- missing clinics (exact name_key match) ----------\n",
    "missing_on_left  = r_key[~r_key[\"name_key\"].isin(l_key[\"name_key\"])]\n",
    "missing_on_right = l_key[~l_key[\"name_key\"].isin(r_key[\"name_key\"])]\n",
    "\n",
    "print(\"\\n=== Missing clinics (by name) ===\")\n",
    "print(f\"In RIGHT not in LEFT:  {missing_on_left['name_key'].nunique()} clinics\")\n",
    "print(f\"In LEFT not in RIGHT:  {missing_on_right['name_key'].nunique()} clinics\")\n",
    "\n",
    "# ---------- write audit CSVs ----------\n",
    "dups_left.to_csv(\"audit_left_duplicates_by_name.csv\", index=False, encoding=\"utf-8\")\n",
    "dups_right.to_csv(\"audit_right_duplicates_by_name.csv\", index=False, encoding=\"utf-8\")\n",
    "missing_on_left.to_csv(\"audit_missing_on_left_by_name.csv\", index=False, encoding=\"utf-8\")\n",
    "missing_on_right.to_csv(\"audit_missing_on_right_by_name.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# ---------- optional fuzzy suggestions ----------\n",
    "if process is not None:\n",
    "    print(\"\\n=== Fuzzy suggestions (RIGHT → LEFT) ===\")\n",
    "    left_keys  = l_key[[\"name_key\",\"clinic_name\"]].drop_duplicates()\n",
    "    right_keys = r_key[[\"name_key\",\"clinic_name\"]].drop_duplicates()\n",
    "    right_unmatched = right_keys[~right_keys[\"name_key\"].isin(left_keys[\"name_key\"])]\n",
    "\n",
    "    lookup = dict(left_keys.values)\n",
    "    left_key_list = list(left_keys[\"name_key\"])\n",
    "\n",
    "    suggestions = []\n",
    "    for _, row in right_unmatched.iterrows():\n",
    "        rk = row[\"name_key\"]\n",
    "        best = process.extractOne(rk, left_key_list, scorer=fuzz.token_set_ratio)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9eba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, unicodedata\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# --------- CONFIG: set your file paths here ----------\n",
    "LEFT_PATH  = \"berlin_vet_clinics_clean.csv\"                # your main dataset\n",
    "RIGHT_PATH = \"vet_clinics_aligned_merged_cleaned.csv\"      # the aligned/merged CSV\n",
    "# ------------------------------------------------------\n",
    "\n",
    "def _load_table(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "    low = path.lower()\n",
    "    if low.endswith((\".geojson\", \".gpkg\", \".shp\", \".json\")):\n",
    "        return gpd.read_file(path)\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def _norm_name(s):\n",
    "    if pd.isna(s): return None\n",
    "    s = unicodedata.normalize(\"NFKC\", str(s)).strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s).lower()\n",
    "    s = re.sub(r\"[^\\w &]\", \"\", s)  # strip punctuation except word chars/space/&\n",
    "    return s or None\n",
    "\n",
    "# 1) Load both datasets (LEFT can be GeoDataFrame or DataFrame)\n",
    "left  = _load_table(LEFT_PATH)\n",
    "right = _load_table(RIGHT_PATH)\n",
    "\n",
    "# 2) Ensure `clinic_name` exists\n",
    "for df in (left, right):\n",
    "    if df is None:\n",
    "        raise ValueError(\"A dataset failed to load and is None.\")\n",
    "    if \"clinic_name\" not in df.columns and \"name\" in df.columns:\n",
    "        df.rename(columns={\"name\": \"clinic_name\"}, inplace=True)\n",
    "\n",
    "if \"clinic_name\" not in left.columns or \"clinic_name\" not in right.columns:\n",
    "    raise KeyError(\"Both datasets must have a `clinic_name` column (rename `name` -> `clinic_name`).\")\n",
    "\n",
    "# 3) Preserve geometry if LEFT is a GeoDataFrame\n",
    "left_is_gdf = isinstance(left, gpd.GeoDataFrame)\n",
    "left_geom = left.geometry if left_is_gdf else None\n",
    "if left_is_gdf:\n",
    "    left = pd.DataFrame(left.drop(columns=left.geometry.name))\n",
    "\n",
    "# 4) Build normalized join key\n",
    "left[\"name_key\"]  = left[\"clinic_name\"].map(_norm_name)\n",
    "right[\"name_key\"] = right[\"clinic_name\"].map(_norm_name)\n",
    "left  = left[~left[\"name_key\"].isna()].copy()\n",
    "right = right[~right[\"name_key\"].isna()].copy()\n",
    "\n",
    "# 5) Quick audit\n",
    "cols_new_in_right = sorted(set(right.columns) - set(left.columns) - {\"name_key\"})\n",
    "dup_left  = left[left.duplicated(\"name_key\", keep=False)].sort_values(\"name_key\")\n",
    "dup_right = right[right.duplicated(\"name_key\", keep=False)].sort_values(\"name_key\")\n",
    "missing_in_left = sorted(set(right[\"name_key\"]) - set(left[\"name_key\"]))\n",
    "\n",
    "print(f\"New columns in RIGHT not in LEFT: {cols_new_in_right}\")\n",
    "print(f\"Duplicate rows in LEFT (by clinic_name): {dup_left.shape[0]}\")\n",
    "print(f\"Duplicate rows in RIGHT (by clinic_name): {dup_right.shape[0]}\")\n",
    "print(f\"Names present in RIGHT but missing in LEFT: {len(missing_in_left)}\")\n",
    "\n",
    "# 6) Bring columns from RIGHT → LEFT\n",
    "to_add = cols_new_in_right.copy()\n",
    "overlap_fill = [c for c in right.columns\n",
    "                if c in left.columns and c not in (\"clinic_id\",\"clinic_name\",\"name_key\")]\n",
    "\n",
    "r_small = right[[\"name_key\"] + to_add + overlap_fill].copy()\n",
    "merged = left.merge(r_small, on=\"name_key\", how=\"left\", suffixes=(\"\", \"_r\"))\n",
    "\n",
    "# Fill NULLs in overlapping columns with RIGHT values\n",
    "for c in overlap_fill:\n",
    "    merged[c] = merged[c].fillna(merged[c + \"_r\"])\n",
    "    merged.drop(columns=[c + \"_r\"], inplace=True)\n",
    "\n",
    "# Remove helper key\n",
    "merged.drop(columns=[\"name_key\"], inplace=True)\n",
    "\n",
    "# 7) Reattach geometry if LEFT was GeoDataFrame\n",
    "if left_is_gdf:\n",
    "    merged = gpd.GeoDataFrame(merged, geometry=left_geom.loc[merged.index], crs=\"EPSG:4326\")\n",
    "\n",
    "# 8) Save\n",
    "out_csv = \"berlin_vet_clinics_clean_enriched_from_aligned.csv\"\n",
    "merged.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "print(\"Saved:\", out_csv, merged.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d51ed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "last fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e809e649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "def _ensure_geodf(df, crs=\"EPSG:4326\"):\n",
    "    \"\"\"Make sure we have a GeoDataFrame with point geometries from lat/lon.\"\"\"\n",
    "    if isinstance(df, gpd.GeoDataFrame):\n",
    "        gdf = df.copy()\n",
    "        if gdf.crs is None: gdf.set_crs(crs, inplace=True, allow_override=True)\n",
    "        elif gdf.crs.to_string() != crs: gdf = gdf.to_crs(crs)\n",
    "        return gdf\n",
    "    # build geometry from lat/lon\n",
    "    if not {\"latitude\",\"longitude\"}.issubset(df.columns):\n",
    "        raise ValueError(\"Need latitude/longitude columns if input is a plain DataFrame.\")\n",
    "    gdf = gpd.GeoDataFrame(df.copy(),\n",
    "                           geometry=gpd.points_from_xy(df[\"longitude\"], df[\"latitude\"]),\n",
    "                           crs=crs)\n",
    "    return gdf\n",
    "\n",
    "def attach_ids_from_geojson(clinics, geojson_path, id_cols, out_cols):\n",
    "    \"\"\"\n",
    "    Spatially joins polygons to clinics.\n",
    "    - id_cols: columns in polygons to copy (e.g., [\"district_id\", \"neighbourhood_id\"])\n",
    "    - out_cols: destination columns in clinics (same length as id_cols)\n",
    "    \"\"\"\n",
    "    gdf = _ensure_geodf(clinics, \"EPSG:4326\")\n",
    "    polys = gpd.read_file(geojson_path)\n",
    "    if polys.crs is None: polys.set_crs(\"EPSG:4326\", inplace=True, allow_override=True)\n",
    "    else: polys = polys.to_crs(\"EPSG:4326\")\n",
    "\n",
    "    # If the polygon properties use different names, rename here once:\n",
    "    # polys = polys.rename(columns={\"bezirk_id\":\"district_id\",\"ortsteil_id\":\"neighbourhood_id\"})\n",
    "\n",
    "    keep = [c for c in id_cols if c in polys.columns]\n",
    "    if len(keep) != len(id_cols):\n",
    "        missing = [c for c in id_cols if c not in polys.columns]\n",
    "        raise KeyError(f\"Missing columns in polygons: {missing}\")\n",
    "\n",
    "    joined = gpd.sjoin(gdf, polys[keep + [\"geometry\"]], how=\"left\", predicate=\"within\")\n",
    "    for src, dst in zip(id_cols, out_cols):\n",
    "        joined[dst] = joined[dst].fillna(joined[src]) if dst in joined.columns else joined[src]\n",
    "        if src != dst: \n",
    "            # keep original dst if it was already filled; otherwise assign src\n",
    "            mask = joined[dst].isna()\n",
    "            joined.loc[mask, dst] = joined.loc[mask, src]\n",
    "    joined = joined.drop(columns=[c for c in id_cols if c not in out_cols] + [\"index_right\"], errors=\"ignore\")\n",
    "    return joined\n",
    "\n",
    "# --- Example usage (edit paths & property names to your files) ---\n",
    "# If your clinics are in CSV:\n",
    "clinics = pd.read_csv(\"berlin_vet_clinics_clean_enriched_from_aligned.csv\")\n",
    "# If they’re already a GeoDataFrame:\n",
    "# clinics = gpd.read_file(\"berlin_vet_clinics_normalized.geojson\")\n",
    "\n",
    "# Fill from districts polygons\n",
    "clinics = attach_ids_from_geojson(\n",
    "    clinics,\n",
    "    \"lor_ortsteile.geojson\",\n",
    "    id_cols=[\"district_id\"],           # or [\"bezirk_id\"] then use a rename above\n",
    "    out_cols=[\"district_id\"]\n",
    ")\n",
    "\n",
    "# Then fill from neighbourhood polygons\n",
    "clinics = attach_ids_from_geojson(\n",
    "    clinics,\n",
    "    \"lor_ortsteile.geojson\",\n",
    "    id_cols=[\"neighbourhood_id\"],      # or [\"ortsteil_id\"]\n",
    "    out_cols=[\"neighbourhood_id\"]\n",
    ")\n",
    "\n",
    "# Save back out\n",
    "# clinics.to_file(\"berlin_vet_clinics_with_admin.gpkg\", layer=\"clinics\", driver=\"GPKG\")\n",
    "# clinics.drop(columns=\"geometry\").to_csv(\"berlin_vet_clinics_with_admin.csv\", index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a149c503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
