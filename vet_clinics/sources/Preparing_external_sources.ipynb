{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9a8df2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/martinsvitek/layered-populate-data-pool-da/layered-populate-data-pool-da/vet_clinics\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (4.13.5)\n",
      "Requirement already satisfied: lxml in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (6.0.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from beautifulsoup4) (4.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "   name                                          full_text address  \\\n",
      "0  None  Klinik für Klein- und Heimtiere, Alt-Biesdorf ...    None   \n",
      "1  None  Klinik für Kleintiere (Olof Löwe), Märkische A...    None   \n",
      "2  None  valera – Medizinisches Kleintierzentrum Berlin...    None   \n",
      "3  None  Tierarztpraxis Bärenwiese, Uhlandstr 151, 1071...    None   \n",
      "4  None  Tierarztpraxis Rödiger, Scharnweberstr. 136, 1...    None   \n",
      "\n",
      "              phone opening_hours_raw  \n",
      "0     030 51 43 760              None  \n",
      "1     030 93 22 093              None  \n",
      "2  030 201 80 57 50              None  \n",
      "3   030 23 36 26 27              None  \n",
      "4     030 412 73 57              None  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f2/z3f134vj17zg0vm9xq6gmy4h0000gn/T/ipykernel_51537/54872285.py:45: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  oh_el = card.find(text=re.compile(\"Öffnungs|opening\", re.I))\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/martinsvitek/layered-populate-data-pool-da/layered-populate-data-pool-da/vet_clinics/\n",
    "%pip install beautifulsoup4 lxml\n",
    "\n",
    "import time, re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"research-bot/1.0 (contact: you@example.com)\"}\n",
    "URL = \"https://tieraerztekammer-berlin.de/notdienst/\"  \n",
    "\n",
    "def clean_text(s):\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def scrape_taek_berlin_emergency(url=URL):\n",
    "    r = requests.get(url, headers=HEADERS, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    # Find the main container that lists clinics (adjust selector to the page)\n",
    "    container = soup.select_one(\"main, .content, .article\") or soup\n",
    "    cards = container.select(\".clinic, .entry, article, li\")  # be flexible\n",
    "\n",
    "    rows = []\n",
    "    for card in cards:\n",
    "        txt = clean_text(card.get_text(\" \", strip=True))\n",
    "\n",
    "        name_el = card.select_one(\"h2, h3, .title, .clinic-name\")\n",
    "        name = clean_text(name_el.get_text()) if name_el else None\n",
    "\n",
    "        addr = None\n",
    "        addr_el = card.select_one(\".address, address, .adr\")\n",
    "        if addr_el:\n",
    "            addr = clean_text(addr_el.get_text(\" \"))\n",
    "\n",
    "        phone = None\n",
    "        tel_el = card.select_one(\"a[href^='tel'], .phone, .tel\")\n",
    "        if tel_el:\n",
    "            phone = clean_text(tel_el.get_text() or tel_el.get(\"href\"))\n",
    "        else:\n",
    "            m = re.search(r\"(?:\\+49|0)\\s?[\\d ()\\-\\/]{6,}\", txt)\n",
    "            phone = m.group(0) if m else None\n",
    "\n",
    "        oh = None\n",
    "        oh_el = card.find(text=re.compile(\"Öffnungs|opening\", re.I))\n",
    "        if oh_el:\n",
    "            oh = clean_text(oh_el.parent.get_text(\" \"))\n",
    "\n",
    "        rows.append({\n",
    "            \"name\": name or None,\n",
    "            \"full_text\": txt,\n",
    "            \"address\": addr,\n",
    "            \"phone\": phone,\n",
    "            \"opening_hours_raw\": oh\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows).drop_duplicates()\n",
    "    return df\n",
    "\n",
    "df = scrape_taek_berlin_emergency()\n",
    "df.to_csv(\"taek_berlin_emergency.csv\", index=False)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5259e3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           full_text           street  \\\n",
      "0  Klinik für Klein- und Heimtiere, Alt-Biesdorf ...     Alt-Biesdorf   \n",
      "1  Klinik für Kleintiere (Olof Löwe), Märkische A...  Märkische Allee   \n",
      "2  valera – Medizinisches Kleintierzentrum Berlin...   Potsdamer Str.   \n",
      "\n",
      "  house_number postcode    city    phone_number email website opening_hours  \\\n",
      "0           22    12683  Berlin    +49305143760  None    None          None   \n",
      "1          258    12679  Berlin    +49309322093  None    None          None   \n",
      "2           23    14163  Berlin  +4930201805750  None    None          None   \n",
      "\n",
      "                        full_address  \n",
      "0      Alt-Biesdorf 22, 12683 Berlin  \n",
      "1  Märkische Allee 258, 12679 Berlin  \n",
      "2    Potsdamer Str. 23, 14163 Berlin  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1) Load your uploaded CSV\n",
    "df = pd.read_csv(\"/Users/martinsvitek/layered-populate-data-pool-da/layered-populate-data-pool-da/vet_clinics/sources/taek_berlin_emergency.csv\", encoding=\"utf-8\")\n",
    "assert \"full_text\" in df.columns, \"full_text column missing\"\n",
    "\n",
    "# --- Regexes (tune for your data) ---\n",
    "RE_EMAIL   = r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+'\n",
    "RE_WEBSITE = r'(https?://[^\\s,;]+|www\\.[^\\s,;]+)'\n",
    "RE_PHONE   = r'(?:(?:\\+?\\s?49)|(?:\\+?\\s?49\\(0\\))|(?:0))[\\s()/\\-]*\\d[\\d\\s()/\\-]{5,}'\n",
    "RE_POSTCODE_CITY = r'(?P<postcode>\\b\\d{5}\\b)\\s+(?P<city>[A-Za-zÄÖÜäöüß\\-\\s]+)'\n",
    "RE_STREET  = r'(?P<street>[A-Za-zÄÖÜäöüß\\.\\-\\s]+?)\\s+(?P<housenumber>\\d+[A-Za-z]?)'\n",
    "RE_OPENING = r'(Öffnungszeiten|Opening hours|Öffnung|hours)\\s*[:\\-]?\\s*(?P<opening_hours>[^•\\|;]+)'\n",
    "\n",
    "def first_match(pattern, text, flags=re.IGNORECASE):\n",
    "    if pd.isna(text): return None\n",
    "    m = re.search(pattern, str(text), flags)\n",
    "    return m.group(0) if m else None\n",
    "\n",
    "def extract_group(pattern, text, group, flags=re.IGNORECASE):\n",
    "    if pd.isna(text): return None\n",
    "    m = re.search(pattern, str(text), flags)\n",
    "    return m.group(group) if m else None\n",
    "\n",
    "def norm_space(s):\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip() if isinstance(s, str) else s\n",
    "\n",
    "def normalize_phone(p):\n",
    "    if not p: return None\n",
    "    s = re.sub(r\"[^\\d+]\", \"\", p)\n",
    "    if s.startswith(\"0\"):  # naive DE normalization\n",
    "        s = \"+49\" + s[1:]\n",
    "    return s\n",
    "\n",
    "def normalize_url(u):\n",
    "    if not u: return None\n",
    "    return u if u.startswith((\"http://\",\"https://\")) else \"https://\" + u\n",
    "\n",
    "# --- Extract ---\n",
    "s = df[\"full_text\"].fillna(\"\")\n",
    "\n",
    "df[\"email\"]          = s.map(lambda t: first_match(RE_EMAIL, t))\n",
    "df[\"website\"]        = s.map(lambda t: normalize_url(first_match(RE_WEBSITE, t)))\n",
    "df[\"phone_number\"]   = s.map(lambda t: normalize_phone(first_match(RE_PHONE, t)))\n",
    "df[\"opening_hours\"]  = s.map(lambda t: extract_group(RE_OPENING, t, \"opening_hours\"))\n",
    "\n",
    "df[\"street\"]         = s.map(lambda t: extract_group(RE_STREET, t, \"street\")).map(norm_space)\n",
    "df[\"house_number\"]   = s.map(lambda t: extract_group(RE_STREET, t, \"housenumber\")).map(norm_space)\n",
    "df[\"postcode\"]       = s.map(lambda t: extract_group(RE_POSTCODE_CITY, t, \"postcode\"))\n",
    "df[\"city\"]           = s.map(lambda t: extract_group(RE_POSTCODE_CITY, t, \"city\")).map(norm_space)\n",
    "\n",
    "# Build full_address from parts (only non-null pieces)\n",
    "def compose_address(row):\n",
    "    a = \" \".join([x for x in [row.street, row.house_number] if pd.notna(x) and x])\n",
    "    b = \" \".join([x for x in [row.postcode, row.city] if pd.notna(x) and x])\n",
    "    return \", \".join([x for x in [a, b] if x]) or None\n",
    "\n",
    "df[\"full_address\"] = df.apply(compose_address, axis=1)\n",
    "\n",
    "# Optional: clean up whitespace in extracted columns\n",
    "for col in [\"email\",\"website\",\"phone_number\",\"opening_hours\",\"street\",\"house_number\",\"postcode\",\"city\",\"full_address\"]:\n",
    "    df[col] = df[col].map(norm_space)\n",
    "\n",
    "# --- Save result ---\n",
    "df.to_csv(\"taek_berlin_emergency_parsed.csv\", index=False)\n",
    "print(df.head(3)[[\"full_text\",\"street\",\"house_number\",\"postcode\",\"city\",\"phone_number\",\"email\",\"website\",\"opening_hours\",\"full_address\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de625f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# German day abbreviations\n",
    "DAYS_DE = r\"(?:Mo|Di|Mi|Do|Fr|Sa|So)\"\n",
    "\n",
    "# OSM-ish compact patterns like: \"Mo-Fr 09:00-18:00; Sa 10:00-14:00\"\n",
    "RE_HOURS_OSMISH = re.compile(\n",
    "    rf\"\\b{DAYS_DE}(?:[,\\-/ ]\\s*{DAYS_DE})*\\s+\\d{{1,2}}[:.]?\\d{{2}}\\s*-\\s*\\d{{1,2}}[:.]?\\d{{2}}\"\n",
    "    rf\"(?:\\s*;\\s*{DAYS_DE}(?:[,\\-/ ]\\s*{DAYS_DE})*\\s+\\d{{1,2}}[:.]?\\d{{2}}\\s*-\\s*\\d{{1,2}}[:.]?\\d{{2}})*\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Labeled variants like \"Öffnungszeiten: Mo–Fr 9-18 Uhr\" or \"Sprechzeiten - ...\"\n",
    "RE_HOURS_LABELED = re.compile(\n",
    "    r\"(Öffnungs(?:zeiten|zeit)|Sprechzeiten?|Sprechstunde|Opening hours)\\s*[:\\-–]?\\s*(?P<label_hours>[^\\n\\r|•;]+)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Common \"emergency\" cues\n",
    "RE_EMERGENCY_FLAG = re.compile(\n",
    "    r\"\\b(Notfall(?:e)?|Notfälle|Notdienst|Notfallsprechstunde|Notaufnahme|24\\s*h|24h|24\\s*Std\\.?|rund um die Uhr|emergency)\\b\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Labeled emergency details like \"Notdienst: 24h\" or \"Notfälle – Tel. 030 ...\"\n",
    "RE_EMERGENCY_DETAILS = re.compile(\n",
    "    r\"(Notdienst|Notfälle?|Notfallsprechstunde)\\s*[:\\-–]?\\s*(?P<em_details>[^\\n\\r|•;]+)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Optional phone pattern if you want to capture emergency phone on the same line\n",
    "RE_PHONE = re.compile(r\"(?:\\+49|0)[\\d\\s()/\\-]{6,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7dc8273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_opening_hours(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    # Prefer structured/OSM-like first\n",
    "    m = RE_HOURS_OSMISH.search(text)\n",
    "    if m:\n",
    "        return m.group(0).strip()\n",
    "    # Fallback to labeled phrase\n",
    "    m2 = RE_HOURS_LABELED.search(text)\n",
    "    if m2:\n",
    "        return m2.group(\"label_hours\").strip()\n",
    "    return None\n",
    "\n",
    "def extract_emergency(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        return False, None, None\n",
    "    flag = bool(RE_EMERGENCY_FLAG.search(text))\n",
    "    details = None\n",
    "    phone = None\n",
    "\n",
    "    m = RE_EMERGENCY_DETAILS.search(text)\n",
    "    if m:\n",
    "        details = m.group(\"em_details\").strip()\n",
    "        # try to find a phone number inside the details\n",
    "        pm = RE_PHONE.search(details)\n",
    "        if pm:\n",
    "            phone = pm.group(0).strip()\n",
    "\n",
    "    return flag, details, phone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6c0d22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"your_file.csv\", encoding=\"utf-8\")\n",
    "assert \"full_text\" in df.columns, \"full_text column missing\"\n",
    "\n",
    "# Opening hours\n",
    "df[\"opening_hours_raw\"] = df[\"full_text\"].apply(extract_opening_hours)\n",
    "\n",
    "# Emergency fields\n",
    "out = df[\"full_text\"].apply(extract_emergency)\n",
    "df[\"emergency_flag\"]    = out.apply(lambda t: t[0])\n",
    "df[\"emergency_details\"] = out.apply(lambda t: (t[1] or None))\n",
    "df[\"emergency_phone\"]   = out.apply(lambda t: (t[2] or None))\n",
    "df.to_csv(\"taek_berlin_emergency_parsed_v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d27d63fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved without overwriting v2 → taek_berlin_emergency_parsed_v4.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 0) Load existing file (v2) ---\n",
    "\n",
    "df = pd.read_csv(\"/Users/martinsvitek/layered-populate-data-pool-da/layered-populate-data-pool-da/vet_clinics/sources/taek_berlin_emergency.csv\", encoding=\"utf-8\")\n",
    "assert \"full_text\" in df.columns, \"full_text column missing in v2 file\"\n",
    "\n",
    "# --- 1) Regex patterns (tuned for your German examples) ---\n",
    "DAYS_DE  = r\"(?:Mo|Di|Mi|Do|Fr|Sa|So)\"\n",
    "TIME     = r\"\\d{1,2}[:.]?\\d{0,2}\\s*(?:Uhr)?\"\n",
    "RANGE    = rf\"{TIME}\\s*[-–]\\s*{TIME}\"\n",
    "RANGE_OR = rf\"{RANGE}(?:\\s*(?:u\\.?|und)\\s*{RANGE})*\"\n",
    "\n",
    "RE_HOURS_BLOCK = re.compile(\n",
    "    rf\"(?:(?:{DAYS_DE})(?:\\s*[-–]\\s*{DAYS_DE})?\\s+{RANGE_OR})\"\n",
    "    rf\"(?:\\s*(?:[|;,\\n]\\s*|\\s{DAYS_DE}\\s))?\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "RE_HOURS_LABELED = re.compile(\n",
    "    r\"(Öffnungs(?:zeiten|zeit)|Sprechzeiten?|Sprechstunde|Opening hours)\\s*[:\\-–]?\\s*(?P<label_hours>[^|;•\\n\\r]+)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "RE_EMERGENCY_FLAG = re.compile(\n",
    "    r\"\\b(?:Notdienst|Notfälle?|Notfallsprechstunde|Notaufnahme|Feiertagsnotdienst|24\\s*h|24h|rund um die Uhr|emergency)\\b\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "RE_EMERGENCY_DETAILS = re.compile(\n",
    "    r\"(Notdienst|Notfälle?|Notfallsprechstunde|Feiertagsnotdienst)\\s*[:\\-–]?\\s*(?P<em_details>[^|;•\\n\\r]+)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def extract_opening_hours(text: str):\n",
    "    if not isinstance(text, str): \n",
    "        return None\n",
    "    blocks = [m.group(0).strip(\" ,;|\") for m in RE_HOURS_BLOCK.finditer(text)]\n",
    "    if blocks:\n",
    "        return \" | \".join(b for b in blocks if b)\n",
    "    m = RE_HOURS_LABELED.search(text)\n",
    "    return m.group(\"label_hours\").strip() if m else None\n",
    "\n",
    "def extract_emergency(text: str):\n",
    "    if not isinstance(text, str): \n",
    "        return False, None\n",
    "    flag = bool(RE_EMERGENCY_FLAG.search(text))\n",
    "    details = None\n",
    "    m = RE_EMERGENCY_DETAILS.search(text)\n",
    "    if m:\n",
    "        details = m.group(\"em_details\").strip(\" ,;|\")\n",
    "    return flag, details\n",
    "\n",
    "# --- 2) Compute new values (from existing full_text) ---\n",
    "s = df[\"full_text\"].fillna(\"\")\n",
    "new_hours = s.map(extract_opening_hours)\n",
    "new_em    = s.map(extract_emergency)\n",
    "new_flag  = new_em.map(lambda x: x[0])\n",
    "new_det   = new_em.map(lambda x: x[1])\n",
    "\n",
    "# --- 3) Add/adjust without overwriting existing non-null values ---\n",
    "for col, series in {\n",
    "    \"opening_hours_raw\": new_hours,\n",
    "    \"emergency_flag\":    new_flag,\n",
    "    \"emergency_details\": new_det,\n",
    "}.items():\n",
    "    if col not in df.columns:\n",
    "        df[col] = series                      # add fresh\n",
    "    else:\n",
    "        # only fill where currently missing/NaN; keep existing values\n",
    "        df[col] = df[col].where(df[col].notna(), series)\n",
    "\n",
    "# --- 4) Save to a NEW versioned file (v3, v4, …) ---\n",
    "base = in_path.stem.replace(\"_v2\", \"\")  # handle typical naming\n",
    "parent = in_path.parent\n",
    "version = 3\n",
    "while True:\n",
    "    out_path = parent / f\"{base}_v{version}.csv\"\n",
    "    if not out_path.exists():\n",
    "        break\n",
    "    version += 1\n",
    "\n",
    "df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"✅ Saved without overwriting v2 → {out_path.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d565d8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /Users/martinsvitek/layered-populate-data-pool-da/layered-populate-data-pool-da/vet_clinics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e45fad12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2 cols: 17\n",
      "v3 cols: 7\n",
      "Missing in v3: ['city', 'email', 'emergency_phone', 'full_address', 'house_number', 'opening_hours', 'phone_number', 'postcode', 'street', 'website']\n",
      "New in v3: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "v2 = \"/Users/martinsvitek/layered-populate-data-pool-da/layered-populate-data-pool-da/vet_clinics/sources/taek_berlin_emergency_parsed_v2.csv\"\n",
    "v3 = \"/Users/martinsvitek/layered-populate-data-pool-da/layered-populate-data-pool-da/vet_clinics/sources/taek_berlin_emergency_parsed_v3.csv\"\n",
    "\n",
    "df2 = pd.read_csv(v2, encoding=\"utf-8\")\n",
    "df3 = pd.read_csv(v3, encoding=\"utf-8\")\n",
    "\n",
    "print(\"v2 cols:\", len(df2.columns))\n",
    "print(\"v3 cols:\", len(df3.columns))\n",
    "print(\"Missing in v3:\", sorted(set(df2.columns) - set(df3.columns)))\n",
    "print(\"New in v3:\", sorted(set(df3.columns) - set(df2.columns)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb153f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/martinsvitek/layered-populate-data-pool-da/layered-populate-data-pool-da/vet_clinics/sources/taek_berlin_emergency_parsed_v3.csv\n"
     ]
    }
   ],
   "source": [
    "import re, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "v2_path = Path( \"/Users/martinsvitek/layered-populate-data-pool-da/layered-populate-data-pool-da/vet_clinics/sources/taek_berlin_emergency_parsed_v2.csv\"\n",
    ")\n",
    "\n",
    "df = pd.read_csv(v2_path, encoding=\"utf-8\")\n",
    "\n",
    "assert \"full_text\" in df.columns, \"v2 must contain full_text\"\n",
    "\n",
    "# ---- regex (your earlier extractors) ----\n",
    "DAYS_DE  = r\"(?:Mo|Di|Mi|Do|Fr|Sa|So)\"\n",
    "TIME     = r\"\\d{1,2}[:.]?\\d{0,2}\\s*(?:Uhr)?\"\n",
    "RANGE    = rf\"{TIME}\\s*[-–]\\s*{TIME}\"\n",
    "RANGE_OR = rf\"{RANGE}(?:\\s*(?:u\\.?|und)\\s*{RANGE})*\"\n",
    "\n",
    "RE_HOURS_BLOCK = re.compile(\n",
    "    rf\"(?:(?:{DAYS_DE})(?:\\s*[-–]\\s*{DAYS_DE})?\\s+{RANGE_OR})(?:\\s*(?:[|;,\\n]\\s*|\\s{DAYS_DE}\\s))?\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "RE_HOURS_LABELED = re.compile(\n",
    "    r\"(Öffnungs(?:zeiten|zeit)|Sprechzeiten?|Sprechstunde|Opening hours)\\s*[:\\-–]?\\s*(?P<label_hours>[^|;•\\n\\r]+)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "RE_EMERGENCY_FLAG = re.compile(\n",
    "    r\"\\b(?:Notdienst|Notfälle?|Notfallsprechstunde|Notaufnahme|Feiertagsnotdienst|24\\s*h|24h|rund um die Uhr|emergency)\\b\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "RE_EMERGENCY_DETAILS = re.compile(\n",
    "    r\"(Notdienst|Notfälle?|Notfallsprechstunde|Feiertagsnotdienst)\\s*[:\\-–]?\\s*(?P<em_details>[^|;•\\n\\r]+)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def extract_opening_hours(text: str):\n",
    "    if not isinstance(text, str): return None\n",
    "    blocks = [m.group(0).strip(\" ,;|\") for m in RE_HOURS_BLOCK.finditer(text)]\n",
    "    if blocks:\n",
    "        return \" | \".join(b for b in blocks if b)\n",
    "    m = RE_HOURS_LABELED.search(text)\n",
    "    return m.group(\"label_hours\").strip() if m else None\n",
    "\n",
    "def extract_emergency(text: str):\n",
    "    if not isinstance(text, str): return False, None\n",
    "    flag = bool(RE_EMERGENCY_FLAG.search(text))\n",
    "    details = None\n",
    "    m = RE_EMERGENCY_DETAILS.search(text)\n",
    "    if m:\n",
    "        details = m.group(\"em_details\").strip(\" ,;|\")\n",
    "    return flag, details\n",
    "\n",
    "# ---- compute new series from full_text ----\n",
    "s = df[\"full_text\"].fillna(\"\")\n",
    "new_hours = s.map(extract_opening_hours)\n",
    "new_flag, new_det = zip(*s.map(extract_emergency))\n",
    "\n",
    "# ---- add/only-fill (never drop, never overwrite non-nulls) ----\n",
    "def add_or_fill(col, series):\n",
    "    if col not in df.columns:\n",
    "        df[col] = series\n",
    "    else:\n",
    "        df[col] = df[col].where(df[col].notna(), pd.Series(series))\n",
    "\n",
    "add_or_fill(\"opening_hours_raw\", new_hours)\n",
    "add_or_fill(\"emergency_flag\",   list(new_flag))\n",
    "add_or_fill(\"emergency_details\",list(new_det))\n",
    "\n",
    "# ---- save to a NEW file, all columns preserved ----\n",
    "out_path = v2_path.with_name(\"taek_berlin_emergency_parsed_v3.csv\")\n",
    "df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"Saved: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c524b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (4.13.5)\n",
      "Requirement already satisfied: lxml in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (6.0.2)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (2.3.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from beautifulsoup4) (4.14.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4 lxml pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "59290248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gefundene Seiten: 9\n",
      "Seite 1: +10\n",
      "Seite 2: +10\n",
      "Seite 3: +10\n",
      "Seite 4: +10\n",
      "Seite 5: +10\n",
      "Seite 6: +10\n",
      "Seite 7: +10\n",
      "Seite 8: +10\n",
      "Seite 9: +4\n",
      "✅ Gespeichert: bpt_tierarztsuche_berlin.csv – 84 Zeilen\n"
     ]
    }
   ],
   "source": [
    "import re, time\n",
    "from urllib.parse import urlparse, parse_qs, urlencode, urlunparse\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "START_URL = \"https://www.tieraerzteverband.de/bpt/ueber-den-bpt/tierarztsuche/index.php?name=&zipcode=&town=berlin&radius=100\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Safari/537.36\",\n",
    "    \"Accept-Language\": \"de-DE,de;q=0.9,en;q=0.8\"\n",
    "}\n",
    "\n",
    "PHONE_CLEAN_RE = re.compile(r\"[^\\d+()\\s/\\-]\")\n",
    "POSTCODE_CITY_RE = re.compile(r\"\\b(\\d{5})\\s+(.+)\")\n",
    "STREET_RE = re.compile(r\".*\\d+\\w?$\")  # Zeile mit Hausnummer\n",
    "\n",
    "def with_page(url, p):\n",
    "    \"\"\"Add/replace ?p=... in URL.\"\"\"\n",
    "    parts = urlparse(url)\n",
    "    q = parse_qs(parts.query)\n",
    "    q[\"p\"] = [str(p)]\n",
    "    new_q = urlencode({k: v[0] for k, v in q.items()})\n",
    "    return urlunparse(parts._replace(query=new_q))\n",
    "\n",
    "def total_pages(html):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    sel = soup.select_one('form[name^=\"pageNaviList\"] select[name=\"p\"]')\n",
    "    if not sel:\n",
    "        return 1\n",
    "    opts = sel.select(\"option\")\n",
    "    return max(int(o.get(\"value\", \"1\")) for o in opts) if opts else 1\n",
    "\n",
    "def text_lines(el):\n",
    "    return [ln.strip() for ln in el.get_text(\"\\n\", strip=True).split(\"\\n\") if ln.strip()]\n",
    "\n",
    "def parse_latlon_from_next_script(result_div):\n",
    "    # Suche das nächste <script> nach diesem Ergebnis, das 'var longtitude' enthält\n",
    "    sc = result_div.find_next(\"script\")\n",
    "    tries = 0\n",
    "    while sc and tries < 5:\n",
    "        t = sc.string or sc.get_text()\n",
    "        if t and \"var longtitude\" in t and \"var latitude\" in t:\n",
    "            m = re.search(r\"longtitude\\s*=\\s*'([\\d\\.]+)';\\s*var\\s+latitude\\s*=\\s*'([\\d\\.]+)'\", t)\n",
    "            if m:\n",
    "                return float(m.group(2)), float(m.group(1))  # lat, lon\n",
    "        sc = sc.find_next(\"script\")\n",
    "        tries += 1\n",
    "    return None, None\n",
    "\n",
    "def parse_results(html):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    rows = []\n",
    "    for card in soup.select(\"div.elementStandard.elementResultLine.mgStyle\"):\n",
    "        # Name\n",
    "        name = (card.select_one(\".headline h2\") or card).get_text(\" \", strip=True)\n",
    "\n",
    "        # Cols\n",
    "        col1 = card.select_one(\".columns .col1\")\n",
    "        col2 = card.select_one(\".columns .col2\")\n",
    "        col3 = card.select_one(\".columns .col3\")\n",
    "\n",
    "        practice = street = postcode = city = distance = None\n",
    "\n",
    "        if col1:\n",
    "            lines = text_lines(col1)\n",
    "            # Entfernung\n",
    "            for ln in lines:\n",
    "                if ln.lower().startswith(\"entfernung\"):\n",
    "                    distance = ln.replace(\"Entfernung:\", \"\").strip()\n",
    "            # Postcode & City\n",
    "            for ln in lines:\n",
    "                m = POSTCODE_CITY_RE.search(ln)\n",
    "                if m:\n",
    "                    postcode, city = m.group(1), m.group(2).strip()\n",
    "            # Street line: letzte Zeile vor PLZ, die Hausnummer enthält\n",
    "            if postcode:\n",
    "                # take the line directly before the postcode line that has a number\n",
    "                for i, ln in enumerate(lines):\n",
    "                    if POSTCODE_CITY_RE.search(ln) and i > 0:\n",
    "                        candidate = lines[i-1]\n",
    "                        if STREET_RE.match(candidate):\n",
    "                            street = candidate\n",
    "                        break\n",
    "            # Praxis/Einrichtung: meist erste Zeile (ohne Entfernung/PLZ/Street)\n",
    "            if lines:\n",
    "                first = lines[0]\n",
    "                if not POSTCODE_CITY_RE.search(first) and not first.lower().startswith(\"entfernung\"):\n",
    "                    practice = first\n",
    "\n",
    "        phone_list, email, website = [], None, None\n",
    "        if col2:\n",
    "            for a in col2.select(\"a.phone, a.mobile\"):\n",
    "                ph = a.get_text(\" \", strip=True)\n",
    "                ph = PHONE_CLEAN_RE.sub(\"\", ph).strip()\n",
    "                if ph and ph not in phone_list:\n",
    "                    phone_list.append(ph)\n",
    "            a_mail = col2.select_one('a.wpst[href^=\"mailto:\"]')\n",
    "            if a_mail:\n",
    "                email = a_mail.get(\"href\").split(\"mailto:\")[-1]\n",
    "            a_www = col2.select_one('a.www[href^=\"http\"]')\n",
    "            if a_www:\n",
    "                website = a_www.get(\"href\")\n",
    "\n",
    "        species = None\n",
    "        if col3:\n",
    "            checks = [c.get_text(\" \", strip=True) for c in col3.select(\".checkbox\")]\n",
    "            if checks:\n",
    "                species = \"; \".join(checks)\n",
    "\n",
    "        lat, lon = parse_latlon_from_next_script(card)\n",
    "\n",
    "        rows.append({\n",
    "            \"name\": name or None,\n",
    "            \"practice\": practice,\n",
    "            \"street\": street,\n",
    "            \"postcode\": postcode,\n",
    "            \"city\": city,\n",
    "            \"distance\": distance,\n",
    "            \"phone\": \" / \".join(phone_list) if phone_list else None,\n",
    "            \"email\": email,\n",
    "            \"website\": website,\n",
    "            \"species\": species,\n",
    "            \"lat\": lat,\n",
    "            \"lon\": lon,\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "def scrape_all(start_url=START_URL, out_csv=\"bpt_tierarztsuche_berlin.csv\"):\n",
    "    s = requests.Session()\n",
    "    s.headers.update(HEADERS)\n",
    "\n",
    "    # 1) Erste Seite laden, Seitenzahl bestimmen\n",
    "    r0 = s.get(start_url, timeout=30)\n",
    "    r0.raise_for_status()\n",
    "    n_pages = total_pages(r0.text)\n",
    "    print(f\"Gefundene Seiten: {n_pages}\")\n",
    "    all_rows = parse_results(r0.text)\n",
    "    print(f\"Seite 1: +{len(all_rows)}\")\n",
    "\n",
    "    # 2) Restliche Seiten p=2..N\n",
    "    for p in range(2, n_pages + 1):\n",
    "        url_p = with_page(start_url, p)\n",
    "        r = s.get(url_p, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        chunk = parse_results(r.text)\n",
    "        print(f\"Seite {p}: +{len(chunk)}\")\n",
    "        all_rows.extend(chunk)\n",
    "        time.sleep(0.8)  # höflich\n",
    "\n",
    "    # 3) Dedupe und speichern\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    # einfache Dedupe-Heuristik:\n",
    "    df[\"dedupe_key\"] = (df[\"name\"].fillna(\"\") + \"|\" + df[\"street\"].fillna(\"\") + \"|\" + df[\"postcode\"].fillna(\"\"))\n",
    "    df = df.drop_duplicates(subset=[\"dedupe_key\"]).drop(columns=[\"dedupe_key\"])\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"✅ Gespeichert: {out_csv} – {len(df)} Zeilen\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_all()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
